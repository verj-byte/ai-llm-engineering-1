{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCk2Rx4cjlYF"
      },
      "source": [
        "# Synthetic Data Generation Using RAGAS - RAG Evaluation with LangSmith\n",
        "\n",
        "In the following notebook we'll explore a use-case for RAGAS' synthetic testset generation workflow!\n",
        "\n",
        "\n",
        "  1. Use RAGAS to Generate Synthetic Data\n",
        "  2. Load them into a LangSmith Dataset\n",
        "  3. Evaluate our RAG chain against the synthetic test data\n",
        "  4. Make changes to our pipeline\n",
        "  5. Evaluate the modified pipeline\n",
        "\n",
        "SDG is a critical piece of the puzzle, especially for early iteration! Without it, it would not be nearly as easy to get high quality early signal for our application's performance.\n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VUI7vF_kbv9"
      },
      "source": [
        "## Task 1: Dependencies and API Keys\n",
        "\n",
        "We'll need to install a number of API keys and dependencies, since we'll be leveraging a number of great technologies for this pipeline!\n",
        "\n",
        "1. OpenAI's endpoints to handle the Synthetic Data Generation\n",
        "2. OpenAI's Endpoints for our RAG pipeline and LangSmith evaluation\n",
        "3. QDrant as our vectorstore\n",
        "4. LangSmith for our evaluation coordinator!\n",
        "\n",
        "Let's install and provide all the required information below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies and API Keys:\n",
        "\n",
        "> NOTE: DO NOT RUN THESE CELLS IF YOU ARE RUNNING THIS NOTEBOOK LOCALLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install -qU ragas==0.2.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install -qU langchain-community==0.3.14 langchain-openai==0.2.14 unstructured==0.16.12 langgraph==0.2.61 langchain-qdrant==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NLTK Import\n",
        "\n",
        "To prevent errors that may occur based on OS - we'll import NLTK and download the needed packages to ensure correct handling of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\DadaV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\DadaV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll also want to set a project name to make things easier for ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Philippines AI Bills RAG - {uuid4().hex[0:8]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OpenAI's API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Synthetic Test Data\n",
        "\n",
        "We wil be using Ragas to build out a set of synthetic test questions, references, and reference contexts. This is useful because it will allow us to find out how our system is performing.\n",
        "\n",
        "> NOTE: Ragas is best suited for finding *directional* changes in your LLM-based systems. The absolute scores aren't comparable in a vacuum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation\n",
        "\n",
        "We'll prepare our data - which should hopefull be familiar at this point since it's our Loan Data use-case!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's load our data into a familiar LangChain format using the `DirectoryLoader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
        "\n",
        "path = \"bills/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Knowledge Graph Based Synthetic Generation\n",
        "\n",
        "Ragas uses a knowledge graph based approach to create data. This is extremely useful as it allows us to create complex queries rather simply. The additional testset complexity allows us to evaluate larger problems more effectively, as systems tend to be very strong on simple evaluation tasks.\n",
        "\n",
        "Let's start by defining our `generator_llm` (which will generate our questions, summaries, and more), and our `generator_embeddings` which will be useful in building our graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unrolled SDG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "# generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we're going to instantiate our Knowledge Graph.\n",
        "\n",
        "This graph will contain N number of nodes that have M number of relationships. These nodes and relationships (AKA \"edges\") will define our knowledge graph and be used later to construct relevant questions and responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 0, relationships: 0)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.graph import KnowledgeGraph\n",
        "\n",
        "kg = KnowledgeGraph()\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step we're going to take is to simply insert each of our full documents into the graph. This will provide a base that we can apply transformations to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 20, relationships: 0)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.graph import Node, NodeType\n",
        "\n",
        "### NOTICE: We're using a subset of the data for this example - this is to keep costs/time down.\n",
        "for doc in docs[:20]:\n",
        "    kg.nodes.append(\n",
        "        Node(\n",
        "            type=NodeType.DOCUMENT,\n",
        "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
        "        )\n",
        "    )\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we'll apply the *default* transformations to our knowledge graph. This will take the nodes currently on the graph and transform them based on a set of [default transformations](https://docs.ragas.io/en/latest/references/transforms/#ragas.testset.transforms.default_transforms).\n",
        "\n",
        "These default transformations are dependent on the corpus length, in our case:\n",
        "\n",
        "- Producing Summaries -> produces summaries of the documents\n",
        "- Extracting Headlines -> finding the overall headline for the document\n",
        "- Theme Extractor -> extracts broad themes about the documents\n",
        "\n",
        "It then uses cosine-similarity and heuristics between the embeddings of the above transformations to construct relationships between the nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fea59c9bae9e46f391c1c5daec2a5ac3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71a603614a15435b8de478b7fecfd81e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node a8d69497-a1e2-43b3-aded-61e98d455ab4 does not have a summary. Skipping filtering.\n",
            "Node 0bff45a8-6782-411c-9692-ae6b64a8bd5a does not have a summary. Skipping filtering.\n",
            "Node 5637a187-1266-4001-8d9c-964282ce8bff does not have a summary. Skipping filtering.\n",
            "Node c115fe00-ea6f-4369-ac12-41d7d80f8600 does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a87722b536974343bc23ae49169daf6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/56 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bce498c3205e4536a668adfad051c1c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 20, relationships: 137)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.transforms import default_transforms, apply_transforms\n",
        "\n",
        "transformer_llm = generator_llm\n",
        "embedding_model = generator_embeddings\n",
        "\n",
        "default_transforms = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n",
        "apply_transforms(kg, default_transforms)\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can save and load our knowledge graphs as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 20, relationships: 137)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kg.save(\"bills/ai_law.json\")\n",
        "bills_data_kg = KnowledgeGraph.load(\"bills/ai_law.json\")\n",
        "bills_data_kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using our knowledge graph, we can construct a \"test set generator\" - which will allow us to create queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=embedding_model, knowledge_graph=bills_data_kg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, we'd like to be able to define the kinds of queries we're generating - which is made simple by Ragas having pre-created a number of different \"QuerySynthesizer\"s.\n",
        "\n",
        "Each of these Synthetsizers is going to tackle a separate kind of query which will be generated from a scenario and a persona.\n",
        "\n",
        "In essence, Ragas will use an LLM to generate a persona of someone who would interact with the data - and then use a scenario to construct a question from that data and persona."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset.synthesizers import default_query_distribution, SingleHopSpecificQuerySynthesizer, MultiHopAbstractQuerySynthesizer, MultiHopSpecificQuerySynthesizer\n",
        "\n",
        "query_distribution = [\n",
        "        (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 0.5),\n",
        "        (MultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.25),\n",
        "        (MultiHopSpecificQuerySynthesizer(llm=generator_llm), 0.25),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can use our `TestSetGenerator` to generate our testset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cbd89067dfa44218a7cb8e777b19137",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe9fe3f6458d4a2a84b835eecec2cde4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4f312060ab445aeb5f4907d7be2bd6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "testset = generator.generate(testset_size=20, query_distribution=query_distribution)\n",
        "testset.to_pandas()\n",
        "testset.to_jsonl(\"bills/golden_dataset.json\")  # Save for reuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Abstracted SDG\n",
        "\n",
        "The above method is the full process - but we can shortcut that using the provided abstractions!\n",
        "\n",
        "This will generate our knowledge graph under the hood, and will - from there - generate our personas and scenarios to construct our queries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e276d86b8d74d1291d3814171443e2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "710f56dc09064e8b8ff6aa60a211b1c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node ac66e9c0-1931-462f-8c33-5de03edf6fea does not have a summary. Skipping filtering.\n",
            "Node 15a7b403-e411-446f-843f-e7b8afbf2120 does not have a summary. Skipping filtering.\n",
            "Node fa0e4f71-d750-42de-a1a4-9354dbdd9c9d does not have a summary. Skipping filtering.\n",
            "Node 8f4625ef-c683-41dc-a4f2-e89eb0257119 does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efe3863f17894841a1d978e136f66a8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/56 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4ab45ff09b847e3ac6938723a0b4d5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "103056acdff847ab915e6001c7be715b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c0feb400aa24ffb8966b883441093ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "854d1e7f68a04e8cb42d0f61e8347832",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs[:20], testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Who is PIA S. CAYETANO in the context of AI re...</td>\n",
              "      <td>[TWENTIETH CONGRESS OF THE \\nREPUBLIC OF THE P...</td>\n",
              "      <td>PIA S. CAYETANO is a senator who introduced an...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does AI impact the Philippines according t...</td>\n",
              "      <td>[AI presents enormous opportunities for the Ph...</td>\n",
              "      <td>AI presents enormous opportunities for the Phi...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What does the P4:56 refer to in the context of...</td>\n",
              "      <td>[TWENTIETH CONGRESS OF THE \\nREPUBLIC OF THE P...</td>\n",
              "      <td>The P4:56 appears to be a reference code or ti...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How does the Philippines regulate the developm...</td>\n",
              "      <td>[1 \\na) Promote innovation, technological adva...</td>\n",
              "      <td>The Philippines regulates the development and ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does the use of AI for manipulatn, disinfo...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n1 \\niii) Mandatory compliance trai...</td>\n",
              "      <td>The context highlights that AI used for manipu...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How does the deployment of AI systems relate t...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8 \\n9\\n10\\n11...</td>\n",
              "      <td>The deployment of AI systems is closely connec...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>whats the repleal of laws and AI hallusinations?</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n1 \\nendorsements, voice recordings...</td>\n",
              "      <td>The context discusses the repeal of laws that ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How do government agencies like DOST and DICT ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n1 \\niii) Mandatory compliance trai...</td>\n",
              "      <td>The context specifies that government agencies...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How do the laws and regulations regarding AI i...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n1 \\niii) Mandatory compliance trai...</td>\n",
              "      <td>The laws and regulations in the Philippines pr...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>how does the national AI commission work with ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n1 \\nSec. 8. NAICSecretariat. - The...</td>\n",
              "      <td>the national AI commission develops policies a...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How does the NAIC's jurisdiction over AI, as o...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n1 \\nSec. 6. Jurisdiction of the NA...</td>\n",
              "      <td>The NAIC's jurisdiction over AI, as specified ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>How does the AI registration process, includin...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n1 \\nSec. 12. Contents of AI Regist...</td>\n",
              "      <td>The AI registration process requires detailed ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   Who is PIA S. CAYETANO in the context of AI re...   \n",
              "1   How does AI impact the Philippines according t...   \n",
              "2   What does the P4:56 refer to in the context of...   \n",
              "3   How does the Philippines regulate the developm...   \n",
              "4   How does the use of AI for manipulatn, disinfo...   \n",
              "5   How does the deployment of AI systems relate t...   \n",
              "6    whats the repleal of laws and AI hallusinations?   \n",
              "7   How do government agencies like DOST and DICT ...   \n",
              "8   How do the laws and regulations regarding AI i...   \n",
              "9   how does the national AI commission work with ...   \n",
              "10  How does the NAIC's jurisdiction over AI, as o...   \n",
              "11  How does the AI registration process, includin...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [TWENTIETH CONGRESS OF THE \\nREPUBLIC OF THE P...   \n",
              "1   [AI presents enormous opportunities for the Ph...   \n",
              "2   [TWENTIETH CONGRESS OF THE \\nREPUBLIC OF THE P...   \n",
              "3   [1 \\na) Promote innovation, technological adva...   \n",
              "4   [<1-hop>\\n\\n1 \\niii) Mandatory compliance trai...   \n",
              "5   [<1-hop>\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8 \\n9\\n10\\n11...   \n",
              "6   [<1-hop>\\n\\n1 \\nendorsements, voice recordings...   \n",
              "7   [<1-hop>\\n\\n1 \\niii) Mandatory compliance trai...   \n",
              "8   [<1-hop>\\n\\n1 \\niii) Mandatory compliance trai...   \n",
              "9   [<1-hop>\\n\\n1 \\nSec. 8. NAICSecretariat. - The...   \n",
              "10  [<1-hop>\\n\\n1 \\nSec. 6. Jurisdiction of the NA...   \n",
              "11  [<1-hop>\\n\\n1 \\nSec. 12. Contents of AI Regist...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   PIA S. CAYETANO is a senator who introduced an...   \n",
              "1   AI presents enormous opportunities for the Phi...   \n",
              "2   The P4:56 appears to be a reference code or ti...   \n",
              "3   The Philippines regulates the development and ...   \n",
              "4   The context highlights that AI used for manipu...   \n",
              "5   The deployment of AI systems is closely connec...   \n",
              "6   The context discusses the repeal of laws that ...   \n",
              "7   The context specifies that government agencies...   \n",
              "8   The laws and regulations in the Philippines pr...   \n",
              "9   the national AI commission develops policies a...   \n",
              "10  The NAIC's jurisdiction over AI, as specified ...   \n",
              "11  The AI registration process requires detailed ...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vSRr2MXk0P_"
      },
      "source": [
        "We'll need to provide our LangSmith API key, and set tracing to \"true\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SLtk1GtnyoY"
      },
      "source": [
        "## Task 4: LangSmith Dataset\n",
        "\n",
        "Now we can move on to creating a dataset for LangSmith!\n",
        "\n",
        "First, we'll need to create a dataset on LangSmith using the `Client`!\n",
        "\n",
        "We'll name our Dataset to make it easy to work with later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "TLgm6OjvYSsm"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"Philippines AI Bills x2\"\n",
        "\n",
        "langsmith_dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Philippines AI Bills\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64SmXMBnzXWm"
      },
      "source": [
        "We'll iterate through the RAGAS created dataframe - and add each example to our created dataset!\n",
        "\n",
        "> NOTE: We need to conform the outputs to the expected format - which in this case is: `question` and `answer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8nFQ6di_XnY7"
      },
      "outputs": [],
      "source": [
        "for data_row in dataset.to_pandas().iterrows():\n",
        "  client.create_example(\n",
        "      inputs={\n",
        "          \"question\": data_row[1][\"user_input\"]\n",
        "      },\n",
        "      outputs={\n",
        "          \"answer\": data_row[1][\"reference\"]\n",
        "      },\n",
        "      metadata={\n",
        "          \"context\": data_row[1][\"reference_contexts\"]\n",
        "      },\n",
        "      dataset_id=langsmith_dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6EbQVyZq-2j"
      },
      "source": [
        "## Basic RAG Chain\n",
        "\n",
        "Time for some RAG!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "4njbUAIsaYjB"
      },
      "outputs": [],
      "source": [
        "rag_documents = docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQorBy8H1AZR"
      },
      "source": [
        "To keep things simple, we'll just use LangChain's recursive character text splitter!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "qWo3Ajaragv1"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "rag_documents = text_splitter.split_documents(rag_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kghuTb9R01oO"
      },
      "source": [
        "We'll create our vectorstore using OpenAI's [`text-embedding-3-small`](https://platform.openai.com/docs/guides/embeddings/embedding-models) embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "UwfJCzP3aqKI"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the \"A\" in RAG, we'll provide a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "You are a helpful and informative assistant. Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "If you cannot answer the question based on the context, or you are unsure, you must say \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For our LLM, we will be using TogetherAI's endpoints as well!\n",
        "\n",
        "We're going to be using Meta Llama 3.1 70B Instruct Turbo - a powerful model which should get us powerful results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As usual, we will power our RAG application with Qdrant!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")\n",
        "\n",
        "semantic_documents = semantic_chunker.split_documents(rag_documents[:20])\n",
        "\n",
        "# semantic chunking OFF - use rag_documents, else semantic_documents\n",
        "\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "text_vectorstore = Qdrant.from_documents(\n",
        "    documents=rag_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"AI Bills RAG\")\n",
        "\n",
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    documents=semantic_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"AI Bills RAG\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangSmith Evaluation Set-up\n",
        "\n",
        "We'll use OpenAI's GPT-4.1 as our evaluation LLM for our base Evaluators.\n",
        "\n",
        "We'll be using a number of evaluators - from LangSmith provided evaluators, to a few custom evaluators!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_llm = ChatOpenAI(model=\"gpt-4.1\")\n",
        "\n",
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm})\n",
        "\n",
        "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"helpfulness\": (\n",
        "                \"Is this submission helpful to the user,\"\n",
        "                \" taking into account the correct reference answer?\"\n",
        "            )\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": run.outputs[\"output\"],\n",
        "        \"reference\": example.outputs[\"answer\"],\n",
        "        \"input\": example.inputs[\"question\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "empathy_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"empathy\": \"Is this response empathetic? Does it make the user feel like they are being heard?\",\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbKSjfSkbTYo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'formal-suit-15' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/f3027cb6-44cc-48c0-94a2-a64502c32b73/compare?selectedSessions=4a336e31-9ed7-422a-a478-6a75f384d4b0\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12c1c29d88cf42f1828c9fcd5661771d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 92032690-42dd-4748-9322-2ecc6277470d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 92032690-42dd-4748-9322-2ecc6277470d: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 92032690-42dd-4748-9322-2ecc6277470d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bcd563df-d76d-43ee-b02b-b7d26e1d525e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bcd563df-d76d-43ee-b02b-b7d26e1d525e: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bcd563df-d76d-43ee-b02b-b7d26e1d525e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 35840d9e-e5d2-42ee-a32b-ad3ee74ecd22: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 35840d9e-e5d2-42ee-a32b-ad3ee74ecd22: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 35840d9e-e5d2-42ee-a32b-ad3ee74ecd22: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 99585e4d-d087-433a-b983-4b529507bff6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 99585e4d-d087-433a-b983-4b529507bff6: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 99585e4d-d087-433a-b983-4b529507bff6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d3a00e1d-c1dc-49c3-84eb-4e6ccd5719f8: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d3a00e1d-c1dc-49c3-84eb-4e6ccd5719f8: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d3a00e1d-c1dc-49c3-84eb-4e6ccd5719f8: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cd78a6a6-a813-445d-b1ef-55ffd78ae9c9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cd78a6a6-a813-445d-b1ef-55ffd78ae9c9: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cd78a6a6-a813-445d-b1ef-55ffd78ae9c9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f8145c7a-cf0f-4c6b-b879-0e21bdd8f3ae: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f8145c7a-cf0f-4c6b-b879-0e21bdd8f3ae: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f8145c7a-cf0f-4c6b-b879-0e21bdd8f3ae: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5344966b-34f4-4ec0-b047-e960d226f908: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5344966b-34f4-4ec0-b047-e960d226f908: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5344966b-34f4-4ec0-b047-e960d226f908: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 21c7f70f-41b4-45cf-aa91-a24487a2eec9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 21c7f70f-41b4-45cf-aa91-a24487a2eec9: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 21c7f70f-41b4-45cf-aa91-a24487a2eec9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5a94b9d9-0aea-4fc2-962f-8c4bc2070cc6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5a94b9d9-0aea-4fc2-962f-8c4bc2070cc6: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5a94b9d9-0aea-4fc2-962f-8c4bc2070cc6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cbe31afe-cf5c-4939-a6af-9023af789b2d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cbe31afe-cf5c-4939-a6af-9023af789b2d: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cbe31afe-cf5c-4939-a6af-9023af789b2d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b909ab44-a38f-45e1-855b-c9caf0d548ea: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b909ab44-a38f-45e1-855b-c9caf0d548ea: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b909ab44-a38f-45e1-855b-c9caf0d548ea: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'diligent-jewel-56' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/f3027cb6-44cc-48c0-94a2-a64502c32b73/compare?selectedSessions=e3eea85e-3120-496a-a147-9e57fb2ee0fe\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f43a071d65d4f0fa00dccfffaf5197b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4fdbb50e-5907-4647-ad29-9c2f438e8525: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4fdbb50e-5907-4647-ad29-9c2f438e8525: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4fdbb50e-5907-4647-ad29-9c2f438e8525: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4cfea9d5-4a07-4317-b870-24451eefb03e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4cfea9d5-4a07-4317-b870-24451eefb03e: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4cfea9d5-4a07-4317-b870-24451eefb03e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bc6631d6-3e0e-4a2f-8d2f-283d813fc5dc: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bc6631d6-3e0e-4a2f-8d2f-283d813fc5dc: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bc6631d6-3e0e-4a2f-8d2f-283d813fc5dc: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 261ea1ab-69cd-4281-83a4-fc2ab09fd855: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 261ea1ab-69cd-4281-83a4-fc2ab09fd855: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 261ea1ab-69cd-4281-83a4-fc2ab09fd855: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 36d563e6-cc0d-40bf-84c6-8e7d308bac0d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 36d563e6-cc0d-40bf-84c6-8e7d308bac0d: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 36d563e6-cc0d-40bf-84c6-8e7d308bac0d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9ccce1ed-1683-4f41-92a0-739856144814: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9ccce1ed-1683-4f41-92a0-739856144814: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9ccce1ed-1683-4f41-92a0-739856144814: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e8107695-1029-426e-8f84-e635fd087174: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e8107695-1029-426e-8f84-e635fd087174: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e8107695-1029-426e-8f84-e635fd087174: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run eb0e7fe6-5033-49be-8ea7-7d87cddb2fdc: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run eb0e7fe6-5033-49be-8ea7-7d87cddb2fdc: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run eb0e7fe6-5033-49be-8ea7-7d87cddb2fdc: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 501ef3e9-fa99-4a15-8910-7eba3e8160af: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 501ef3e9-fa99-4a15-8910-7eba3e8160af: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 501ef3e9-fa99-4a15-8910-7eba3e8160af: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run db5691c7-1957-430b-a2de-68470d34e8e1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run db5691c7-1957-430b-a2de-68470d34e8e1: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run db5691c7-1957-430b-a2de-68470d34e8e1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 91691adf-1d6e-421b-9956-a6999be165ff: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 91691adf-1d6e-421b-9956-a6999be165ff: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 91691adf-1d6e-421b-9956-a6999be165ff: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5805c516-33aa-4fb0-93ce-0ce3f126f1bd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5805c516-33aa-4fb0-93ce-0ce3f126f1bd: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28768\\2881423215.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5805c516-33aa-4fb0-93ce-0ce3f126f1bd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001CA94158A70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001CA9415B770>, root_client=<openai.OpenAI object at 0x000001CA93E5D450>, root_async_client=<openai.AsyncOpenAI object at 0x000001CA93E5EE50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 96\u001b[39m\n\u001b[32m     86\u001b[39m rag_chain = (\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\u001b[39;00m\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# | rag_prompt | llm | StrOutputParser()\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m     | {\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: rag_prompt | chat_model, \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: itemgetter(\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m)}\n\u001b[32m     92\u001b[39m )\n\u001b[32m     94\u001b[39m rag_chain.invoke({\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33mHow does the Philippines regulate the development of AI in the country?\u001b[39m\u001b[33m\"\u001b[39m})[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m].content\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m latency = \u001b[43mtime\u001b[49m.time() - start_time\n\u001b[32m     97\u001b[39m evaluate(\n\u001b[32m     98\u001b[39m     rag_chain.invoke,\n\u001b[32m     99\u001b[39m     data=dataset_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m         }\n\u001b[32m    110\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<stringsource>:69\u001b[39m, in \u001b[36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1481\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._line_event\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1586\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1961\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[43mnotify_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\threading.py:659\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    657\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\threading.py:363\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    365\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "# Naive retriever\n",
        "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "retriever_names = [\n",
        "    \"Naive\", \"BM25\", \"Multi-Query\", \"Parent-Document\", \"Contextual Compression\", \"Ensemble\"\n",
        "]\n",
        "results = []\n",
        "\n",
        "import time\n",
        "\n",
        "for sc in [False, True]:  # semantic chunking off/on\n",
        "    for retriever_name in retriever_names: # loop through each retriever\n",
        "    \n",
        "        start_time = time.time()\n",
        "        if sc:\n",
        "            vectorstore = semantic_vectorstore\n",
        "        else:\n",
        "            vectorstore = text_vectorstore\n",
        "\n",
        "        match retriever_name:\n",
        "        \n",
        "            case 'Naive':\n",
        "                retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "                naive_retriever = retriever\n",
        "            \n",
        "            case 'BM25':\n",
        "                retriever = BM25Retriever.from_documents(rag_documents) \n",
        "                bm25_retriever = retriever\n",
        "            \n",
        "            case 'Contextual Compression':\n",
        "                compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "                retriever = ContextualCompressionRetriever(\n",
        "                            base_compressor=compressor, base_retriever=naive_retriever)\n",
        "                compression_retriever = retriever\n",
        "\n",
        "            case 'Multi-Query':\n",
        "                retriever = MultiQueryRetriever.from_llm(retriever=naive_retriever, llm=chat_model)\n",
        "                multi_query_retriever = retriever\n",
        "                \n",
        "            case 'Parent-Document':\n",
        "                parent_docs = rag_documents\n",
        "                child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)\n",
        "\n",
        "                client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "                client.create_collection(\n",
        "                    collection_name=\"full_documents\",\n",
        "                    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        "                )\n",
        "\n",
        "                parent_document_vectorstore = QdrantVectorStore(\n",
        "                    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        "                )\n",
        "                store = InMemoryStore()\n",
        "                retriever = ParentDocumentRetriever(\n",
        "                                vectorstore = parent_document_vectorstore,\n",
        "                                docstore=store,\n",
        "                                child_splitter=child_splitter)\n",
        "                retriever.add_documents(parent_docs, ids=None)\n",
        "                parent_document_retriever = retriever\n",
        "\n",
        "            case 'Ensemble':\n",
        "                retriever_list = [naive_retriever, bm25_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "                equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "                retriever = EnsembleRetriever(retrievers=retriever_list, weights=equal_weighting)\n",
        "            \n",
        "            case ' ':\n",
        "                retriever = \"Invalid option\"\n",
        "        \n",
        "        from operator import itemgetter\n",
        "        from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "        from langchain.schema import StrOutputParser\n",
        "\n",
        "        rag_chain = (\n",
        "            # {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "            # | rag_prompt | llm | StrOutputParser()\n",
        "            {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "            | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "            | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        "        )\n",
        "\n",
        "        rag_chain.invoke({\"question\" : \"How does the Philippines regulate the development of AI in the country?\"})[\"response\"].content\n",
        "\n",
        "        latency = time.time() - start_time\n",
        "        evaluate(\n",
        "            rag_chain.invoke,\n",
        "            data=dataset_name,\n",
        "            evaluators=[\n",
        "                qa_evaluator,\n",
        "                labeled_helpfulness_evaluator,\n",
        "                empathy_evaluator],\n",
        "            metadata={\n",
        "                \"revision_id\": \"default_chain_init\",\n",
        "                \"retriever\": retriever_name,\n",
        "                \"semantic chunking\": sc\n",
        "                # \"latency\": latency\n",
        "                }\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmTL6-pc1ZGz"
      },
      "source": [
        "Finally, we can set-up our RAG LCEL chain!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R35sQMHVrnpl"
      },
      "source": [
        "## LangSmith Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "122b1bd1f0e9417a8dcb57d4eebe4d2e",
            "e0c233ad01604540a6c873f4a731982d",
            "e9a01115c75b499884f7e0ef32e9e599",
            "5faba4ad609448b2b49024add4ad3b8e",
            "ef25efa751304e4699910f1fbc14345f",
            "0b44cb0f8e34446c8dde668a75d3d8ad",
            "edaac6587b2d4bd5be52b89bb097f99f",
            "7cb241365f604419af454c1c28de197a",
            "9cf586576ff44dba86ba2eb389593c61",
            "849b5c95008541d49f1ceedf0a59ac60",
            "f3665a86662746c4ac7cb0796604781d"
          ]
        },
        "id": "t7t_Uz0tdumL",
        "outputId": "d684e218-294e-4dc3-c8de-a01d397f021c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'giving-trick-63' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/ef297373-b111-4973-ab34-102fa9ead893/compare?selectedSessions=59c23679-3fb3-463c-ae54-7bb8705f7490\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a0ef0e17a854b6f90f8ac92e997bfa4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.output</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.correctness</th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.empathy</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does the AI registration process, includin...</td>\n",
              "      <td>Based on the provided context, the AI registra...</td>\n",
              "      <td>None</td>\n",
              "      <td>The AI registration process requires detailed ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5.208305</td>\n",
              "      <td>5224b263-77a3-4a2b-b3e2-165729d7b4f0</td>\n",
              "      <td>a39166aa-51d4-48d9-ab83-6218f233cbec</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the NAIC's jurisdiction over AI, as o...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>The NAIC's jurisdiction over AI, as specified ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.300400</td>\n",
              "      <td>5f49dd4e-5938-41f3-86a7-0134d10e9cfd</td>\n",
              "      <td>d2900b56-f564-445e-b917-bd0088a69d7d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>how does the national AI commission work with ...</td>\n",
              "      <td>The National AI Commission (NAIC) coordinates ...</td>\n",
              "      <td>None</td>\n",
              "      <td>the national AI commission develops policies a...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3.111340</td>\n",
              "      <td>703738e2-269e-49d9-92b3-76f70563ebdd</td>\n",
              "      <td>f68bcf18-e913-4b13-a85b-4f33e1540732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How do the laws and regulations regarding AI i...</td>\n",
              "      <td>The laws and regulations regarding AI in the P...</td>\n",
              "      <td>None</td>\n",
              "      <td>The laws and regulations in the Philippines pr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.560005</td>\n",
              "      <td>6557c129-dcb4-465f-b1d3-3201a8b2470b</td>\n",
              "      <td>e4f4b466-ff97-47cf-8c68-ca368d407850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do government agencies like DOST and DICT ...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>The context specifies that government agencies...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.866825</td>\n",
              "      <td>262b891c-be05-4f04-b079-4407068bc933</td>\n",
              "      <td>903a9571-f41c-40b7-a4bb-33cecda1b736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>whats the repleal of laws and AI hallusinations?</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>The context discusses the repeal of laws that ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.990699</td>\n",
              "      <td>e2584e77-e0b0-4af4-b348-26650406ec39</td>\n",
              "      <td>5f9e6b05-9b7a-4801-af1b-b7ad6a4f29e0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the deployment of AI systems relate t...</td>\n",
              "      <td>Based on the provided context, the deployment ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The deployment of AI systems is closely connec...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5.766852</td>\n",
              "      <td>0846ba3a-7e0c-499b-80cb-061874e55ac9</td>\n",
              "      <td>4c32fdc6-d8ad-4635-9e86-8e13155b4ae7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How does the use of AI for manipulatn, disinfo...</td>\n",
              "      <td>Based on the provided context, the NAIC (Natio...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that AI used for manipu...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6.190494</td>\n",
              "      <td>3be306f4-3391-41f6-a9df-23d150ba847c</td>\n",
              "      <td>4cf4a9da-98ec-4578-8480-091332fa6fdc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the Philippines regulate the developm...</td>\n",
              "      <td>According to the provided context, the Philipp...</td>\n",
              "      <td>None</td>\n",
              "      <td>The Philippines regulates the development and ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3.235278</td>\n",
              "      <td>1985fb8a-385b-4602-afc4-dfa241eead63</td>\n",
              "      <td>c93ec0d0-edd0-4370-9d45-c162f4c40cbb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What does the P4:56 refer to in the context of...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>The P4:56 appears to be a reference code or ti...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.814284</td>\n",
              "      <td>f56ccff2-ab8d-4b7e-b993-e2c5236f6930</td>\n",
              "      <td>d1d62e0d-7d1e-4cc6-be27-71bca1498c95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How does AI impact the Philippines according t...</td>\n",
              "      <td>According to the provided context, AI presents...</td>\n",
              "      <td>None</td>\n",
              "      <td>AI presents enormous opportunities for the Phi...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.319625</td>\n",
              "      <td>d56c8872-a388-450d-bc8e-52f91d64d493</td>\n",
              "      <td>f3e1b3bb-8979-426d-a201-d9694008a90d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Who is PIA S. CAYETANO in the context of AI re...</td>\n",
              "      <td>Based on the provided context, PIA S. CAYETANO...</td>\n",
              "      <td>None</td>\n",
              "      <td>PIA S. CAYETANO is a senator who introduced an...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.035867</td>\n",
              "      <td>c3cd0729-c2f1-410c-b1ff-bc22b9e8e29e</td>\n",
              "      <td>bac852cf-a283-4eaa-a1f3-9d8ab6b20afc</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults giving-trick-63>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        empathy_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"default_chain_init\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq7fCVinrpI4"
      },
      "source": [
        "## Dope-ifying Our Application\n",
        "\n",
        "We'll be making a few changes to our RAG chain to increase its performance on our SDG evaluation test dataset!\n",
        "\n",
        "- Include a \"dope\" prompt augmentation\n",
        "- Use larger chunks\n",
        "- Improve the retriever model to: `text-embedding-3-large`\n",
        "\n",
        "Let's see how this changes our evaluation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "z56pXwyUgFUt"
      },
      "outputs": [],
      "source": [
        "EMPATHY_RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
        "\n",
        "You must answer the question using empathy and kindness, and make sure the user feels heard.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "empathy_rag_prompt = ChatPromptTemplate.from_template(EMPATHY_RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rZLcTstJgfv5"
      },
      "outputs": [],
      "source": [
        "rag_documents = docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-LYsyirngj6n"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "rag_documents = text_splitter.split_documents(rag_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "b9MI2Bm2go1r"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "hVUY25FKgxXx"
      },
      "outputs": [],
      "source": [
        "vectorstore = Qdrant.from_documents(\n",
        "    documents=rag_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"AI Bills RAG 2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Q4TOZNYIg2v1"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqYGFrnKDB91"
      },
      "source": [
        "Setting up our new and improved DOPE RAG CHAIN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "HqnTqeXMhAdx"
      },
      "outputs": [],
      "source": [
        "empathy_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | empathy_rag_prompt | llm | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21pTxoqJDI1Y"
      },
      "source": [
        "Let's test it on the same output that we saw before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OfZZ3MoN3fKv",
        "outputId": "d65722dd-92c2-4e4e-9cca-c42ee6f3f208"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Thank you for your thoughtful question. Based on the context provided, the Philippines AI Bill is important because it aims to create a national framework that balances the encouragement of technological innovation with ensuring that AI systems are safe, ethical, transparent, and under meaningful human oversight. It recognizes the transformative impact of AI on industries and society and seeks to promote responsible and lawful AI development that supports Filipino ingenuity and addresses national development challenges.\\n\\nMoreover, the bill emphasizes protecting the rights and welfare of every citizen by preventing AI from being used to commit crimes, abuse rights, or cause harm, whether intentionally or accidentally. This shows a deep concern for the well-being of the people while fostering progress and innovation in technology.\\n\\nIt’s clear that the bill strives to create a secure, inclusive, and ethical digital future for the Philippines, thoughtfully anticipating both the opportunities and risks that AI presents. If you have more questions or need further clarification, I’m here to help!'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "empathy_rag_chain.invoke({\"question\" : \"Why is the Philippines AI Bill important?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpj7v1inDLnQ"
      },
      "source": [
        "Finally, we can evaluate the new chain on the same test set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "bf8dcc0895054529af356da401c513f6",
            "7dce19ac55264f2b88a0e4730e55867b",
            "2a0755d4476543feb4a64538e3e37213",
            "158212a630f04cbd884c937f2f60f5c8",
            "11c7f66acc1d45be9517d0addf49331e",
            "ddffd834e09940a4bd3874c3f39b4e21",
            "ef63c3b2d51e452da03cdae5d9b034be",
            "c20b539cd70b4ba99601ad1d69fd9cec",
            "a6d681eeafa44d18b933a4c5dec88382",
            "d1d54ccd56494c4d831f71b416a1f880",
            "530f696feefe499da08c6312047379b2"
          ]
        },
        "id": "Dx11S2b-hIM8",
        "outputId": "d3a3ea78-aa32-4bd2-8c2a-d0d0303695c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'complicated-station-33' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/861faf01-85b7-44d1-8a8c-72463fffb4fb/compare?selectedSessions=112f58bd-e760-499e-9456-900ca391a6b3\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dfce57083084dfeab8ff95cbe21ffd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.output</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.correctness</th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.empathy</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does the DOST support AI policies and init...</td>\n",
              "      <td>Thank you for your thoughtful question. From t...</td>\n",
              "      <td>None</td>\n",
              "      <td>The DOST supports AI policies and initiatives ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.705640</td>\n",
              "      <td>0768fd5a-970b-482f-aa5c-ec1ef5a0d906</td>\n",
              "      <td>c5f0d260-aa2b-4ab3-bdc3-508429074e06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does TESDA collaborate with the NAIC to su...</td>\n",
              "      <td>Thank you for your thoughtful question. Based ...</td>\n",
              "      <td>None</td>\n",
              "      <td>TESDA collaborates with the NAIC by supporting...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.433222</td>\n",
              "      <td>7baaaa67-bf6a-48fd-9255-d6d474fc30d3</td>\n",
              "      <td>2dea2471-7639-479e-8b25-dc6a1333f59b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does TESDA's role in AI regulation relate ...</td>\n",
              "      <td>Thank you for your thoughtful question. From t...</td>\n",
              "      <td>None</td>\n",
              "      <td>TESDA is listed as a member of the NAIC, which...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.652752</td>\n",
              "      <td>a2141d56-0bcd-4a44-83d0-ce71916c01d0</td>\n",
              "      <td>a8a0436f-3524-477c-8046-67d06d6d5fcb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>how DOST and NAIC work together in AI regulati...</td>\n",
              "      <td>Thank you for your thoughtful question. From t...</td>\n",
              "      <td>None</td>\n",
              "      <td>The NAIC, which is attached to the DOST, has j...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9.333190</td>\n",
              "      <td>38c5205d-984d-4c7f-96b3-2001897dc841</td>\n",
              "      <td>57be7d9d-48b4-4781-8139-4d9421ecf097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does the regulation of AI responsibility f...</td>\n",
              "      <td>Thank you for your thoughtful question. Based ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The regulation of AI responsibility for harms ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.489557</td>\n",
              "      <td>db67c379-1fa4-4938-a726-861cd73c7e19</td>\n",
              "      <td>47964008-2975-4263-97db-ba3c15d056ea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How do the United Nations' recommendations for...</td>\n",
              "      <td>Thank you for your thoughtful question. From t...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that international effo...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.487013</td>\n",
              "      <td>33a985ea-db0d-49d3-a7b7-22d7cd6567a1</td>\n",
              "      <td>2abf25db-1383-477d-b5d0-dece432f7ad8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the need for regulation and oversight...</td>\n",
              "      <td>Thank you for your thoughtful question. It's c...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that while AI offers si...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10.912735</td>\n",
              "      <td>cdcf2c9c-9950-4642-b234-36eec086d17f</td>\n",
              "      <td>951709d5-a3cd-4d29-a8de-5c65a50c48e1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How can the development of multimodal models e...</td>\n",
              "      <td>Thank you for your thoughtful question. From t...</td>\n",
              "      <td>None</td>\n",
              "      <td>The development of multimodal models can enhan...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7.673174</td>\n",
              "      <td>b8033ddb-c877-49df-acac-bebfecc89e36</td>\n",
              "      <td>29c0abb1-dead-4a86-8f72-48abd4795d36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the regulation of Artificial General ...</td>\n",
              "      <td>Thank you for your thoughtful question. Based ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context states that policies should promot...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.787368</td>\n",
              "      <td>fdb75978-1d94-4b8d-b2c2-54e67e231d59</td>\n",
              "      <td>42463c9f-beca-4058-bd5d-51a25fec38db</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What does the PHILIPPINE CONSTITUTION say abou...</td>\n",
              "      <td>Thank you for your question about what the Phi...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context references Article XIV, Section 10...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.799059</td>\n",
              "      <td>1f7ddec1-711c-4e11-bca1-351189c11a8d</td>\n",
              "      <td>b2306d1c-746d-4483-ab9b-2167cbc9a440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Georgetown University how does it relate to AI...</td>\n",
              "      <td>Thank you for your thoughtful question. Based ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context mentions Georgetown University in ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.645414</td>\n",
              "      <td>9e02174d-66d4-4b77-8e12-ff448ea703b6</td>\n",
              "      <td>b2d260b2-e61e-431b-a72c-e548a0256599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What is the significance of regulating AI in t...</td>\n",
              "      <td>Thank you for your thoughtful question. Based ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The regulation of AI in the Philippines aims t...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.056313</td>\n",
              "      <td>c354b332-5d2d-46cb-90b8-4860830246d6</td>\n",
              "      <td>23ac1c02-5ebf-4958-9486-ff956f008240</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults complicated-station-33>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    empathy_rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        empathy_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"empathy_rag_chain\"},\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "13-advanced-retrieval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07ab3dc0790241bbb85a7f488a42ef8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7710c7377cbc4c30b55b28b4bc99e88f",
              "IPY_MODEL_41bdd49fab5f4826959d0d50663ff539",
              "IPY_MODEL_60168d85131d4afc99d55d61ab954ee6"
            ],
            "layout": "IPY_MODEL_9edf898aeeab40dda9b9475395776521"
          }
        },
        "095f680d37a3430fb82d223615662db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b44cb0f8e34446c8dde668a75d3d8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10df31709059484c99f102453d780473": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1160a44dc18e47b0890f70c40eaa7eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11c7f66acc1d45be9517d0addf49331e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "122b1bd1f0e9417a8dcb57d4eebe4d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0c233ad01604540a6c873f4a731982d",
              "IPY_MODEL_e9a01115c75b499884f7e0ef32e9e599",
              "IPY_MODEL_5faba4ad609448b2b49024add4ad3b8e"
            ],
            "layout": "IPY_MODEL_ef25efa751304e4699910f1fbc14345f"
          }
        },
        "158212a630f04cbd884c937f2f60f5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1d54ccd56494c4d831f71b416a1f880",
            "placeholder": "​",
            "style": "IPY_MODEL_530f696feefe499da08c6312047379b2",
            "value": " 20/? [01:43&lt;00:00,  5.25s/it]"
          }
        },
        "23863bc37a8645029934b8c106622c51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2508d229935744cbb5fc340222e2d660": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a0755d4476543feb4a64538e3e37213": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c20b539cd70b4ba99601ad1d69fd9cec",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6d681eeafa44d18b933a4c5dec88382",
            "value": 1
          }
        },
        "33f063017b7c4c7fa8cbafc89674350b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6864c81e2bcf459bbaf5acbb36bdfcbe",
              "IPY_MODEL_59d6e269eadf429a924f6f79bc8ba4ba",
              "IPY_MODEL_ca791fc471e34b9da2f9070fc1053c0f"
            ],
            "layout": "IPY_MODEL_8baf0ed3d0f743f294e07f2b5407e820"
          }
        },
        "3a8537e37fc14fd9b16ca0ceee4fede6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41bdd49fab5f4826959d0d50663ff539": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eb8b2e3262c45248708a2082c366f0a",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_095f680d37a3430fb82d223615662db5",
            "value": 64
          }
        },
        "530f696feefe499da08c6312047379b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59d6e269eadf429a924f6f79bc8ba4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_890e0dd7fa524ceca1e805cb6253ee71",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61b52ff459214129b8f7e6d67b192b78",
            "value": 20
          }
        },
        "5ab5f08afa5841709aedb2f78a52a11c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c2fda99d4204d85b1bf7ad354fd58d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5faba4ad609448b2b49024add4ad3b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_849b5c95008541d49f1ceedf0a59ac60",
            "placeholder": "​",
            "style": "IPY_MODEL_f3665a86662746c4ac7cb0796604781d",
            "value": " 20/? [01:27&lt;00:00,  6.45s/it]"
          }
        },
        "60168d85131d4afc99d55d61ab954ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8537e37fc14fd9b16ca0ceee4fede6",
            "placeholder": "​",
            "style": "IPY_MODEL_1160a44dc18e47b0890f70c40eaa7eb0",
            "value": " 61/64 [00:02&lt;00:00, 23.36it/s]"
          }
        },
        "61b52ff459214129b8f7e6d67b192b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6864c81e2bcf459bbaf5acbb36bdfcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10df31709059484c99f102453d780473",
            "placeholder": "​",
            "style": "IPY_MODEL_2508d229935744cbb5fc340222e2d660",
            "value": "Generating: 100%"
          }
        },
        "6eb8b2e3262c45248708a2082c366f0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7710c7377cbc4c30b55b28b4bc99e88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c2fda99d4204d85b1bf7ad354fd58d4",
            "placeholder": "​",
            "style": "IPY_MODEL_93cd4d35c5fd41f5904ca1d52d1f52a8",
            "value": "embedding nodes:  95%"
          }
        },
        "7cb241365f604419af454c1c28de197a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7dce19ac55264f2b88a0e4730e55867b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddffd834e09940a4bd3874c3f39b4e21",
            "placeholder": "​",
            "style": "IPY_MODEL_ef63c3b2d51e452da03cdae5d9b034be",
            "value": ""
          }
        },
        "849b5c95008541d49f1ceedf0a59ac60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "890e0dd7fa524ceca1e805cb6253ee71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8baf0ed3d0f743f294e07f2b5407e820": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93cd4d35c5fd41f5904ca1d52d1f52a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cf586576ff44dba86ba2eb389593c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9edf898aeeab40dda9b9475395776521": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "a6d681eeafa44d18b933a4c5dec88382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf8dcc0895054529af356da401c513f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dce19ac55264f2b88a0e4730e55867b",
              "IPY_MODEL_2a0755d4476543feb4a64538e3e37213",
              "IPY_MODEL_158212a630f04cbd884c937f2f60f5c8"
            ],
            "layout": "IPY_MODEL_11c7f66acc1d45be9517d0addf49331e"
          }
        },
        "c20b539cd70b4ba99601ad1d69fd9cec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ca791fc471e34b9da2f9070fc1053c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23863bc37a8645029934b8c106622c51",
            "placeholder": "​",
            "style": "IPY_MODEL_5ab5f08afa5841709aedb2f78a52a11c",
            "value": " 20/20 [00:52&lt;00:00,  4.50s/it]"
          }
        },
        "d1d54ccd56494c4d831f71b416a1f880": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddffd834e09940a4bd3874c3f39b4e21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0c233ad01604540a6c873f4a731982d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b44cb0f8e34446c8dde668a75d3d8ad",
            "placeholder": "​",
            "style": "IPY_MODEL_edaac6587b2d4bd5be52b89bb097f99f",
            "value": ""
          }
        },
        "e9a01115c75b499884f7e0ef32e9e599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb241365f604419af454c1c28de197a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cf586576ff44dba86ba2eb389593c61",
            "value": 1
          }
        },
        "edaac6587b2d4bd5be52b89bb097f99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef25efa751304e4699910f1fbc14345f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef63c3b2d51e452da03cdae5d9b034be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3665a86662746c4ac7cb0796604781d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
