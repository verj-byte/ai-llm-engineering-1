{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCk2Rx4cjlYF"
      },
      "source": [
        "# Synthetic Data Generation Using RAGAS - RAG Evaluation with LangSmith\n",
        "\n",
        "In the following notebook we'll explore a use-case for RAGAS' synthetic testset generation workflow!\n",
        "\n",
        "\n",
        "  1. Use RAGAS to Generate Synthetic Data\n",
        "  2. Load them into a LangSmith Dataset\n",
        "  3. Evaluate our RAG chain against the synthetic test data\n",
        "  4. Make changes to our pipeline\n",
        "  5. Evaluate the modified pipeline\n",
        "\n",
        "SDG is a critical piece of the puzzle, especially for early iteration! Without it, it would not be nearly as easy to get high quality early signal for our application's performance.\n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VUI7vF_kbv9"
      },
      "source": [
        "## Task 1: Dependencies and API Keys\n",
        "\n",
        "We'll need to install a number of API keys and dependencies, since we'll be leveraging a number of great technologies for this pipeline!\n",
        "\n",
        "1. OpenAI's endpoints to handle the Synthetic Data Generation\n",
        "2. OpenAI's Endpoints for our RAG pipeline and LangSmith evaluation\n",
        "3. QDrant as our vectorstore\n",
        "4. LangSmith for our evaluation coordinator!\n",
        "\n",
        "Let's install and provide all the required information below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies and API Keys:\n",
        "\n",
        "> NOTE: DO NOT RUN THESE CELLS IF YOU ARE RUNNING THIS NOTEBOOK LOCALLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install -qU ragas==0.2.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install -qU langchain-community==0.3.14 langchain-openai==0.2.14 unstructured==0.16.12 langgraph==0.2.61 langchain-qdrant==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NLTK Import\n",
        "\n",
        "To prevent errors that may occur based on OS - we'll import NLTK and download the needed packages to ensure correct handling of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\DadaV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\DadaV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll also want to set a project name to make things easier for ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Philippines AI Bills RAG - {uuid4().hex[0:8]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OpenAI's API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Synthetic Test Data\n",
        "\n",
        "We wil be using Ragas to build out a set of synthetic test questions, references, and reference contexts. This is useful because it will allow us to find out how our system is performing.\n",
        "\n",
        "> NOTE: Ragas is best suited for finding *directional* changes in your LLM-based systems. The absolute scores aren't comparable in a vacuum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation\n",
        "\n",
        "We'll prepare our data - which should hopefull be familiar at this point since it's our Loan Data use-case!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's load our data into a familiar LangChain format using the `DirectoryLoader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
        "\n",
        "path = \"bills/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Knowledge Graph Based Synthetic Generation\n",
        "\n",
        "Ragas uses a knowledge graph based approach to create data. This is extremely useful as it allows us to create complex queries rather simply. The additional testset complexity allows us to evaluate larger problems more effectively, as systems tend to be very strong on simple evaluation tasks.\n",
        "\n",
        "Let's start by defining our `generator_llm` (which will generate our questions, summaries, and more), and our `generator_embeddings` which will be useful in building our graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unrolled SDG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "# generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we're going to instantiate our Knowledge Graph.\n",
        "\n",
        "This graph will contain N number of nodes that have M number of relationships. These nodes and relationships (AKA \"edges\") will define our knowledge graph and be used later to construct relevant questions and responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 0, relationships: 0)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.graph import KnowledgeGraph\n",
        "\n",
        "kg = KnowledgeGraph()\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step we're going to take is to simply insert each of our full documents into the graph. This will provide a base that we can apply transformations to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 20, relationships: 0)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.graph import Node, NodeType\n",
        "\n",
        "### NOTICE: We're using a subset of the data for this example - this is to keep costs/time down.\n",
        "for doc in docs[:20]:\n",
        "    kg.nodes.append(\n",
        "        Node(\n",
        "            type=NodeType.DOCUMENT,\n",
        "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
        "        )\n",
        "    )\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we'll apply the *default* transformations to our knowledge graph. This will take the nodes currently on the graph and transform them based on a set of [default transformations](https://docs.ragas.io/en/latest/references/transforms/#ragas.testset.transforms.default_transforms).\n",
        "\n",
        "These default transformations are dependent on the corpus length, in our case:\n",
        "\n",
        "- Producing Summaries -> produces summaries of the documents\n",
        "- Extracting Headlines -> finding the overall headline for the document\n",
        "- Theme Extractor -> extracts broad themes about the documents\n",
        "\n",
        "It then uses cosine-similarity and heuristics between the embeddings of the above transformations to construct relationships between the nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "506ceb3d19a84192a9279174ba4454e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa8592635dd84976aa2ff58f9fddcb71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node 1d683277-4947-4e86-92e9-6852810f7514 does not have a summary. Skipping filtering.\n",
            "Node 997e4588-63e7-4bb6-b03f-dc185b06a1ce does not have a summary. Skipping filtering.\n",
            "Node 249c7c95-d982-4e7d-83f1-9496dad1496f does not have a summary. Skipping filtering.\n",
            "Node 26160b66-9a9f-4915-bd6d-ca424ad30ad2 does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0ba66d9621f466d97c070d511b12704",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/56 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9c4a932efeb462bbd916af95336e0b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 20, relationships: 137)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.transforms import default_transforms, apply_transforms\n",
        "\n",
        "transformer_llm = generator_llm\n",
        "embedding_model = generator_embeddings\n",
        "\n",
        "default_transforms = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n",
        "apply_transforms(kg, default_transforms)\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can save and load our knowledge graphs as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 20, relationships: 137)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kg.save(\"bills/ai_law.json\")\n",
        "bills_data_kg = KnowledgeGraph.load(\"bills/ai_law.json\")\n",
        "bills_data_kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using our knowledge graph, we can construct a \"test set generator\" - which will allow us to create queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=embedding_model, knowledge_graph=bills_data_kg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, we'd like to be able to define the kinds of queries we're generating - which is made simple by Ragas having pre-created a number of different \"QuerySynthesizer\"s.\n",
        "\n",
        "Each of these Synthetsizers is going to tackle a separate kind of query which will be generated from a scenario and a persona.\n",
        "\n",
        "In essence, Ragas will use an LLM to generate a persona of someone who would interact with the data - and then use a scenario to construct a question from that data and persona."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset.synthesizers import default_query_distribution, SingleHopSpecificQuerySynthesizer, MultiHopAbstractQuerySynthesizer, MultiHopSpecificQuerySynthesizer\n",
        "\n",
        "query_distribution = [\n",
        "        (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 0.5),\n",
        "        (MultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.25),\n",
        "        (MultiHopSpecificQuerySynthesizer(llm=generator_llm), 0.25),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can use our `TestSetGenerator` to generate our testset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e78d57e595c4e0b8a1c94b3a3deb1e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08bba3a596c740c1a8febf9468395218",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f1970c74adf41f4b8f9ed233d28ed66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "testset = generator.generate(testset_size=20, query_distribution=query_distribution)\n",
        "testset.to_pandas()\n",
        "testset.to_jsonl(\"bills/golden_dataset.json\")  # Save for reuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Abstracted SDG\n",
        "\n",
        "The above method is the full process - but we can shortcut that using the provided abstractions!\n",
        "\n",
        "This will generate our knowledge graph under the hood, and will - from there - generate our personas and scenarios to construct our queries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bd2bdc9b94b44e99db798919d2fd4f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bce8a79eceb4635a6228dde0326983e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node 91d45b63-134f-4a2c-8b5f-42646c369283 does not have a summary. Skipping filtering.\n",
            "Node fe0b4b07-9395-4043-9735-35520124fe06 does not have a summary. Skipping filtering.\n",
            "Node 3eb8450b-1d07-4756-9bac-c59c58e67535 does not have a summary. Skipping filtering.\n",
            "Node f7a87d43-4c37-46ff-a937-f55f3e4a18d1 does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dec6dc0af59f4afdb08c9942e6c80025",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/56 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a0307e0e682452bbda8f571d6ae960e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c0a920ffcb64224950612642e799437",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "299ea6892e3e4d88ba2f5fcac230e629",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94db7bbbf06b410bbc87222507b8a2ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs[:20], testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Who is PIA S. CAYETANO in the context of the P...</td>\n",
              "      <td>[TWENTIETH CONGRESS OF THE \\nREPUBLIC OF THE P...</td>\n",
              "      <td>PIA S. CAYETANO is the senator who introduced ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why Georgetown University is important for AI ...</td>\n",
              "      <td>[AI presents enormous opportunities for the Ph...</td>\n",
              "      <td>The context mentions Georgetown University in ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the significance of the REPUBLIC OF TH...</td>\n",
              "      <td>[TWENTIETH CONGRESS OF THE \\nREPUBLIC OF THE P...</td>\n",
              "      <td>The context discusses the TWENTIETH CONGRESS O...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AI is like what do it do for Philippines?</td>\n",
              "      <td>[1 \\na) Promote innovation, technological adva...</td>\n",
              "      <td>AI refers to systems that allow machines to th...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does the creation of an AI Ethics Review B...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nAI presents enormous opportunities...</td>\n",
              "      <td>The creation of an AI Ethics Review Board, as ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How do the registration and licensing requirem...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n1 \\nground its responses in verifi...</td>\n",
              "      <td>The Philippine AI regulations specify that any...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How do coordnaton with gov agencies and mainte...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n1 \\nSec. 8. NAICSecretariat. - The...</td>\n",
              "      <td>Coordnaton with gov agencies, local units, and...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How does the Philippines' AI regulation framew...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nAI presents enormous opportunities...</td>\n",
              "      <td>The Philippines' AI regulation framework aims ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does DepEd relate to AI regulation under t...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n1 \\nSec. 6. Jurisdiction of the NA...</td>\n",
              "      <td>DepEd is listed as one of the government agenc...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does the bill address ASI risks and promot...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nAI presents enormous opportunities...</td>\n",
              "      <td>The bill recognizes the potential rise of Arti...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How does the Department of Science and Technol...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n1 \\nground its responses in verifi...</td>\n",
              "      <td>The Department of Science and Technology (DOST...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>How does the NAIC's regulation of AI in employ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n1 \\nd) Take full responsibility fo...</td>\n",
              "      <td>The NAIC's regulation of AI in employment and ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   Who is PIA S. CAYETANO in the context of the P...   \n",
              "1   Why Georgetown University is important for AI ...   \n",
              "2   What is the significance of the REPUBLIC OF TH...   \n",
              "3           AI is like what do it do for Philippines?   \n",
              "4   How does the creation of an AI Ethics Review B...   \n",
              "5   How do the registration and licensing requirem...   \n",
              "6   How do coordnaton with gov agencies and mainte...   \n",
              "7   How does the Philippines' AI regulation framew...   \n",
              "8   How does DepEd relate to AI regulation under t...   \n",
              "9   How does the bill address ASI risks and promot...   \n",
              "10  How does the Department of Science and Technol...   \n",
              "11  How does the NAIC's regulation of AI in employ...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [TWENTIETH CONGRESS OF THE \\nREPUBLIC OF THE P...   \n",
              "1   [AI presents enormous opportunities for the Ph...   \n",
              "2   [TWENTIETH CONGRESS OF THE \\nREPUBLIC OF THE P...   \n",
              "3   [1 \\na) Promote innovation, technological adva...   \n",
              "4   [<1-hop>\\n\\nAI presents enormous opportunities...   \n",
              "5   [<1-hop>\\n\\n1 \\nground its responses in verifi...   \n",
              "6   [<1-hop>\\n\\n1 \\nSec. 8. NAICSecretariat. - The...   \n",
              "7   [<1-hop>\\n\\nAI presents enormous opportunities...   \n",
              "8   [<1-hop>\\n\\n1 \\nSec. 6. Jurisdiction of the NA...   \n",
              "9   [<1-hop>\\n\\nAI presents enormous opportunities...   \n",
              "10  [<1-hop>\\n\\n1 \\nground its responses in verifi...   \n",
              "11  [<1-hop>\\n\\n1 \\nd) Take full responsibility fo...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   PIA S. CAYETANO is the senator who introduced ...   \n",
              "1   The context mentions Georgetown University in ...   \n",
              "2   The context discusses the TWENTIETH CONGRESS O...   \n",
              "3   AI refers to systems that allow machines to th...   \n",
              "4   The creation of an AI Ethics Review Board, as ...   \n",
              "5   The Philippine AI regulations specify that any...   \n",
              "6   Coordnaton with gov agencies, local units, and...   \n",
              "7   The Philippines' AI regulation framework aims ...   \n",
              "8   DepEd is listed as one of the government agenc...   \n",
              "9   The bill recognizes the potential rise of Arti...   \n",
              "10  The Department of Science and Technology (DOST...   \n",
              "11  The NAIC's regulation of AI in employment and ...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vSRr2MXk0P_"
      },
      "source": [
        "We'll need to provide our LangSmith API key, and set tracing to \"true\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SLtk1GtnyoY"
      },
      "source": [
        "## Task 4: LangSmith Dataset\n",
        "\n",
        "Now we can move on to creating a dataset for LangSmith!\n",
        "\n",
        "First, we'll need to create a dataset on LangSmith using the `Client`!\n",
        "\n",
        "We'll name our Dataset to make it easy to work with later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TLgm6OjvYSsm"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"Philippines AI Bills x5\"\n",
        "\n",
        "langsmith_dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Philippines AI Bills\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64SmXMBnzXWm"
      },
      "source": [
        "We'll iterate through the RAGAS created dataframe - and add each example to our created dataset!\n",
        "\n",
        "> NOTE: We need to conform the outputs to the expected format - which in this case is: `question` and `answer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8nFQ6di_XnY7"
      },
      "outputs": [],
      "source": [
        "for data_row in dataset.to_pandas().iterrows():\n",
        "  client.create_example(\n",
        "      inputs={\n",
        "          \"question\": data_row[1][\"user_input\"]\n",
        "      },\n",
        "      outputs={\n",
        "          \"answer\": data_row[1][\"reference\"]\n",
        "      },\n",
        "      metadata={\n",
        "          \"context\": data_row[1][\"reference_contexts\"]\n",
        "      },\n",
        "      dataset_id=langsmith_dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6EbQVyZq-2j"
      },
      "source": [
        "## Basic RAG Chain\n",
        "\n",
        "Time for some RAG!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4njbUAIsaYjB"
      },
      "outputs": [],
      "source": [
        "rag_documents = docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQorBy8H1AZR"
      },
      "source": [
        "To keep things simple, we'll just use LangChain's recursive character text splitter!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "qWo3Ajaragv1"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "rag_documents = text_splitter.split_documents(rag_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kghuTb9R01oO"
      },
      "source": [
        "We'll create our vectorstore using OpenAI's [`text-embedding-3-small`](https://platform.openai.com/docs/guides/embeddings/embedding-models) embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "UwfJCzP3aqKI"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the \"A\" in RAG, we'll provide a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "You are a helpful and informative assistant. Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "If you cannot answer the question based on the context, or you are unsure, you must say \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For our LLM, we will be using TogetherAI's endpoints as well!\n",
        "\n",
        "We're going to be using Meta Llama 3.1 70B Instruct Turbo - a powerful model which should get us powerful results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As usual, we will power our RAG application with Qdrant!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")\n",
        "\n",
        "semantic_documents = semantic_chunker.split_documents(rag_documents[:20])\n",
        "\n",
        "# semantic chunking OFF - use rag_documents, else semantic_documents\n",
        "\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "text_vectorstore = Qdrant.from_documents(\n",
        "    documents=rag_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"AI Bills RAG\")\n",
        "\n",
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    documents=semantic_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"AI Bills RAG\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangSmith Evaluation Set-up\n",
        "\n",
        "We'll use OpenAI's GPT-4.1 as our evaluation LLM for our base Evaluators.\n",
        "\n",
        "We'll be using a number of evaluators - from LangSmith provided evaluators, to a few custom evaluators!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_llm = ChatOpenAI(model=\"gpt-4.1\")\n",
        "\n",
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\n",
        "    \"llm\" : eval_llm\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": run.outputs[\"result\"],      # student's answer (model output)\n",
        "        \"reference\": example.outputs[\"answer\"],   # gold / true answer\n",
        "        \"input\": example.inputs[\"query\"],         # the question\n",
        "    }\n",
        "    )\n",
        "\n",
        "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"helpfulness\": (\n",
        "                \"Is this submission helpful to the user,\"\n",
        "                \" taking into account the correct reference answer?\"\n",
        "            )\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": run.outputs[\"result\"],\n",
        "        \"reference\": example.outputs[\"answer\"],\n",
        "        \"input\": example.inputs[\"question\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "empathy_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"empathy\": \"Is this response empathetic? Does it make the user feel like they are being heard?\",\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbKSjfSkbTYo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'bold-peace-15' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/4f35f17d-671b-460c-9fd0-ae389801030a/compare?selectedSessions=c4bf537c-6c37-4149-8046-d3a09d7c4ab8\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "682d8a9f1752441ba9d6f62d6f89ed68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1a4b593d-03a3-46f8-9b74-3c552f633759: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1a4b593d-03a3-46f8-9b74-3c552f633759: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run db994c84-5b91-4aba-8d05-0eeebbabb54f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run db994c84-5b91-4aba-8d05-0eeebbabb54f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a3ecac62-e5bb-463c-8542-be14442982fc: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a3ecac62-e5bb-463c-8542-be14442982fc: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 591d1e30-88bf-4f07-8176-6485a6656538: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 591d1e30-88bf-4f07-8176-6485a6656538: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d5224d97-8636-4369-a03c-9d1eadc31ded: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d5224d97-8636-4369-a03c-9d1eadc31ded: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 16599bd6-ae20-48d3-a804-780466863146: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 16599bd6-ae20-48d3-a804-780466863146: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ea77215b-44c3-4ec7-95e2-8ddad3e4bb6b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ea77215b-44c3-4ec7-95e2-8ddad3e4bb6b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d9380302-2757-4298-8343-3723ccf213d9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d9380302-2757-4298-8343-3723ccf213d9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 44e5c0db-261e-4336-befa-6e010b85468f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 44e5c0db-261e-4336-befa-6e010b85468f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 18d300f9-7893-4b42-99b2-8adefcb7131b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 18d300f9-7893-4b42-99b2-8adefcb7131b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2a037b29-c28c-4cc5-9023-39afb746d672: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2a037b29-c28c-4cc5-9023-39afb746d672: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run affb1bd5-4dcd-4b52-849b-37d274980d42: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run affb1bd5-4dcd-4b52-849b-37d274980d42: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'new-root-96' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/4f35f17d-671b-460c-9fd0-ae389801030a/compare?selectedSessions=5e1912ac-6a60-4505-bdeb-a423df366be1\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b01fb895f534ebb9ab60dc73e769564",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3b89702f-9fe5-4da7-aa9e-6fdcb78e7ff9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3b89702f-9fe5-4da7-aa9e-6fdcb78e7ff9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a625dbbf-5739-4a97-86c4-e92916c5aec9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a625dbbf-5739-4a97-86c4-e92916c5aec9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bc8cebd6-3798-4887-911a-3d8859125eaa: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bc8cebd6-3798-4887-911a-3d8859125eaa: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 90de4866-3339-4a2c-9da6-643385b39c3c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 90de4866-3339-4a2c-9da6-643385b39c3c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 92d5c882-4489-4f42-9bbc-eb3447e653e2: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 92d5c882-4489-4f42-9bbc-eb3447e653e2: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d25bf3e8-6327-41e1-8c2b-61ad78e5fa5e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d25bf3e8-6327-41e1-8c2b-61ad78e5fa5e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5d71843c-a668-47ed-9e3e-182c70317d62: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5d71843c-a668-47ed-9e3e-182c70317d62: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 57e8a348-d419-49c4-9138-bc15a0dc99c4: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 57e8a348-d419-49c4-9138-bc15a0dc99c4: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 04171b27-659c-4513-acb2-30e3250f86b1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 04171b27-659c-4513-acb2-30e3250f86b1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e984b77a-96d5-46d4-8e7e-1a46d1765d37: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e984b77a-96d5-46d4-8e7e-1a46d1765d37: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9ebac424-934d-4249-b803-d7d9398dcd6f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9ebac424-934d-4249-b803-d7d9398dcd6f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 78a4c095-5762-4876-b2aa-99190af48528: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 78a4c095-5762-4876-b2aa-99190af48528: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'drab-nut-90' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/4f35f17d-671b-460c-9fd0-ae389801030a/compare?selectedSessions=e1ec8a3d-03b6-48d3-b1d1-beb4d5c2ba83\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "450a0270756b4e71ad8f46cac6265ca6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run afb48638-bb71-46bb-8d56-10b9b7d978e5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run afb48638-bb71-46bb-8d56-10b9b7d978e5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1ee046d0-75cc-464b-9660-3270af8889e9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1ee046d0-75cc-464b-9660-3270af8889e9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d32ec193-86da-4c22-8e22-4cb64fb988be: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d32ec193-86da-4c22-8e22-4cb64fb988be: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cca03ffe-86fd-4428-9a91-5fe1e6855596: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cca03ffe-86fd-4428-9a91-5fe1e6855596: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e56c9d3c-0113-4db0-9414-11a3db439f74: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e56c9d3c-0113-4db0-9414-11a3db439f74: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d7b29b4c-6738-44f6-a4c7-44883d50b7c5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d7b29b4c-6738-44f6-a4c7-44883d50b7c5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 716396c2-8b30-4f07-8902-dcd6b62d429c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 716396c2-8b30-4f07-8902-dcd6b62d429c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f90b50ae-e1ed-451d-845b-5b5ec3af8241: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f90b50ae-e1ed-451d-845b-5b5ec3af8241: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 17197044-43d7-4d22-98e6-7fa2c906a642: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 17197044-43d7-4d22-98e6-7fa2c906a642: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ed0f1cbb-4e78-4eb6-9cbd-d8391ccd0958: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ed0f1cbb-4e78-4eb6-9cbd-d8391ccd0958: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e6cf8384-3d2d-4429-9a12-9668474ce984: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e6cf8384-3d2d-4429-9a12-9668474ce984: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 83007c7c-ccb8-4b3b-a781-8c4545049c83: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 83007c7c-ccb8-4b3b-a781-8c4545049c83: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'loyal-invention-66' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/4f35f17d-671b-460c-9fd0-ae389801030a/compare?selectedSessions=70dcf428-5210-482d-875b-3acf28062187\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d215c3c30dee43cb87d08d77242cb7dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0bde4a67-09ab-4d52-b439-4a622a79082e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0bde4a67-09ab-4d52-b439-4a622a79082e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ed471a2c-5c9f-4748-a04d-c4df1034cd6e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ed471a2c-5c9f-4748-a04d-c4df1034cd6e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7b9df37b-9e16-4c93-a0fb-b415501f132b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7b9df37b-9e16-4c93-a0fb-b415501f132b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fe5c2611-bf44-4f0d-ae10-d1fd918659e3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fe5c2611-bf44-4f0d-ae10-d1fd918659e3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3894e201-78e3-40da-89b3-2651faa4ff51: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3894e201-78e3-40da-89b3-2651faa4ff51: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d8f40f16-c065-4da2-9d06-8683e6fdb487: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d8f40f16-c065-4da2-9d06-8683e6fdb487: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5d87cf14-af61-493e-a726-1f8099fc357b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5d87cf14-af61-493e-a726-1f8099fc357b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 742b0553-0297-4598-9d5d-9f08267c9af7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 742b0553-0297-4598-9d5d-9f08267c9af7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3010d4fa-ec3c-4ba9-b0a1-8815131cd173: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3010d4fa-ec3c-4ba9-b0a1-8815131cd173: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run faee7e68-a2ee-491b-ad27-83e1877c2298: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run faee7e68-a2ee-491b-ad27-83e1877c2298: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 212b59c7-7739-416b-9be0-b4a3f2595dc5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 212b59c7-7739-416b-9be0-b4a3f2595dc5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c939114a-fa17-48d7-9908-f56210192e3c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c939114a-fa17-48d7-9908-f56210192e3c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'dear-stomach-69' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/4f35f17d-671b-460c-9fd0-ae389801030a/compare?selectedSessions=f0d49c8f-370f-4471-aef9-82986b9946c8\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60ec96c174ad4c09a1d289b641d39416",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e131ec2e-a21a-4711-b89a-62e50259a2ad: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e131ec2e-a21a-4711-b89a-62e50259a2ad: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b9915879-b0c2-4233-b486-e543af12ce36: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b9915879-b0c2-4233-b486-e543af12ce36: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 44706c70-35d3-4a83-948f-4f2e80a60671: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 44706c70-35d3-4a83-948f-4f2e80a60671: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 400dadea-648b-4433-8e1c-e89710020353: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 400dadea-648b-4433-8e1c-e89710020353: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 303e4e0c-9df0-41b1-ab6b-00e5ba55e685: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 303e4e0c-9df0-41b1-ab6b-00e5ba55e685: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a9326698-0192-4edd-83ea-a3750fd9cea1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a9326698-0192-4edd-83ea-a3750fd9cea1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a6f1f519-3b55-4193-8ea7-7e24052b80ae: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a6f1f519-3b55-4193-8ea7-7e24052b80ae: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 220067e2-76b4-41c8-a3e5-ef62cdcc1eb4: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 220067e2-76b4-41c8-a3e5-ef62cdcc1eb4: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 16fecd10-b8ec-49fd-8692-a9c1eaba80bb: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 16fecd10-b8ec-49fd-8692-a9c1eaba80bb: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running target function: status_code: 429, body: data=None id='3a6868b1-3d9d-44c2-8abf-6272e900cc75' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1924, in _forward\n",
            "    fn(*args, langsmith_extra=langsmith_extra)\n",
            "    ~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3047, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3777, in invoke\n",
            "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
            "                   ~~~~~~~~~~~~~^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py\", line 456, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\concurrent\\futures\\thread.py\", line 59, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3761, in _invoke_step\n",
            "    return context.run(\n",
            "           ~~~~~~~~~~~^\n",
            "        step.invoke,\n",
            "        ^^^^^^^^^^^^\n",
            "        input_,\n",
            "        ^^^^^^^\n",
            "        child_config,\n",
            "        ^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3049, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\retrievers.py\", line 261, in invoke\n",
            "    result = self._get_relevant_documents(\n",
            "        input, run_manager=run_manager, **kwargs_\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain\\retrievers\\contextual_compression.py\", line 46, in _get_relevant_documents\n",
            "    compressed_docs = self.base_compressor.compress_documents(\n",
            "        docs,\n",
            "        query,\n",
            "        callbacks=run_manager.get_child(),\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_cohere\\rerank.py\", line 151, in compress_documents\n",
            "    for res in self.rerank(documents, query):\n",
            "               ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_cohere\\rerank.py\", line 119, in rerank\n",
            "    results = self.client.rerank(\n",
            "        query=query,\n",
            "    ...<3 lines>...\n",
            "        max_tokens_per_doc=max_tokens_per_doc,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\cohere\\v2\\client.py\", line 1065, in rerank\n",
            "    raise TooManyRequestsError(\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "cohere.errors.too_many_requests_error.TooManyRequestsError: status_code: 429, body: data=None id='3a6868b1-3d9d-44c2-8abf-6272e900cc75' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cd5a9952-14d2-40ab-8bda-53f046531a50: KeyError('response')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28456\\1503600772.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"response\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'response'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cec9fc9d-ee94-4596-aecb-681c340b8314: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cec9fc9d-ee94-4596-aecb-681c340b8314: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bb0303ec-4c0b-49a5-a5a0-78079cf37b95: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bb0303ec-4c0b-49a5-a5a0-78079cf37b95: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'roasted-pleasure-32' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/4f35f17d-671b-460c-9fd0-ae389801030a/compare?selectedSessions=916a0593-126f-4e24-be10-920656b1a272\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9321a0aec35e41e99ed0cd6b8ccbcdfe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d6daa772-8be2-4af7-83ae-4338967a4b8b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d6daa772-8be2-4af7-83ae-4338967a4b8b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c74e0a57-4348-41cd-9ee4-d8199552906d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c74e0a57-4348-41cd-9ee4-d8199552906d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7e8044e9-c0e0-4f2b-a458-ec707bda89b3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7e8044e9-c0e0-4f2b-a458-ec707bda89b3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 031ea59c-4366-437c-a497-d605ce368f6a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 031ea59c-4366-437c-a497-d605ce368f6a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6b7558c3-7d04-4f41-9894-d243a50cab7f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6b7558c3-7d04-4f41-9894-d243a50cab7f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b1e0549b-7263-43ac-8bba-223e5a35050a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b1e0549b-7263-43ac-8bba-223e5a35050a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 48d8c5be-f7b8-4f1e-8ce9-8d1176638f39: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 48d8c5be-f7b8-4f1e-8ce9-8d1176638f39: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bc252985-b93c-4b42-87b0-6e9440a7edf2: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bc252985-b93c-4b42-87b0-6e9440a7edf2: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run eb403605-32b2-4bc8-98f5-bad240ad52d5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run eb403605-32b2-4bc8-98f5-bad240ad52d5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 23fcdd51-37b0-4872-95e6-0b749f45d2d9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 23fcdd51-37b0-4872-95e6-0b749f45d2d9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fefc1639-fa57-428b-bad6-0f7d5802eb8a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fefc1639-fa57-428b-bad6-0f7d5802eb8a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d31c61b2-86ef-4390-9522-609760b33067: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d31c61b2-86ef-4390-9522-609760b33067: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'virtual-coffee-57' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/4f35f17d-671b-460c-9fd0-ae389801030a/compare?selectedSessions=de8c26bc-9c6e-4b0a-bbbc-1f31e71ef2e4\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f92d463f07c541eb8e93b80676bb7008",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e4a4ab13-7b4d-4356-939b-42d22de6d0c5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e4a4ab13-7b4d-4356-939b-42d22de6d0c5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fd677da6-6460-42d6-b56b-c600468e19bd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fd677da6-6460-42d6-b56b-c600468e19bd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ba353eba-e9b6-468d-917d-44f2665eeb18: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ba353eba-e9b6-468d-917d-44f2665eeb18: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e6cc138d-5550-4d3b-aa7c-1c34ca6af77d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e6cc138d-5550-4d3b-aa7c-1c34ca6af77d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a7269aed-91bd-47ca-855a-b300732eed75: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a7269aed-91bd-47ca-855a-b300732eed75: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cd4a4c8a-fc4b-4aa8-81cf-c0b675ddfd84: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cd4a4c8a-fc4b-4aa8-81cf-c0b675ddfd84: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3831f825-8db1-4059-b3d7-c4ef92fd422e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3831f825-8db1-4059-b3d7-c4ef92fd422e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d1044b72-6fa6-4c5d-80fc-033f22bfe15a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d1044b72-6fa6-4c5d-80fc-033f22bfe15a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 03488939-6bfb-44d4-9ca8-81b170878b52: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 03488939-6bfb-44d4-9ca8-81b170878b52: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bdae79d6-6b70-4ea1-a44d-78513a5eeb33: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bdae79d6-6b70-4ea1-a44d-78513a5eeb33: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 97727066-a427-4fd4-8dfd-1bb1e151b0df: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 97727066-a427-4fd4-8dfd-1bb1e151b0df: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1718be1a-1167-4827-90bc-1050976ba383: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1718be1a-1167-4827-90bc-1050976ba383: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'elderly-acknowledgment-44' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/4f35f17d-671b-460c-9fd0-ae389801030a/compare?selectedSessions=0cb85e1b-fcfd-4369-92df-fb650eb7cf2e\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae22072363bf412f8d5d923b2b81f074",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0ff8ca69-b52c-49d1-a3a3-08eb73f3b8e4: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0ff8ca69-b52c-49d1-a3a3-08eb73f3b8e4: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4d03b3f2-9a97-411f-93cc-48c2a3064a85: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4d03b3f2-9a97-411f-93cc-48c2a3064a85: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c1b2f0e0-151a-4644-a790-f284d5c7a638: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c1b2f0e0-151a-4644-a790-f284d5c7a638: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 221c344e-4764-4f05-a3c1-1c1a4457d691: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 221c344e-4764-4f05-a3c1-1c1a4457d691: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bdd10cd9-a819-442c-a06e-fffc27b90a4a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bdd10cd9-a819-442c-a06e-fffc27b90a4a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4a6a51e2-a66c-4fc4-b7d1-d232b0555c49: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4a6a51e2-a66c-4fc4-b7d1-d232b0555c49: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d3367bee-9840-41cc-815d-4bef54137a03: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d3367bee-9840-41cc-815d-4bef54137a03: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 125fa18e-fb87-4401-99e5-82d401c60525: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 125fa18e-fb87-4401-99e5-82d401c60525: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ebab4823-b264-446c-8d8d-8418be51e5dd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ebab4823-b264-446c-8d8d-8418be51e5dd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6089264a-a7b1-4f3b-a446-3b5b5d07cf9f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6089264a-a7b1-4f3b-a446-3b5b5d07cf9f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a82e5bd4-a0b8-4189-81b9-a96cd0221a50: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a82e5bd4-a0b8-4189-81b9-a96cd0221a50: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0cdda320-ebb2-4e97-9372-599d1d3a786b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0cdda320-ebb2-4e97-9372-599d1d3a786b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'enchanted-town-24' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/4f35f17d-671b-460c-9fd0-ae389801030a/compare?selectedSessions=e6d46e6a-a1f9-41a0-87c8-f9e67e320f7f\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a5c8a5398e14648886ddde98b4c356e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 580a7cec-0518-4617-8d86-304e22803a60: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 580a7cec-0518-4617-8d86-304e22803a60: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9b739883-9fb4-4a9a-9471-cffe7a1d87b7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9b739883-9fb4-4a9a-9471-cffe7a1d87b7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 09b2c9fe-9eb3-4afa-bd11-ced7489eb347: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 09b2c9fe-9eb3-4afa-bd11-ced7489eb347: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fd757b13-0af5-4efc-9814-0787dc5f2ea7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fd757b13-0af5-4efc-9814-0787dc5f2ea7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 10a36821-9033-436d-b57b-83d99e7a8087: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 10a36821-9033-436d-b57b-83d99e7a8087: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 712d454f-2899-4a6e-8297-58b3e83fa5f3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 712d454f-2899-4a6e-8297-58b3e83fa5f3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 708e4822-2129-42fa-9d60-4b463c1b5ca3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 708e4822-2129-42fa-9d60-4b463c1b5ca3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2145eb42-e0c8-4d46-b636-dd7e7ca6ca30: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2145eb42-e0c8-4d46-b636-dd7e7ca6ca30: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8c5a47fb-8652-48b2-8f9e-c38cc3a0947b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8c5a47fb-8652-48b2-8f9e-c38cc3a0947b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1dbea5ab-9c7e-4323-8239-ea7fc2edd509: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1dbea5ab-9c7e-4323-8239-ea7fc2edd509: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 06a50fc5-9bbe-43eb-ac33-ea78077cb92e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 06a50fc5-9bbe-43eb-ac33-ea78077cb92e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7643456e-acda-4e49-a34f-3df5b7cfb617: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7643456e-acda-4e49-a34f-3df5b7cfb617: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'complicated-rabbit-64' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/4f35f17d-671b-460c-9fd0-ae389801030a/compare?selectedSessions=7e594197-ab67-4793-bb20-90ff7d3d111f\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f773c7d5a8b48df985b413f4c38ce86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 22bb948c-e4a9-460d-84fd-1aac32a062fc: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 22bb948c-e4a9-460d-84fd-1aac32a062fc: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3faf8230-e2ac-47df-be7c-c8c4e1e0df92: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3faf8230-e2ac-47df-be7c-c8c4e1e0df92: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c04f5eef-ff51-4980-97bd-b6d2952b296d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c04f5eef-ff51-4980-97bd-b6d2952b296d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3983f8e6-7eb4-495e-909c-ed0466842e19: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3983f8e6-7eb4-495e-909c-ed0466842e19: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5abbab02-2fcd-4367-a656-55b024e256d6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5abbab02-2fcd-4367-a656-55b024e256d6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5925d735-74ce-49a1-9ee0-951af15ef65f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5925d735-74ce-49a1-9ee0-951af15ef65f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d6362c41-39ce-40e8-98d1-640dba9507c2: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d6362c41-39ce-40e8-98d1-640dba9507c2: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c9c121e7-4c11-42fd-bee8-d8216c3be789: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c9c121e7-4c11-42fd-bee8-d8216c3be789: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f63e8cbb-27e7-4571-a15a-2765f229626d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f63e8cbb-27e7-4571-a15a-2765f229626d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c29925f3-7b1a-4dd1-90b3-9a392e861f1b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c29925f3-7b1a-4dd1-90b3-9a392e861f1b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6cd10dca-0b3e-4d60-864f-b7afd1e2d594: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6cd10dca-0b3e-4d60-864f-b7afd1e2d594: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ef76b9e5-9aaa-486a-983f-717e183ba570: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ef76b9e5-9aaa-486a-983f-717e183ba570: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'crushing-team-83' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/4f35f17d-671b-460c-9fd0-ae389801030a/compare?selectedSessions=6da8d800-14fd-4aac-818f-570445b333cf\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bc1b72d209c477680b1be5d4a7920f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5567d2f2-59f2-4bc3-bf07-a17bceeeec8a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5567d2f2-59f2-4bc3-bf07-a17bceeeec8a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0846c2dd-6897-417b-9649-3a13203388d1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0846c2dd-6897-417b-9649-3a13203388d1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 86da3b44-2189-4b05-a0e6-0e22179a681a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 86da3b44-2189-4b05-a0e6-0e22179a681a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d7a69a31-7c3f-4b36-9c5e-5a5bbba84a44: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d7a69a31-7c3f-4b36-9c5e-5a5bbba84a44: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bce989b6-a502-487b-b087-27ce1cb8fb45: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bce989b6-a502-487b-b087-27ce1cb8fb45: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 178234b8-32af-4fb7-8420-a177e6ee03f1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 178234b8-32af-4fb7-8420-a177e6ee03f1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9e761450-450e-4ce7-8db3-aba3b8267188: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9e761450-450e-4ce7-8db3-aba3b8267188: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d41952ed-612d-49df-8e58-c58e48c25901: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d41952ed-612d-49df-8e58-c58e48c25901: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 674c1d99-f6e7-44e4-a5a1-5ca99810fa2c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 674c1d99-f6e7-44e4-a5a1-5ca99810fa2c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running target function: status_code: 429, body: data=None id='6f96a984-c5cf-45fb-a8a7-7afcde56066e' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1924, in _forward\n",
            "    fn(*args, langsmith_extra=langsmith_extra)\n",
            "    ~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3047, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3777, in invoke\n",
            "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
            "                   ~~~~~~~~~~~~~^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py\", line 456, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\concurrent\\futures\\thread.py\", line 59, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3761, in _invoke_step\n",
            "    return context.run(\n",
            "           ~~~~~~~~~~~^\n",
            "        step.invoke,\n",
            "        ^^^^^^^^^^^^\n",
            "        input_,\n",
            "        ^^^^^^^\n",
            "        child_config,\n",
            "        ^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3049, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\retrievers.py\", line 261, in invoke\n",
            "    result = self._get_relevant_documents(\n",
            "        input, run_manager=run_manager, **kwargs_\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain\\retrievers\\contextual_compression.py\", line 46, in _get_relevant_documents\n",
            "    compressed_docs = self.base_compressor.compress_documents(\n",
            "        docs,\n",
            "        query,\n",
            "        callbacks=run_manager.get_child(),\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_cohere\\rerank.py\", line 151, in compress_documents\n",
            "    for res in self.rerank(documents, query):\n",
            "               ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_cohere\\rerank.py\", line 119, in rerank\n",
            "    results = self.client.rerank(\n",
            "        query=query,\n",
            "    ...<3 lines>...\n",
            "        max_tokens_per_doc=max_tokens_per_doc,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\cohere\\v2\\client.py\", line 1065, in rerank\n",
            "    raise TooManyRequestsError(\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "cohere.errors.too_many_requests_error.TooManyRequestsError: status_code: 429, body: data=None id='6f96a984-c5cf-45fb-a8a7-7afcde56066e' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cdb92dfe-004b-4333-924e-e1d56f16c11e: KeyError('response')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28456\\1503600772.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"response\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'response'\n",
            "Error running target function: status_code: 429, body: data=None id='996ee2ac-18c5-42f8-a7a2-9a0ce98d2d39' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1924, in _forward\n",
            "    fn(*args, langsmith_extra=langsmith_extra)\n",
            "    ~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3047, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3777, in invoke\n",
            "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
            "                   ~~~~~~~~~~~~~^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py\", line 456, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\concurrent\\futures\\thread.py\", line 59, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3761, in _invoke_step\n",
            "    return context.run(\n",
            "           ~~~~~~~~~~~^\n",
            "        step.invoke,\n",
            "        ^^^^^^^^^^^^\n",
            "        input_,\n",
            "        ^^^^^^^\n",
            "        child_config,\n",
            "        ^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3049, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_core\\retrievers.py\", line 261, in invoke\n",
            "    result = self._get_relevant_documents(\n",
            "        input, run_manager=run_manager, **kwargs_\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain\\retrievers\\contextual_compression.py\", line 46, in _get_relevant_documents\n",
            "    compressed_docs = self.base_compressor.compress_documents(\n",
            "        docs,\n",
            "        query,\n",
            "        callbacks=run_manager.get_child(),\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_cohere\\rerank.py\", line 151, in compress_documents\n",
            "    for res in self.rerank(documents, query):\n",
            "               ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langchain_cohere\\rerank.py\", line 119, in rerank\n",
            "    results = self.client.rerank(\n",
            "        query=query,\n",
            "    ...<3 lines>...\n",
            "        max_tokens_per_doc=max_tokens_per_doc,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\cohere\\v2\\client.py\", line 1065, in rerank\n",
            "    raise TooManyRequestsError(\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "cohere.errors.too_many_requests_error.TooManyRequestsError: status_code: 429, body: data=None id='996ee2ac-18c5-42f8-a7a2-9a0ce98d2d39' message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1df9696f-05aa-490c-8fac-de7f84c25965: KeyError('response')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\DadaV\\AppData\\Local\\Temp\\ipykernel_28456\\1503600772.py\", line 19, in <lambda>\n",
            "    \"prediction\": run.outputs[\"response\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'response'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 37f70b18-b72e-4f86-8aaa-d0db98cdde1f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 37f70b18-b72e-4f86-8aaa-d0db98cdde1f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'ample-animal-5' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/4f35f17d-671b-460c-9fd0-ae389801030a/compare?selectedSessions=b26a64cd-90ef-4bfb-ae4f-f1948349bb81\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0313c88f00e4c59a30d2aba6dc71e8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ff689085-d4b3-4fa0-b2c2-e8de30e94170: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ff689085-d4b3-4fa0-b2c2-e8de30e94170: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cf42da11-34fc-40c9-851b-ae2c95f1f121: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cf42da11-34fc-40c9-851b-ae2c95f1f121: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2c0d1742-f3de-40c4-9885-3753908c9f17: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2c0d1742-f3de-40c4-9885-3753908c9f17: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a23ef9f7-7e3a-46f7-a2e4-5f5f211bcc91: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a23ef9f7-7e3a-46f7-a2e4-5f5f211bcc91: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0091b30b-e9c6-42ad-8dac-add96355c442: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0091b30b-e9c6-42ad-8dac-add96355c442: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c1519ba4-9513-4401-83e1-69d18dedb95c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c1519ba4-9513-4401-83e1-69d18dedb95c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4b6b25b9-54b0-443d-a713-95ab3e087457: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4b6b25b9-54b0-443d-a713-95ab3e087457: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run be2b7e47-41df-4221-a191-9d1f1bf6aa30: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run be2b7e47-41df-4221-a191-9d1f1bf6aa30: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cc4f55e9-6394-4c1e-8085-2a500b8ebe62: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cc4f55e9-6394-4c1e-8085-2a500b8ebe62: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c0872bf0-b614-4c10-9643-d1cc5c8e4e59: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c0872bf0-b614-4c10-9643-d1cc5c8e4e59: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c91782a0-378b-437e-976a-139a709fdd19: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c91782a0-378b-437e-976a-139a709fdd19: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ae7ddff7-a346-4ce6-8db2-f4a29a2f377f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ae7ddff7-a346-4ce6-8db2-f4a29a2f377f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \\'empathy: Is this response empathetic? Does it make the user feel like they are being heard?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name=\\'gpt-4.1\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'empathy\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
            "        run=run,\n",
            "        example=example,\n",
            "        evaluator_run_id=evaluator_run_id,\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 701, in wrapper\n",
            "    function_result = run_container[\"context\"].run(\n",
            "        func, *args, **kwargs\n",
            "    )\n",
            "  File \"c:\\Common\\AI-LLM\\fromGITHUBNET\\ai-llm-engineering-1\\day_5\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': 'empathy: Is this response empathetic? Does it make the user feel like they are being heard?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001527E4F0510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001527E4F0050>, root_client=<openai.OpenAI object at 0x0000015277D28950>, root_async_client=<openai.AsyncOpenAI object at 0x0000015277D29B50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='empathy' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "# Naive retriever\n",
        "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "retriever_names = [\n",
        "    \"Naive\", \"BM25\", \"Multi-Query\", \"Parent-Document\", \"Contextual Compression\", \"Ensemble\"\n",
        "]\n",
        "results = []\n",
        "\n",
        "import time\n",
        "\n",
        "for sc in [False, True]:  # semantic chunking off/on\n",
        "    for retriever_name in retriever_names: # loop through each retriever\n",
        "    \n",
        "        start_time = time.time()\n",
        "        if sc:\n",
        "            vectorstore = semantic_vectorstore\n",
        "        else:\n",
        "            vectorstore = text_vectorstore\n",
        "\n",
        "        match retriever_name:\n",
        "        \n",
        "            case 'Naive':\n",
        "                retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "                naive_retriever = retriever\n",
        "            \n",
        "            case 'BM25':\n",
        "                retriever = BM25Retriever.from_documents(rag_documents) \n",
        "                bm25_retriever = retriever\n",
        "            \n",
        "            case 'Contextual Compression':\n",
        "                compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "                retriever = ContextualCompressionRetriever(\n",
        "                            base_compressor=compressor, base_retriever=naive_retriever)\n",
        "                compression_retriever = retriever\n",
        "\n",
        "            case 'Multi-Query':\n",
        "                retriever = MultiQueryRetriever.from_llm(retriever=naive_retriever, llm=chat_model)\n",
        "                multi_query_retriever = retriever\n",
        "                \n",
        "            case 'Parent-Document':\n",
        "                parent_docs = rag_documents\n",
        "                child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)\n",
        "\n",
        "                client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "                client.create_collection(\n",
        "                    collection_name=\"full_documents\",\n",
        "                    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        "                )\n",
        "\n",
        "                parent_document_vectorstore = QdrantVectorStore(\n",
        "                    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        "                )\n",
        "                store = InMemoryStore()\n",
        "                retriever = ParentDocumentRetriever(\n",
        "                                vectorstore = parent_document_vectorstore,\n",
        "                                docstore=store,\n",
        "                                child_splitter=child_splitter)\n",
        "                retriever.add_documents(parent_docs, ids=None)\n",
        "                parent_document_retriever = retriever\n",
        "\n",
        "            case 'Ensemble':\n",
        "                retriever_list = [naive_retriever, bm25_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "                equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "                retriever = EnsembleRetriever(retrievers=retriever_list, weights=equal_weighting)\n",
        "            \n",
        "            case ' ':\n",
        "                retriever = \"Invalid option\"\n",
        "        \n",
        "        from operator import itemgetter\n",
        "        from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "        from langchain.schema import StrOutputParser\n",
        "\n",
        "        rag_chain = (\n",
        "            # {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "            # | rag_prompt | llm | StrOutputParser()\n",
        "            {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "            | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "            | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        "        )\n",
        "\n",
        "        rag_chain.invoke({\"question\" : \"How does the Philippines regulate the development of AI in the country?\"})\n",
        "        # [\"response\"].content\n",
        "\n",
        "        latency = time.time() - start_time\n",
        "        evaluate(\n",
        "            rag_chain.invoke,\n",
        "            data=dataset_name,\n",
        "            evaluators=[\n",
        "                qa_evaluator,\n",
        "                labeled_helpfulness_evaluator,\n",
        "                empathy_evaluator],\n",
        "            metadata={\n",
        "                \"retriever\": retriever_name,\n",
        "                \"semantic chunking\": sc\n",
        "                # \"latency\": latency\n",
        "                }\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmTL6-pc1ZGz"
      },
      "source": [
        "****** EVALUATION ENDS HERE ********\n",
        "\n",
        "Please ignore the rest down below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R35sQMHVrnpl"
      },
      "source": [
        "## LangSmith Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "122b1bd1f0e9417a8dcb57d4eebe4d2e",
            "e0c233ad01604540a6c873f4a731982d",
            "e9a01115c75b499884f7e0ef32e9e599",
            "5faba4ad609448b2b49024add4ad3b8e",
            "ef25efa751304e4699910f1fbc14345f",
            "0b44cb0f8e34446c8dde668a75d3d8ad",
            "edaac6587b2d4bd5be52b89bb097f99f",
            "7cb241365f604419af454c1c28de197a",
            "9cf586576ff44dba86ba2eb389593c61",
            "849b5c95008541d49f1ceedf0a59ac60",
            "f3665a86662746c4ac7cb0796604781d"
          ]
        },
        "id": "t7t_Uz0tdumL",
        "outputId": "d684e218-294e-4dc3-c8de-a01d397f021c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'giving-trick-63' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/ef297373-b111-4973-ab34-102fa9ead893/compare?selectedSessions=59c23679-3fb3-463c-ae54-7bb8705f7490\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a0ef0e17a854b6f90f8ac92e997bfa4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.output</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.correctness</th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.empathy</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does the AI registration process, includin...</td>\n",
              "      <td>Based on the provided context, the AI registra...</td>\n",
              "      <td>None</td>\n",
              "      <td>The AI registration process requires detailed ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5.208305</td>\n",
              "      <td>5224b263-77a3-4a2b-b3e2-165729d7b4f0</td>\n",
              "      <td>a39166aa-51d4-48d9-ab83-6218f233cbec</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the NAIC's jurisdiction over AI, as o...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>The NAIC's jurisdiction over AI, as specified ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.300400</td>\n",
              "      <td>5f49dd4e-5938-41f3-86a7-0134d10e9cfd</td>\n",
              "      <td>d2900b56-f564-445e-b917-bd0088a69d7d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>how does the national AI commission work with ...</td>\n",
              "      <td>The National AI Commission (NAIC) coordinates ...</td>\n",
              "      <td>None</td>\n",
              "      <td>the national AI commission develops policies a...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3.111340</td>\n",
              "      <td>703738e2-269e-49d9-92b3-76f70563ebdd</td>\n",
              "      <td>f68bcf18-e913-4b13-a85b-4f33e1540732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How do the laws and regulations regarding AI i...</td>\n",
              "      <td>The laws and regulations regarding AI in the P...</td>\n",
              "      <td>None</td>\n",
              "      <td>The laws and regulations in the Philippines pr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.560005</td>\n",
              "      <td>6557c129-dcb4-465f-b1d3-3201a8b2470b</td>\n",
              "      <td>e4f4b466-ff97-47cf-8c68-ca368d407850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do government agencies like DOST and DICT ...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>The context specifies that government agencies...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.866825</td>\n",
              "      <td>262b891c-be05-4f04-b079-4407068bc933</td>\n",
              "      <td>903a9571-f41c-40b7-a4bb-33cecda1b736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>whats the repleal of laws and AI hallusinations?</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>The context discusses the repeal of laws that ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.990699</td>\n",
              "      <td>e2584e77-e0b0-4af4-b348-26650406ec39</td>\n",
              "      <td>5f9e6b05-9b7a-4801-af1b-b7ad6a4f29e0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the deployment of AI systems relate t...</td>\n",
              "      <td>Based on the provided context, the deployment ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The deployment of AI systems is closely connec...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5.766852</td>\n",
              "      <td>0846ba3a-7e0c-499b-80cb-061874e55ac9</td>\n",
              "      <td>4c32fdc6-d8ad-4635-9e86-8e13155b4ae7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How does the use of AI for manipulatn, disinfo...</td>\n",
              "      <td>Based on the provided context, the NAIC (Natio...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that AI used for manipu...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6.190494</td>\n",
              "      <td>3be306f4-3391-41f6-a9df-23d150ba847c</td>\n",
              "      <td>4cf4a9da-98ec-4578-8480-091332fa6fdc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the Philippines regulate the developm...</td>\n",
              "      <td>According to the provided context, the Philipp...</td>\n",
              "      <td>None</td>\n",
              "      <td>The Philippines regulates the development and ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3.235278</td>\n",
              "      <td>1985fb8a-385b-4602-afc4-dfa241eead63</td>\n",
              "      <td>c93ec0d0-edd0-4370-9d45-c162f4c40cbb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What does the P4:56 refer to in the context of...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>The P4:56 appears to be a reference code or ti...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.814284</td>\n",
              "      <td>f56ccff2-ab8d-4b7e-b993-e2c5236f6930</td>\n",
              "      <td>d1d62e0d-7d1e-4cc6-be27-71bca1498c95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How does AI impact the Philippines according t...</td>\n",
              "      <td>According to the provided context, AI presents...</td>\n",
              "      <td>None</td>\n",
              "      <td>AI presents enormous opportunities for the Phi...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.319625</td>\n",
              "      <td>d56c8872-a388-450d-bc8e-52f91d64d493</td>\n",
              "      <td>f3e1b3bb-8979-426d-a201-d9694008a90d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Who is PIA S. CAYETANO in the context of AI re...</td>\n",
              "      <td>Based on the provided context, PIA S. CAYETANO...</td>\n",
              "      <td>None</td>\n",
              "      <td>PIA S. CAYETANO is a senator who introduced an...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.035867</td>\n",
              "      <td>c3cd0729-c2f1-410c-b1ff-bc22b9e8e29e</td>\n",
              "      <td>bac852cf-a283-4eaa-a1f3-9d8ab6b20afc</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults giving-trick-63>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        empathy_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"default_chain_init\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq7fCVinrpI4"
      },
      "source": [
        "## Dope-ifying Our Application\n",
        "\n",
        "We'll be making a few changes to our RAG chain to increase its performance on our SDG evaluation test dataset!\n",
        "\n",
        "- Include a \"dope\" prompt augmentation\n",
        "- Use larger chunks\n",
        "- Improve the retriever model to: `text-embedding-3-large`\n",
        "\n",
        "Let's see how this changes our evaluation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "z56pXwyUgFUt"
      },
      "outputs": [],
      "source": [
        "EMPATHY_RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
        "\n",
        "You must answer the question using empathy and kindness, and make sure the user feels heard.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "empathy_rag_prompt = ChatPromptTemplate.from_template(EMPATHY_RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rZLcTstJgfv5"
      },
      "outputs": [],
      "source": [
        "rag_documents = docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-LYsyirngj6n"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "rag_documents = text_splitter.split_documents(rag_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "b9MI2Bm2go1r"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "hVUY25FKgxXx"
      },
      "outputs": [],
      "source": [
        "vectorstore = Qdrant.from_documents(\n",
        "    documents=rag_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"AI Bills RAG 2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Q4TOZNYIg2v1"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqYGFrnKDB91"
      },
      "source": [
        "Setting up our new and improved DOPE RAG CHAIN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "HqnTqeXMhAdx"
      },
      "outputs": [],
      "source": [
        "empathy_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | empathy_rag_prompt | llm | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21pTxoqJDI1Y"
      },
      "source": [
        "Let's test it on the same output that we saw before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OfZZ3MoN3fKv",
        "outputId": "d65722dd-92c2-4e4e-9cca-c42ee6f3f208"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Thank you for your thoughtful question. Based on the context provided, the Philippines AI Bill is important because it aims to create a national framework that balances the encouragement of technological innovation with ensuring that AI systems are safe, ethical, transparent, and under meaningful human oversight. It recognizes the transformative impact of AI on industries and society and seeks to promote responsible and lawful AI development that supports Filipino ingenuity and addresses national development challenges.\\n\\nMoreover, the bill emphasizes protecting the rights and welfare of every citizen by preventing AI from being used to commit crimes, abuse rights, or cause harm, whether intentionally or accidentally. This shows a deep concern for the well-being of the people while fostering progress and innovation in technology.\\n\\nIts clear that the bill strives to create a secure, inclusive, and ethical digital future for the Philippines, thoughtfully anticipating both the opportunities and risks that AI presents. If you have more questions or need further clarification, Im here to help!'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "empathy_rag_chain.invoke({\"question\" : \"Why is the Philippines AI Bill important?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpj7v1inDLnQ"
      },
      "source": [
        "Finally, we can evaluate the new chain on the same test set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "bf8dcc0895054529af356da401c513f6",
            "7dce19ac55264f2b88a0e4730e55867b",
            "2a0755d4476543feb4a64538e3e37213",
            "158212a630f04cbd884c937f2f60f5c8",
            "11c7f66acc1d45be9517d0addf49331e",
            "ddffd834e09940a4bd3874c3f39b4e21",
            "ef63c3b2d51e452da03cdae5d9b034be",
            "c20b539cd70b4ba99601ad1d69fd9cec",
            "a6d681eeafa44d18b933a4c5dec88382",
            "d1d54ccd56494c4d831f71b416a1f880",
            "530f696feefe499da08c6312047379b2"
          ]
        },
        "id": "Dx11S2b-hIM8",
        "outputId": "d3a3ea78-aa32-4bd2-8c2a-d0d0303695c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'complicated-station-33' at:\n",
            "https://smith.langchain.com/o/92af5c49-0a9e-4f85-beea-085fbd240cd1/datasets/861faf01-85b7-44d1-8a8c-72463fffb4fb/compare?selectedSessions=112f58bd-e760-499e-9456-900ca391a6b3\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dfce57083084dfeab8ff95cbe21ffd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.output</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.correctness</th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.empathy</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does the DOST support AI policies and init...</td>\n",
              "      <td>Thank you for your thoughtful question. From t...</td>\n",
              "      <td>None</td>\n",
              "      <td>The DOST supports AI policies and initiatives ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.705640</td>\n",
              "      <td>0768fd5a-970b-482f-aa5c-ec1ef5a0d906</td>\n",
              "      <td>c5f0d260-aa2b-4ab3-bdc3-508429074e06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does TESDA collaborate with the NAIC to su...</td>\n",
              "      <td>Thank you for your thoughtful question. Based ...</td>\n",
              "      <td>None</td>\n",
              "      <td>TESDA collaborates with the NAIC by supporting...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.433222</td>\n",
              "      <td>7baaaa67-bf6a-48fd-9255-d6d474fc30d3</td>\n",
              "      <td>2dea2471-7639-479e-8b25-dc6a1333f59b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does TESDA's role in AI regulation relate ...</td>\n",
              "      <td>Thank you for your thoughtful question. From t...</td>\n",
              "      <td>None</td>\n",
              "      <td>TESDA is listed as a member of the NAIC, which...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.652752</td>\n",
              "      <td>a2141d56-0bcd-4a44-83d0-ce71916c01d0</td>\n",
              "      <td>a8a0436f-3524-477c-8046-67d06d6d5fcb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>how DOST and NAIC work together in AI regulati...</td>\n",
              "      <td>Thank you for your thoughtful question. From t...</td>\n",
              "      <td>None</td>\n",
              "      <td>The NAIC, which is attached to the DOST, has j...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9.333190</td>\n",
              "      <td>38c5205d-984d-4c7f-96b3-2001897dc841</td>\n",
              "      <td>57be7d9d-48b4-4781-8139-4d9421ecf097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does the regulation of AI responsibility f...</td>\n",
              "      <td>Thank you for your thoughtful question. Based ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The regulation of AI responsibility for harms ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.489557</td>\n",
              "      <td>db67c379-1fa4-4938-a726-861cd73c7e19</td>\n",
              "      <td>47964008-2975-4263-97db-ba3c15d056ea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How do the United Nations' recommendations for...</td>\n",
              "      <td>Thank you for your thoughtful question. From t...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that international effo...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.487013</td>\n",
              "      <td>33a985ea-db0d-49d3-a7b7-22d7cd6567a1</td>\n",
              "      <td>2abf25db-1383-477d-b5d0-dece432f7ad8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the need for regulation and oversight...</td>\n",
              "      <td>Thank you for your thoughtful question. It's c...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that while AI offers si...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10.912735</td>\n",
              "      <td>cdcf2c9c-9950-4642-b234-36eec086d17f</td>\n",
              "      <td>951709d5-a3cd-4d29-a8de-5c65a50c48e1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How can the development of multimodal models e...</td>\n",
              "      <td>Thank you for your thoughtful question. From t...</td>\n",
              "      <td>None</td>\n",
              "      <td>The development of multimodal models can enhan...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7.673174</td>\n",
              "      <td>b8033ddb-c877-49df-acac-bebfecc89e36</td>\n",
              "      <td>29c0abb1-dead-4a86-8f72-48abd4795d36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the regulation of Artificial General ...</td>\n",
              "      <td>Thank you for your thoughtful question. Based ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context states that policies should promot...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.787368</td>\n",
              "      <td>fdb75978-1d94-4b8d-b2c2-54e67e231d59</td>\n",
              "      <td>42463c9f-beca-4058-bd5d-51a25fec38db</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What does the PHILIPPINE CONSTITUTION say abou...</td>\n",
              "      <td>Thank you for your question about what the Phi...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context references Article XIV, Section 10...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.799059</td>\n",
              "      <td>1f7ddec1-711c-4e11-bca1-351189c11a8d</td>\n",
              "      <td>b2306d1c-746d-4483-ab9b-2167cbc9a440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Georgetown University how does it relate to AI...</td>\n",
              "      <td>Thank you for your thoughtful question. Based ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context mentions Georgetown University in ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.645414</td>\n",
              "      <td>9e02174d-66d4-4b77-8e12-ff448ea703b6</td>\n",
              "      <td>b2d260b2-e61e-431b-a72c-e548a0256599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What is the significance of regulating AI in t...</td>\n",
              "      <td>Thank you for your thoughtful question. Based ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The regulation of AI in the Philippines aims t...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.056313</td>\n",
              "      <td>c354b332-5d2d-46cb-90b8-4860830246d6</td>\n",
              "      <td>23ac1c02-5ebf-4958-9486-ff956f008240</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults complicated-station-33>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    empathy_rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        empathy_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"empathy_rag_chain\"},\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "13-advanced-retrieval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07ab3dc0790241bbb85a7f488a42ef8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7710c7377cbc4c30b55b28b4bc99e88f",
              "IPY_MODEL_41bdd49fab5f4826959d0d50663ff539",
              "IPY_MODEL_60168d85131d4afc99d55d61ab954ee6"
            ],
            "layout": "IPY_MODEL_9edf898aeeab40dda9b9475395776521"
          }
        },
        "095f680d37a3430fb82d223615662db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b44cb0f8e34446c8dde668a75d3d8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10df31709059484c99f102453d780473": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1160a44dc18e47b0890f70c40eaa7eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11c7f66acc1d45be9517d0addf49331e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "122b1bd1f0e9417a8dcb57d4eebe4d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0c233ad01604540a6c873f4a731982d",
              "IPY_MODEL_e9a01115c75b499884f7e0ef32e9e599",
              "IPY_MODEL_5faba4ad609448b2b49024add4ad3b8e"
            ],
            "layout": "IPY_MODEL_ef25efa751304e4699910f1fbc14345f"
          }
        },
        "158212a630f04cbd884c937f2f60f5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1d54ccd56494c4d831f71b416a1f880",
            "placeholder": "",
            "style": "IPY_MODEL_530f696feefe499da08c6312047379b2",
            "value": "20/?[01:43&lt;00:00,5.25s/it]"
          }
        },
        "23863bc37a8645029934b8c106622c51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2508d229935744cbb5fc340222e2d660": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a0755d4476543feb4a64538e3e37213": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c20b539cd70b4ba99601ad1d69fd9cec",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6d681eeafa44d18b933a4c5dec88382",
            "value": 1
          }
        },
        "33f063017b7c4c7fa8cbafc89674350b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6864c81e2bcf459bbaf5acbb36bdfcbe",
              "IPY_MODEL_59d6e269eadf429a924f6f79bc8ba4ba",
              "IPY_MODEL_ca791fc471e34b9da2f9070fc1053c0f"
            ],
            "layout": "IPY_MODEL_8baf0ed3d0f743f294e07f2b5407e820"
          }
        },
        "3a8537e37fc14fd9b16ca0ceee4fede6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41bdd49fab5f4826959d0d50663ff539": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eb8b2e3262c45248708a2082c366f0a",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_095f680d37a3430fb82d223615662db5",
            "value": 64
          }
        },
        "530f696feefe499da08c6312047379b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59d6e269eadf429a924f6f79bc8ba4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_890e0dd7fa524ceca1e805cb6253ee71",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61b52ff459214129b8f7e6d67b192b78",
            "value": 20
          }
        },
        "5ab5f08afa5841709aedb2f78a52a11c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c2fda99d4204d85b1bf7ad354fd58d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5faba4ad609448b2b49024add4ad3b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_849b5c95008541d49f1ceedf0a59ac60",
            "placeholder": "",
            "style": "IPY_MODEL_f3665a86662746c4ac7cb0796604781d",
            "value": "20/?[01:27&lt;00:00,6.45s/it]"
          }
        },
        "60168d85131d4afc99d55d61ab954ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8537e37fc14fd9b16ca0ceee4fede6",
            "placeholder": "",
            "style": "IPY_MODEL_1160a44dc18e47b0890f70c40eaa7eb0",
            "value": "61/64[00:02&lt;00:00,23.36it/s]"
          }
        },
        "61b52ff459214129b8f7e6d67b192b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6864c81e2bcf459bbaf5acbb36bdfcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10df31709059484c99f102453d780473",
            "placeholder": "",
            "style": "IPY_MODEL_2508d229935744cbb5fc340222e2d660",
            "value": "Generating:100%"
          }
        },
        "6eb8b2e3262c45248708a2082c366f0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7710c7377cbc4c30b55b28b4bc99e88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c2fda99d4204d85b1bf7ad354fd58d4",
            "placeholder": "",
            "style": "IPY_MODEL_93cd4d35c5fd41f5904ca1d52d1f52a8",
            "value": "embeddingnodes:95%"
          }
        },
        "7cb241365f604419af454c1c28de197a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7dce19ac55264f2b88a0e4730e55867b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddffd834e09940a4bd3874c3f39b4e21",
            "placeholder": "",
            "style": "IPY_MODEL_ef63c3b2d51e452da03cdae5d9b034be",
            "value": ""
          }
        },
        "849b5c95008541d49f1ceedf0a59ac60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "890e0dd7fa524ceca1e805cb6253ee71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8baf0ed3d0f743f294e07f2b5407e820": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93cd4d35c5fd41f5904ca1d52d1f52a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cf586576ff44dba86ba2eb389593c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9edf898aeeab40dda9b9475395776521": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "a6d681eeafa44d18b933a4c5dec88382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf8dcc0895054529af356da401c513f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dce19ac55264f2b88a0e4730e55867b",
              "IPY_MODEL_2a0755d4476543feb4a64538e3e37213",
              "IPY_MODEL_158212a630f04cbd884c937f2f60f5c8"
            ],
            "layout": "IPY_MODEL_11c7f66acc1d45be9517d0addf49331e"
          }
        },
        "c20b539cd70b4ba99601ad1d69fd9cec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ca791fc471e34b9da2f9070fc1053c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23863bc37a8645029934b8c106622c51",
            "placeholder": "",
            "style": "IPY_MODEL_5ab5f08afa5841709aedb2f78a52a11c",
            "value": "20/20[00:52&lt;00:00,4.50s/it]"
          }
        },
        "d1d54ccd56494c4d831f71b416a1f880": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddffd834e09940a4bd3874c3f39b4e21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0c233ad01604540a6c873f4a731982d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b44cb0f8e34446c8dde668a75d3d8ad",
            "placeholder": "",
            "style": "IPY_MODEL_edaac6587b2d4bd5be52b89bb097f99f",
            "value": ""
          }
        },
        "e9a01115c75b499884f7e0ef32e9e599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb241365f604419af454c1c28de197a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cf586576ff44dba86ba2eb389593c61",
            "value": 1
          }
        },
        "edaac6587b2d4bd5be52b89bb097f99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef25efa751304e4699910f1fbc14345f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef63c3b2d51e452da03cdae5d9b034be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3665a86662746c4ac7cb0796604781d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
