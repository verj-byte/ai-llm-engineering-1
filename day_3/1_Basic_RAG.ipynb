{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lElF3o5PR6ys"
      },
      "source": [
        "# Basic RAG - From Scratch!!\n",
        "\n",
        "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application.\n",
        "\n",
        "We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Activity #1: First let's explore embeddings and cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(vec_1, vec_2):\n",
        "  return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's use the `text-embedding-3-small` embedding model (more on that in a second) to embed two sentences. In order to use this embedding model endpoint - we'll need to provide our OpenAI API key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass(\"OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, define the embedding model. Credit goes to AI Makerspace for the library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from psi.openai_utils.embedding import EmbeddingModel\n",
        "\n",
        "embedding_model = EmbeddingModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's explore cosine similiarity of 2 embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5596981772056681"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "puppy_sentence = \"I love dogs!\"\n",
        "# dog_sentence = \"Bayang magiliw!\"\n",
        "dog_sentence = \"I hate huskies!\"\n",
        "\n",
        "puppy_vector = embedding_model.get_embedding(puppy_sentence)\n",
        "dog_vector = embedding_model.get_embedding(dog_sentence)\n",
        "cosine_similarity(puppy_vector, dog_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### 🎯 Breakout Room - Group Discussion: \n",
        "\n",
        "Explore how cosine similarity works and discuss your observations with the group. Write down final answer below.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Observations :\n",
        "\n",
        "* Cosine similarity works by deriving a result when comparing two vectors regardless of their magnitude.\n",
        "In the example given, each of the sentences are converted into vectors and a cosine of the angle\n",
        "between the two vectors is calculated in a multi-dimensional space.\n",
        "\n",
        "The result of:\n",
        "\n",
        "- a positive value approaching 1 indicates likeness or similarity of the vectors (converted sentences or phrases)\n",
        "- a value of 0 indicates no relation between the vectos\n",
        "- a negative value approaching -1 indicates unlikeness or having no similarity at all\n",
        "\n",
        "The result therefore is used programmatically to determine the relation of the given sentences or phrases.\n",
        "</span>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CtcL8P8R6yt"
      },
      "source": [
        "## Let's RAG-ing Time!\n",
        "\n",
        "- Task 1: Imports and Utilities\n",
        "- Task 2: Documents\n",
        "- Task 3: Embeddings and Vectors\n",
        "- Task 4: Prompts\n",
        "- Task 5: Retrieval Augmented Generation\n",
        "  - 🚧 Activity #1: Augment RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dz6GYilR6yt"
      },
      "source": [
        "Here's a more visual representation of the task to be performed.\n",
        "\n",
        "![RAG Steps](images/ragging.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjmC0KFtR6yt"
      },
      "source": [
        "## Import Wall\n",
        "\n",
        "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z1dyrG4hR6yt"
      },
      "outputs": [],
      "source": [
        "from psi.text_utils import TextFileLoader, CharacterTextSplitter\n",
        "from psi.vectordatabase import VectorDatabase\n",
        "import asyncio\n",
        "\n",
        "#  need  this support async in jupyter notebook\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Loading Source Documents\n",
        "\n",
        "So, first things first, we need some documents to work with.\n",
        "\n",
        "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
        "\n",
        "In this case, we're going to parse our text file into a single document in memory.\n",
        "\n",
        "Let's look at the relevant bits of the `TextFileLoader` class:\n",
        "\n",
        "```python\n",
        "def load_file(self):\n",
        "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
        "            self.documents.append(f.read())\n",
        "```\n",
        "\n",
        "We're simply loading the document using the built in `open` method, and storing that output in our `self.documents` list.\n",
        "\n",
        "> NOTE: We're using text from Noli Me Tangere as our sample data. This data is largely irrelevant as we want to focus on the mechanisms of RAG, which includes out data's shape and quality - but not specifically what the contents of the data are. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia2sUEuGR6yu",
        "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_loader = TextFileLoader(\"data/noli.txt\")\n",
        "documents = text_loader.load_documents()\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV-tj5WFR6yu",
        "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Beginning (characters 0-100)\n",
            "==================================================\n",
            "The Project Gutenberg eBook of The Social Cancer: A Complete English Version of Noli Me Tangere\n",
            "    \n",
            "\n",
            "==================================================\n",
            "Middle Section (characters 1480-2000)\n",
            "==================================================\n",
            "     THE NOVELS OF JOSÉ RIZAL\n",
            "\n",
            "                      Translated from Spanish into English\n",
            "\n",
            "                             BY CHARLES DERBYSHIRE\n",
            "\n",
            "\n",
            "                      THE SOCIAL CANCER (NOLI ME TANGERE)\n",
            "                     THE REIGN OF GREED (EL FILIBUSTERISMO)\n",
            "\n",
            "\n",
            "\n",
            "            Copyright, 1912, by Philippine Education Company.\n",
            "            Entered at Stationers' Hall.\n",
            "            Registrado en las Islas Filipinas.\n",
            "            All rights reserved.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "TRANSLATOR'S INTRODUCTION\n",
            "\n",
            "\n",
            "I\n",
            "\n",
            "    \"We travel rapidly in these hist\n"
          ]
        }
      ],
      "source": [
        "def print_document_portion(doc, start, end, title=\"Document Portion\"):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{title} (characters {start}-{end})\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(doc[start:end])\n",
        "\n",
        "# Usage\n",
        "print_document_portion(documents[0], 0, 100, \"Beginning\")\n",
        "print_document_portion(documents[0], 1480, 2000, \"Middle Section\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHlTvCzYR6yu"
      },
      "source": [
        "### Splitting Text Into Chunks\n",
        "\n",
        "As we can see, there is one massive document.\n",
        "\n",
        "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
        "\n",
        "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
        "\n",
        "For this toy example, we'll just split blindly on length.\n",
        "\n",
        ">There's an opportunity to clear up some terminology here, for this course we will be stick to the following:\n",
        ">\n",
        ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
        ">- \"document(s)\" : single (or more) text object(s)\n",
        ">- \"corpus\" : the combination of all of our documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6Voc0jR6yv"
      },
      "source": [
        "As you can imagine (though it's not specifically true in this toy example) the idea of splitting documents is to break them into managable sized chunks that retain the most relevant local context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMC4tsEmR6yv",
        "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1283"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_splitter = CharacterTextSplitter()\n",
        "split_documents = text_splitter.split_texts(documents)\n",
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2wKT0WLR6yv"
      },
      "source": [
        "Let's take a look at some of the documents we've managed to split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcYMwWJoR6yv",
        "outputId": "20d69876-feca-4826-b4be-32915276987a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The Project Gutenberg eBook of The Social Cancer: A Complete English Version of Noli Me Tangere\\n    \\nThis ebook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this ebook or online\\nat www.gutenberg.org. If you are not located in the United States,\\nyou will have to check the laws of the country where you are located\\nbefore using this eBook.\\n\\nTitle: The Social Cancer: A Complete English Version of Noli Me Tangere\\n\\nAuthor: José Rizal\\n\\nTranslator: Charles E. Derbyshire\\n\\nRelease date: October 1, 2004 [eBook #6737]\\n                Most recently updated: April 13, 2010\\n\\nLanguage: English\\n\\nCredits: Produced by Jeroen Hellingman.\\n\\n\\n*** START OF THE PROJECT GUTENBERG EBOOK THE SOCIAL CANCER: A COMPLETE ENGLISH VERSION OF NOLI ME TANGERE ***\\n\\n\\n\\n\\nProduced by Jeroen Hellingman.\\n\\n\\n\\n\\n\\n\\n\\n\\n              ',\n",
              " 'Produced by Jeroen Hellingman.\\n\\n\\n*** START OF THE PROJECT GUTENBERG EBOOK THE SOCIAL CANCER: A COMPLETE ENGLISH VERSION OF NOLI ME TANGERE ***\\n\\n\\n\\n\\nProduced by Jeroen Hellingman.\\n\\n\\n\\n\\n\\n\\n\\n\\n                               The Social Cancer\\n\\n\\n\\n       A Complete English Version of Noli Me Tangere from the Spanish of\\n                                   José Rizal\\n\\n                                       By\\n\\n                               Charles Derbyshire\\n\\n\\n\\n                                     Manila\\n                          Philippine Education Company\\n                          New York: World Book Company\\n                                      1912\\n\\n\\n\\n\\n\\n\\n                            THE NOVELS OF JOSÉ RIZAL\\n\\n                      Translated from Spanish into English\\n\\n                             BY CHARLES DERBYSHIRE\\n\\n\\n                      THE SOCIAL CANCER (NOLI ME TANGERE)\\n                     THE REIGN OF GREED (EL FILIBUSTERISMO)\\n\\n\\n\\n            Copyright, 1912, by Philippine Education Co',\n",
              " 'BY CHARLES DERBYSHIRE\\n\\n\\n                      THE SOCIAL CANCER (NOLI ME TANGERE)\\n                     THE REIGN OF GREED (EL FILIBUSTERISMO)\\n\\n\\n\\n            Copyright, 1912, by Philippine Education Company.\\n            Entered at Stationers\\' Hall.\\n            Registrado en las Islas Filipinas.\\n            All rights reserved.\\n\\n\\n\\n\\n\\nTRANSLATOR\\'S INTRODUCTION\\n\\n\\nI\\n\\n    \"We travel rapidly in these historical sketches. The reader\\n    flies in his express train in a few minutes through a couple\\n    of centuries. The centuries pass more slowly to those to\\n    whom the years are doled out day by day. Institutions grow\\n    and beneficently develop themselves, making their way into\\n    the hearts of generations which are shorter-lived than they,\\n    attracting love and respect, and winning loyal obedience;\\n    and then as gradually forfeiting by their shortcomings\\n    the allegiance which had been honorably gained in worthier\\n    periods. We see wealth and greatness; we see corruption and\\n    vic']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents[0:1]\n",
        "split_documents[0:2]\n",
        "split_documents[0:3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOU-RFP_R6yv"
      },
      "source": [
        "## Step 2: Compute Embeddings per Chunk\n",
        "\n",
        "Next, we have to convert our corpus into a \"machine readable\" format as we explored in the Embedding Primer notebook.\n",
        "\n",
        "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI API Key\n",
        "\n",
        "In order to access OpenAI's APIs, we'll need to provide our OpenAI API Key!\n",
        "\n",
        "You can work through the folder \"OpenAI API Key Setup\" for more information on this process if you don't already have an API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass(\"OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector Database\n",
        "\n",
        "Let's set up our vector database to hold all our documents and their embeddings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQrfAR1R6yv"
      },
      "source": [
        "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
        "\n",
        "Let's look at our `VectorDatabase().__init__()`:\n",
        "\n",
        "```python\n",
        "def __init__(self, embedding_model: EmbeddingModel = None):\n",
        "        self.vectors = defaultdict(np.array)\n",
        "        self.embedding_model = embedding_model or EmbeddingModel()\n",
        "```\n",
        "\n",
        "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
        "\n",
        "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
        "\n",
        "> **Quick Info About `text-embedding-3-small`**:\n",
        "> - It has a context window of **8191** tokens\n",
        "> - It returns vectors with dimension **1536**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L273pRdeR6yv"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "#### ❓Question #1:\n",
        "\n",
        "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
        "\n",
        "1. Is there any way to modify this dimension?\n",
        "2. What technique does OpenAI use to achieve this?\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Answer :\n",
        "\n",
        "* 1. OpenAI allows overriding the default embedding dimension of 1536 for text-embedding-3-small model by passing a different size value, such as 512 or 256, in API parameter called \"dimension\".\n",
        "\n",
        "* 2. Open AI uses techniques like Matryoshka Representation Learning (MRL) to make embeddings flexible to allow the ability to shrink them without losing too much meaning or relations and therefore allowing users to pick smaller dimensions if needed rather than the default 1536-dimensional vector. With this technique, MRL trains the model so that embeddings can be cut to, say 256, a smaller size (the value that can be specified via 'dimension' parameter) by taking the first 256 numbers and still works well for many other tasks making the embeddings flexible for various needs including ability to fit into smaller vector databases.\n",
        "</span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1, and [this documentation](https://platform.openai.com/docs/guides/embeddings/use-cases) for an answer to question #2!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5FZY7K3R6yv"
      },
      "source": [
        "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
        "\n",
        "```python\n",
        "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
        "        return await aget_embeddings(\n",
        "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSct6X0aR6yv"
      },
      "source": [
        "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
        "\n",
        "```python\n",
        "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
        "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
        "        for text, embedding in zip(list_of_text, embeddings):\n",
        "            self.insert(text, np.array(embedding))\n",
        "        return self\n",
        "```\n",
        "\n",
        "And that's all we need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "O4KoLbVDR6yv"
      },
      "outputs": [],
      "source": [
        "vector_db = VectorDatabase()\n",
        "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZwaGvpR6yv"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### ❓ Assignment Question #2:\n",
        "\n",
        "What are the benefits of using an `async` approach to collecting our embeddings?\n",
        "\n",
        "### Answer :\n",
        "\n",
        "* An 'async' approach allows the computer to work on multiple tasks at once without waiting for each one to finish before starting the next. Some of the benefits are:\n",
        "\n",
        "1. Efficient use of resources. APIs like OpenAI have rate limits (e.g. 60 requests per minute for free accounts) and async helps to make the most of those limits by sending requests without waiting. This keeps the program busy and efficient, especially when dealing with large datasets.\n",
        "\n",
        "2. Keeps apps smooth and responsive for users. Async lets app send embedding requests in the background while still responding to user actions, thus making the app responsive avoiding user to keep staring at a loading screen for a long period of time.\n",
        "\n",
        "3. Better error handling for big batches. It allows you to retry failed requests or skip them without messing up the whole job which is super useful when processing thousands of embeddings.\n",
        "\n",
        "4. Faster. For far more obvious reason, it is way faster. Collecting embeddings can take time and async mode let you send multiple requests to the API model at the same time so you get embedding results back much faster.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBdIt-xR6yw"
      },
      "source": [
        "So, to review what we've done so far in natural language:\n",
        "\n",
        "1. We load source documents\n",
        "2. We split those source documents into smaller chunks (documents)\n",
        "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
        "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-vWANZyR6yw"
      },
      "source": [
        "## Step 3: Finding Best Match with Semantic Similarity\n",
        "\n",
        "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
        "\n",
        "We're going to use the following process to achieve this in our toy example:\n",
        "\n",
        "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
        "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
        "3. We return a list of the top `k` closest vectors, with their text representations\n",
        "\n",
        "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
        "\n",
        "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
        "\n",
        "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76d96uavR6yw",
        "outputId": "bbfccc31-20a2-41c7-c14d-46554a43ed2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('f Capitan\\nTiago with a telegram, to open which he naturally requested the\\npermission of the others, who very naturally begged him to do so. The\\nworthy capitan at first knitted his eyebrows, then raised them;\\nhis face became pale, then lighted up as he hastily folded the paper\\nand arose.\\n\\n\"Gentlemen,\" he announced in confusion, \"his Excellency the\\nCaptain-General is coming this evening to honor my house.\" Thereupon he\\nset off at a run, hatless, taking with him the message and his napkin.\\n\\nHe was followed by exclamations and questions, for a cry of\\n\"Tulisanes!\" would not have produced greater effect. \"But,\\nlisten!\" \"When is he coming?\" \"Tell us about it!\" \"His Excellency!\" But\\nCapitan Tiago was already far away.\\n\\n\"His Excellency is coming and will stay at Capitan Tiago\\'s!\" exclaimed\\nsome without taking into consideration the fact that his daughter\\nand future son-in-law were present.\\n\\n\"The choice couldn\\'t be better,\" answered the latter.\\n\\nThe friars gazed at one another with looks that se',\n",
              "  0.6047060781825192),\n",
              " ('his cell, with his elbow upon the window\\nsill and his pale, worn cheek resting on the palm of his hand, he\\nwas gazing silently into the distance where a bright star glittered\\nin the dark sky. The star paled and disappeared, the dim light of the\\nwaning moon faded, but the friar did not move from his place--he was\\ngazing out over the field of Bagumbayan and the sleeping sea at the\\nfar horizon wrapped in the morning mist.\\n\\n\\n\\n\\nCHAPTER VI\\n\\nCapitan Tiago\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tThy will be done on earth.\\n\\n\\nWhile our characters are deep in slumber or busy with their breakfasts,\\nlet us turn our attention to Capitan Tiago. We have never had the\\nhonor of being his guest, so it is neither our right nor our duty to\\npass him by slightingly, even under the stress of important events.\\n\\nLow in stature, with a clear complexion, a corpulent figure and a\\nfull face, thanks to the liberal supply of fat which according to his\\nadmirers was the gift of Heaven and which his enemies averred was the\\nblood of the poor, Capi',\n",
              "  0.47338597110150865),\n",
              " ('pitan Tiago had great respect for this Chinese, who passed himself\\noff as a prophet and a physician. Examining the palm of the deceased\\nlady just before her daughter was born, he had prognosticated:\\n\"If it\\'s not a boy and doesn\\'t die, it\\'ll be a fine girl!\" [169] and\\nMaria Clara had come into the world to fulfill the infidel\\'s prophecy.\\n\\nCapitan Tiago, then, as a prudent and cautious man, could not decide\\nso easily as Trojan Paris--he could not so lightly give the preference\\nto one Virgin for fear of offending another, a situation that might be\\nfraught with grave consequences. \"Prudence!\" he said to himself. \"Let\\'s\\nnot go and spoil it all now.\"\\n\\nHe was still in the midst of these doubts when the governmental party\\narrived,--Doña Victorina, Don Tiburcio, and Linares. Doña Victorina did\\nthe talking for the three men as well as for herself. She mentioned\\nLinares\\' visits to the Captain-General and repeatedly insinuated\\nthe advantages of a relative of \"quality.\" \"Now,\" she concluded,\\n\"as we',\n",
              "  0.4705545555073407),\n",
              " ('sand pesos. Capitan Tiago hoped that the old woman\\nwould breathe her last almost any day, or that she would lose five or\\nsix of her lawsuits, so that he might be alone in serving God; but\\nunfortunately the best lawyers of the _Real Audiencia_ looked after\\nher interests, and as to her health, there was no part of her that\\ncould be attacked by sickness; she seemed to be a steel wire, no doubt\\nfor the edification of souls, and she hung on in this vale of tears\\nwith the tenacity of a boil on the skin. Her adherents were secure in\\nthe belief that she would be canonized at her death and that Capitan\\nTiago himself would have to worship her at the altars--all of which\\nhe agreed to and cheerfully promised, provided only that she die soon.\\n\\nSuch was Capitan Tiago in the days of which we write. As for the past,\\nhe was the only son of a sugar-planter of Malabon, wealthy enough,\\nbut so miserly that he would not spend a cent to educate his son,\\nfor which reason the little Santiago had been the serva',\n",
              "  0.4507500634338771),\n",
              " (\" of the rich guild of mestizos in spite of\\nthe protests of many of them, who did not regard him as one of\\nthemselves. In the two years that he held this office he wore out ten\\nfrock coats, an equal number of high hats, and half a dozen canes. The\\nfrock coat and the high hat were in evidence at the Ayuntamiento,\\nin the governor-general's palace, and at military headquarters; the\\nhigh hat and the frock coat might have been noticed in the cockpit,\\nin the market, in the processions, in the Chinese shops, and under the\\nhat and within the coat might have been seen the perspiring Capitan\\nTiago, waving his tasseled cane, directing, arranging, and throwing\\neverything into disorder with marvelous activity and a gravity even\\nmore marvelous.\\n\\nSo the authorities saw in him a safe man, gifted with the best of\\ndispositions, peaceful, tractable, and obsequious, who read no books\\nor newspapers from Spain, although he spoke Spanish well. Indeed,\\nthey rather looked upon him with the feeling with which a \",\n",
              "  0.4391004947648992),\n",
              " ('orcillo, Capitan Pablo,\\nCapitan Basilio, and Lucas, the man with the sear on his face who\\nfelt so deeply the death of his brother.\\n\\nCapitan Basilio approaches one of the townsmen and asks, \"Do you know\\nwhich cock Capitan Tiago is going to bring?\"\\n\\n\"I don\\'t know, sir. This morning two came, one of them the _lásak_\\nthat whipped the Consul\\'s _talisain_.\" [127]\\n\\n\"Do you think that my _bulik_ is a match for it?\"\\n\\n\"I should say so! I\\'ll bet my house and my camisa on it!\"\\n\\nAt that moment Capitan Tiago arrives, dressed like the heavy gamblers,\\nin a camisa of Canton linen, woolen pantaloons, and a wide straw\\nhat. Behind him come two servants carrying the _lásak_ and a white\\ncock of enormous size.\\n\\n\"Sinang tells me that Maria is improving all the time,\" says Capitan\\nBasilio.\\n\\n\"She has no more fever but is still very weak.\"\\n\\n\"Did you lose last night?\"\\n\\n\"A little. I hear that you won. I\\'m going to see if I can\\'t get\\neven here.\"\\n\\n\"Do you want to fight the _lásak?_\" asks Capitan Basilio, looking at\\n',\n",
              "  0.42698930003104246),\n",
              " ('complexion, a corpulent figure and a\\nfull face, thanks to the liberal supply of fat which according to his\\nadmirers was the gift of Heaven and which his enemies averred was the\\nblood of the poor, Capitan Tiago appeared to be younger than he really\\nwas; he might have been thought between thirty and thirty-five years of\\nage. At the time of our story his countenance always wore a sanctified\\nlook; his little round head, covered with ebony-black hair cut long in\\nfront and short behind, was reputed to contain many things of weight;\\nhis eyes, small but with no Chinese slant, never varied in expression;\\nhis nose was slender and not at all inclined to flatness; and if his\\nmouth had not been disfigured by the immoderate use of tobacco and\\nbuyo, which, when chewed and gathered in one cheek, marred the symmetry\\nof his features, we would say that he might properly have considered\\nhimself a handsome man and have passed for such. Yet in spite of this\\nbad habit he kept marvelously white both his natur',\n",
              "  0.42444938154838135),\n",
              " ('sses and, just as the Romans consulted\\nthe augurs before a battle, giving food to the sacred fowls, so Capitan\\nTiago would also consult his augurs, with the modifications befitting\\nthe times and the new truths, tie would watch closely the flame of\\nthe tapers, the smoke from the incense, the voice of the priest,\\nand from it all attempt to forecast his luck. It was an admitted\\nfact that he lost very few wagers, and in those cases it was due to\\nthe unlucky circumstance that the officiating priest was hoarse,\\nor that the altar-candles were few or contained too much tallow,\\nor that a bad piece of money had slipped in with the rest. The\\nwarden of the Brotherhood would then assure him that such reverses\\nwere tests to which he was subjected by Heaven to receive assurance\\nof his fidelity and devotion. So, beloved by the priests, respected\\nby the sacristans, humored by the Chinese chandlers and the dealers\\nin fireworks, he was a man happy in the religion of this world, and\\npersons of discernment',\n",
              "  0.4106400442244834),\n",
              " ('controlled by him and a\\nChinese brought in large profits. They also had the lucrative contract\\nof feeding the prisoners in Bilibid and furnished zacate to many of the\\nstateliest establishments in Manila u through the medium of contracts,\\nof course. Standing well with all the authorities, clever, cunning,\\nand even bold in speculating upon the wants of others, he was the only\\nformidable rival of a certain Perez in the matter of the farming-out of\\nrevenues and the sale of offices and appointments, which the Philippine\\ngovernment always confides to private persons. Thus, at the time of\\nthe events here narrated, Capitan Tiago was a happy man in so far as\\nit is possible for a narrow-brained individual to be happy in such\\na land: he was rich, and at peace with God, the government, and men.\\n\\nThat he was at peace with God was beyond doubt,--almost like religion\\nitself. There is no need to be on bad terms with the good God when one\\nis prosperous on earth, when one has never had any direct dealin',\n",
              "  0.4097145558959502),\n",
              " ('estial apparitions,\\none of them told him about the affair of Padre Damaso, with a somewhat\\nheightened coloring although substantially correct as to matter.\\n\\n\"From whom did you learn this?\" asked his Excellency, smiling.\\n\\n\"From Laruja, who was telling it this morning in the office.\"\\n\\nThe Captain-General again smiled and said: \"A woman or a friar can\\'t\\ninsult one. I contemplate living in peace for the time that I shall\\nremain in this country and I don\\'t want any more quarrels with men who\\nwear skirts. Besides, I\\'ve learned that the Provincial has scoffed\\nat my orders. I asked for the removal of this friar as a punishment\\nand they transferred him to a better town \\'monkish tricks,\\' as we\\nsay in Spain.\"\\n\\nBut when his Excellency found himself alone he stopped smiling. \"Ah,\\nif this people were not so stupid, I would put a curb on their\\nReverences,\" he sighed to himself. \"But every people deserves its fate,\\nso let\\'s do as everybody else does.\"\\n\\nCapitan Tiago, meanwhile, had concluded his inter',\n",
              "  0.40714209163307613)]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.search_by_text(\"Who is Capitan Tiago?\", k=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehsfIiKR6yw"
      },
      "source": [
        "## Task 5: Generate the Response\n",
        "\n",
        "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
        "\n",
        "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
        "\n",
        "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpA0UveR6yw"
      },
      "source": [
        "### XYZRolePrompt\n",
        "\n",
        "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
        "\n",
        "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
        "\n",
        "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
        "\n",
        "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
        "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
        "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
        "\n",
        "The main idea is this:\n",
        "\n",
        "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
        "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
        "3. Then, you prompt the model with the true \"user\" message.\n",
        "\n",
        "In this example, we'll be forgoing the 2nd step for simplicities sake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdZ2KWKSR6yw"
      },
      "source": [
        "#### Utility Functions\n",
        "\n",
        "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFbeJDDsR6yw"
      },
      "source": [
        "##### XYZRolePrompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mojJSE3R6yw"
      },
      "source": [
        "Here we have our `system`, `user`, and `assistant` role prompts.\n",
        "\n",
        "Let's take a peek at what they look like:\n",
        "\n",
        "```python\n",
        "class BasePrompt:\n",
        "    def __init__(self, prompt):\n",
        "        \"\"\"\n",
        "        Initializes the BasePrompt object with a prompt template.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        \"\"\"\n",
        "        self.prompt = prompt\n",
        "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
        "\n",
        "    def format_prompt(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Formats the prompt string using the keyword arguments provided.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: The formatted prompt string\n",
        "        \"\"\"\n",
        "        matches = self._pattern.findall(self.prompt)\n",
        "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
        "\n",
        "    def get_input_variables(self):\n",
        "        \"\"\"\n",
        "        Gets the list of input variable names from the prompt string.\n",
        "\n",
        "        :return: List of input variable names\n",
        "        \"\"\"\n",
        "        return self._pattern.findall(self.prompt)\n",
        "```\n",
        "\n",
        "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
        "\n",
        "```python\n",
        "class RolePrompt(BasePrompt):\n",
        "    def __init__(self, prompt, role: str):\n",
        "        \"\"\"\n",
        "        Initializes the RolePrompt object with a prompt template and a role.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
        "        \"\"\"\n",
        "        super().__init__(prompt)\n",
        "        self.role = role\n",
        "\n",
        "    def create_message(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Creates a message dictionary with a role and a formatted message.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: Dictionary containing the role and the formatted message\n",
        "        \"\"\"\n",
        "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
        "```\n",
        "\n",
        "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
        "\n",
        "```python\n",
        "class SystemRolePrompt(RolePrompt):\n",
        "    def __init__(self, prompt: str):\n",
        "        super().__init__(prompt, \"system\")\n",
        "```\n",
        "\n",
        "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D361R6sMR6yw"
      },
      "source": [
        "##### ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJVQ2Pm8R6yw"
      },
      "source": [
        "Next we have our model, which is converted to a format analagous to libraries like LangChain and LlamaIndex.\n",
        "\n",
        "Let's take a peek at how that is constructed:\n",
        "\n",
        "```python\n",
        "class ChatOpenAI:\n",
        "    def __init__(self, model_name: str = \"gpt-4o-mini\"):\n",
        "        self.model_name = model_name\n",
        "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if self.openai_api_key is None:\n",
        "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
        "\n",
        "    def run(self, messages, text_only: bool = True):\n",
        "        if not isinstance(messages, list):\n",
        "            raise ValueError(\"messages must be a list\")\n",
        "\n",
        "        openai.api_key = self.openai_api_key\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=self.model_name, messages=messages\n",
        "        )\n",
        "\n",
        "        if text_only:\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCU7FfhIR6yw"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### ❓ Question #3:\n",
        "\n",
        "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Answer:\n",
        "\n",
        "* Some of the things that can be done to achieve more reproducible outputs when calling OpenAI API are:\n",
        "\n",
        "1. Using a Fixed Model Version. By specifying the exact model version in the API call, it locks in the behavior of the model at a specific point in time. Example, using model \"text-embedding-ada-002\".\n",
        "\n",
        "2. Control Randomness for Text Generation Models. For text generation models (like gpt-4), randomness comes from parameters like temperature and top_p, so to make the outputs reproducible:\n",
        "\n",
        "    - set temperature to 0 to make the model pick the most likely response every time and thus reducing randomness\n",
        "    - set top_p to 1 or disable it to avoid sampling from less likely options\n",
        "    - for OpenAI, use a fixed see to make output more deterministic. If the same seed is provided with the same input, you should get the same output\n",
        "\n",
        "3. Normalize Input Text. Extra spaces, unnecessary capitalizations or punctuations can lead to different embeddings or outputs and a way to avoid this is by cleaning and standardizing the input before sending it to the API. It helps since it ensures that the model sees the exact same input every time which is critical for embeddings since they are sensitive to text changes.\n",
        "\n",
        "4. Use Consistent API Parameters. Always specify the same dimension size to avoid variations and stick to one encoding_format such as float or base64 for consistency.\n",
        "\n",
        "5. Cache Results for Repeated Inputs. caching same inputs if they are processed multiple times allows you to store the embeddings or outputs to avoid re-calling the API and save costs. Use a dictionary or a database like Pinecone to store input-output pairs.\n",
        "\n",
        "6. Handle Floating-Point Precision. As embeddings are lists of numbers, tiny differences in floating-point math precision across computers or API servers can cause slight variations during processing.\n",
        "\n",
        "7. Pin your code to specific API version.\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "> NOTE: Check out [this section](https://platform.openai.com/docs/guides/text-generation/) of the OpenAI documentation for the answer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5wcjMLCR6yw"
      },
      "source": [
        "### Creating and Prompting OpenAI's `gpt-4o-mini`!\n",
        "\n",
        "Let's tie all these together and use it to prompt `gpt-4o-mini`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WIfpIot7R6yw"
      },
      "outputs": [],
      "source": [
        "from psi.openai_utils.prompts import (\n",
        "    UserRolePrompt,\n",
        "    SystemRolePrompt,\n",
        "    AssistantRolePrompt,\n",
        ")\n",
        "\n",
        "from psi.openai_utils.chatmodel import ChatOpenAI\n",
        "\n",
        "chat_openai = ChatOpenAI()\n",
        "user_prompt_template = \"{content}\"\n",
        "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
        "system_prompt_template = (\n",
        "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
        ")\n",
        "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
        "\n",
        "messages = [\n",
        "    system_role_prompt.create_message(expertise=\"Python\"),\n",
        "    user_role_prompt.create_message(\n",
        "        content=\"What is the best way to write a loop?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = chat_openai.run(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHo7lssNR6yw",
        "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best way to write a loop in Python depends on what you're trying to achieve, but I'll share a few common techniques along with some best practices to keep in mind. \n",
            "\n",
            "### 1. **Using a `for` Loop:**\n",
            "The `for` loop is often the most readable and convenient way to iterate over a collection (like a list, tuple, or string).\n",
            "\n",
            "```python\n",
            "# Example of a for loop\n",
            "numbers = [1, 2, 3, 4, 5]\n",
            "\n",
            "for number in numbers:\n",
            "    print(number)\n",
            "```\n",
            "\n",
            "### 2. **Using a `while` Loop:**\n",
            "A `while` loop is useful when the number of iterations is not known beforehand and depends on a condition.\n",
            "\n",
            "```python\n",
            "# Example of a while loop\n",
            "count = 0\n",
            "\n",
            "while count < 5:\n",
            "    print(count)\n",
            "    count += 1\n",
            "```\n",
            "\n",
            "### 3. **Using List Comprehensions:**\n",
            "For simple loops where you want to create a new list based on an existing one, list comprehensions can be a concise and readable option.\n",
            "\n",
            "```python\n",
            "# Example of a list comprehension\n",
            "squared_numbers = [x ** 2 for x in range(6)]\n",
            "print(squared_numbers)  # Output: [0, 1, 4, 9, 16, 25]\n",
            "```\n",
            "\n",
            "### Best Practices:\n",
            "- **Readability:** Keep your loops clear and straightforward. Using meaningful variable names can enhance readability.\n",
            "  \n",
            "- **Avoid Deep Nesting:** If you find yourself nesting loops too deeply, consider if there’s a more efficient way to structure your code.\n",
            "\n",
            "- **Use Built-in Functions:** Whenever possible, use Python's built-in functions like `map()`, `filter()`, or `reduce()` from the `functools` module instead of manual loops for better performance and readability.\n",
            "\n",
            "- **Break and Continue:** Use `break` to exit a loop prematurely and `continue` to skip to the next iteration. Use these cautiously; they can make code harder to understand if overused.\n",
            "\n",
            "Here’s a combined example that showcases a for loop with `continue`:\n",
            "\n",
            "```python\n",
            "for number in range(10):\n",
            "    if number % 2 == 0:  # Skip even numbers\n",
            "        continue\n",
            "    print(number)  # Prints odd numbers only\n",
            "```\n",
            "\n",
            "Ultimately, the best way to write a loop can depend on the specific task and context, but focusing on clarity and efficiency will always help you create better code! If you have a specific scenario in mind, feel free to share, and I’d be happy to help further!\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2nxxhB2R6yy"
      },
      "source": [
        "## Task 5: Retrieval Augmented Generation\n",
        "\n",
        "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
        "\n",
        "There is much you could do here, many tweaks and improvements to be made!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "D1hamzGaR6yy"
      },
      "outputs": [],
      "source": [
        "RAG_SYSTEM_TEMPLATE = \"\"\"You are a knowledgeable assistant that answers questions based strictly on provided context.\n",
        "\n",
        "Instructions:\n",
        "- Only answer questions using information from the provided context\n",
        "- If the context doesn't contain relevant information, respond with \"I don't know\"\n",
        "- Be accurate and cite specific parts of the context when possible\n",
        "- Keep responses {response_style} and {response_length}\n",
        "- Only use the provided context. Do not use external knowledge.\n",
        "- Only provide answers when you are confident the context supports your response.\"\"\"\n",
        "\n",
        "RAG_USER_TEMPLATE = \"\"\"Context Information:\n",
        "{context}\n",
        "\n",
        "Number of relevant sources found: {context_count}\n",
        "{similarity_scores}\n",
        "\n",
        "Question: {user_query}\n",
        "\n",
        "Please provide your answer based solely on the context above.\"\"\"\n",
        "\n",
        "rag_system_prompt = SystemRolePrompt(\n",
        "    RAG_SYSTEM_TEMPLATE,\n",
        "    strict=True,\n",
        "    defaults={\n",
        "        \"response_style\": \"concise\",\n",
        "        \"response_length\": \"brief\"\n",
        "    }\n",
        ")\n",
        "\n",
        "rag_user_prompt = UserRolePrompt(\n",
        "    RAG_USER_TEMPLATE,\n",
        "    strict=True,\n",
        "    defaults={\n",
        "        \"context_count\": \"\",\n",
        "        \"similarity_scores\": \"\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RetrievalAugmentedQAPipeline:\n",
        "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase, \n",
        "                 response_style: str = \"detailed\", include_scores: bool = False) -> None:\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "        self.response_style = response_style\n",
        "        self.include_scores = include_scores\n",
        "\n",
        "    def run_pipeline(self, user_query: str, k: int = 4, **system_kwargs) -> dict:\n",
        "        # Retrieve relevant contexts\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=k)\n",
        "        \n",
        "        context_prompt = \"\"\n",
        "        similarity_scores = []\n",
        "        \n",
        "        for i, (context, score) in enumerate(context_list, 1):\n",
        "            context_prompt += f\"[Source {i}]: {context}\\n\\n\"\n",
        "            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n",
        "        \n",
        "        # Create system message with parameters\n",
        "        system_params = {\n",
        "            \"response_style\": self.response_style,\n",
        "            \"response_length\": system_kwargs.get(\"response_length\", \"detailed\")\n",
        "        }\n",
        "        \n",
        "        formatted_system_prompt = rag_system_prompt.create_message(**system_params)\n",
        "        \n",
        "        user_params = {\n",
        "            \"user_query\": user_query,\n",
        "            \"context\": context_prompt.strip(),\n",
        "            \"context_count\": len(context_list),\n",
        "            \"similarity_scores\": f\"Relevance scores: {', '.join(similarity_scores)}\" if self.include_scores else \"\"\n",
        "        }\n",
        "        \n",
        "        formatted_user_prompt = rag_user_prompt.create_message(**user_params)\n",
        "\n",
        "        return {\n",
        "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]), \n",
        "            \"context\": context_list,\n",
        "            \"context_count\": len(context_list),\n",
        "            \"similarity_scores\": similarity_scores if self.include_scores else None,\n",
        "            \"prompts_used\": {\n",
        "                \"system\": formatted_system_prompt,\n",
        "                \"user\": formatted_user_prompt\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: I don't know.\n",
            "\n",
            "Context Count: 3\n",
            "Similarity Scores: ['Source 1: 0.175', 'Source 2: 0.170', 'Source 3: 0.167']\n"
          ]
        }
      ],
      "source": [
        "rag_pipeline = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db,\n",
        "    llm=chat_openai,\n",
        "    response_style=\"detailed\",\n",
        "    include_scores=True\n",
        ")\n",
        "\n",
        "result = rag_pipeline.run_pipeline(\n",
        "    \"Who is President Trump?\",\n",
        "    k=3,\n",
        "    response_length=\"comprehensive\", \n",
        "    include_warnings=True,\n",
        "    confidence_required=True\n",
        ")\n",
        "\n",
        "print(f\"Response: {result['response']}\")\n",
        "print(f\"\\nContext Count: {result['context_count']}\")\n",
        "print(f\"Similarity Scores: {result['similarity_scores']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### 🎯 Breakout Room - Group Discussion: \n",
        "\n",
        "Explore different prompts and check if RAG is able to correctly answer the question. Write down final answer below.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Observations: \n",
        "\n",
        "* If RAG is prompted with some strict and specific instructions such as the following:\n",
        "\n",
        "RAG_SYSTEM_TEMPLATE = \"\"\"You are a knowledgeable assistant that answers questions based strictly on provided context.\n",
        "\n",
        "Instructions:\n",
        "- Only answer questions using information from the provided context\n",
        "- If the context doesn't contain relevant information, respond with \"I don't know\"\n",
        "- Be accurate and cite specific parts of the context when possible\n",
        "- Keep responses {response_style} and {response_length}\n",
        "- Only use the provided context. Do not use external knowledge.\n",
        "- Only provide answers when you are confident the context supports your response.\"\"\"\n",
        "\n",
        "Then its response is very specific and related to the context:\n",
        "\n",
        "Response: Capitan Tiago is a character described as low in stature, with a clear complexion, a corpulent figure, and a full face attributed to his abundant flesh, which some admirers viewed as a divine gift while his enemies criticized it as the result of the suffering of the poor. He is depicted as a prudent and cautious man, concerned with making decisions that might offend others, especially in the context of choosing between two Virgins for a significant matter, indicating his careful nature.\n",
        "\n",
        "He is noted to have great respect for a Chinese man who claimed to be a prophet and physician, who once made a prophecy about his daughter Maria Clara, suggesting a degree of belief in omens and fortunes. Additionally, Capitan Tiago is depicted as being involved in social interactions and governance, hosting important guests such as the Captain-General at his house, which highlights his status in the society. \n",
        "\n",
        "Overall, he is portrayed as a significant figure within the community, balancing social expectations and personal relationships.\n",
        "\n",
        "Context Count: 3\n",
        "Similarity Scores: ['Source 1: 0.605', 'Source 2: 0.472', 'Source 3: 0.470']\n",
        "\n",
        "=================================================================================================================================================================\n",
        "\n",
        "And if we ask a question that is irrelevant to the context such as \"Who is President Trump\", then the model says:\n",
        "\n",
        "Response: I don't know.\n",
        "\n",
        "Context Count: 3\n",
        "Similarity Scores: ['Source 1: 0.175', 'Source 2: 0.170', 'Source 3: 0.167']\n",
        "\n",
        "=================================================================================================================================================================\n",
        "\n",
        "However, if we remove restrictions and ask about \"Who is President Trump?\", then the model answered the question even though the similarity scores were very low or out of context:\n",
        "\n",
        "Response: Donald John Trump, born on June 14, 1946, is an American businessman, television personality, and politician who served as the 45th president of the United States from January 20, 2017, to January 20, 2021. Before entering politics, Trump gained prominence as a real estate developer and businessman, inheriting his family's real estate business in New York City, which he later expanded into a large and multifaceted enterprise. He is also known for his branding endeavors, which include real estate developments, golf courses, and hotels, along with a range of consumer products.\n",
        "\n",
        "Trump's political career is marked by his unconventional style and his use of social media platforms, particularly Twitter, to communicate directly with the public. He ran for president as a Republican in the 2016 election, campaigning on a platform that included strict immigration policies, trade reform, and a focus on \"America First\" policies. His campaign adopted a populist tone, appealing to a segment of the electorate dissatisfied with the political establishment. \n",
        "\n",
        "In the 2016 election, Trump won against the Democratic nominee Hillary Clinton, earning a significant Electoral College victory while losing the popular vote. His presidency was characterized by a series of controversial policies and actions, including tax cuts, immigration restrictions, trade disputes with various countries, and the nomination of three Supreme Court justices, which shifted the Court's ideological balance.\n",
        "\n",
        "Moreover, Trump's term included extensive investigations into Russian interference in the 2016 election, which ultimately led to his impeachment by the House of Representatives in December 2019 on charges of abuse of power and obstruction of Congress. He was acquitted by the Senate in early 2020.\n",
        "\n",
        "After losing the 2020 presidential election to Democrat Joe Biden, Trump contested the results, claiming widespread voter fraud without presenting substantial evidence. He was impeached a second time by the House in January 2021 for incitement of insurrection following the storming of the Capitol on January 6, 2021, by his supporters. He was again acquitted by the Senate.\n",
        "\n",
        "In addition to his political and business career, Trump is also known for his role as the host of the reality television series \"The Apprentice,\" further contributing to his celebrity status. His leadership style and decisions have continued to influence and polarize American politics and societal discourses even after leaving office.\n",
        "\n",
        "Context Count: 3\n",
        "Similarity Scores: ['Source 1: 0.175', 'Source 2: 0.170', 'Source 3: 0.167']\n",
        "\n",
        "=================================================================================================================================================================\n",
        "\n",
        "In summary, altering the prompt specification to restrict or loosen the expected behaviour within or out of context will definitely cause AI to respond accordingly.\n",
        " \n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ce393d9afcf427d9d352259c5d32678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e6efd99f7d346e485b002fb0fa85cc7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3dfb67c39958461da6071e4c19c3fa41",
            "value": 1
          }
        },
        "3a4ba348cb004f8ab7b2b1395539c81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ea5009dd16442cb5d8a0ac468e50a8",
            "placeholder": "​",
            "style": "IPY_MODEL_5f00135fe1044051a50ee5e841cbb8e3",
            "value": "0.018 MB of 0.018 MB uploaded\r"
          }
        },
        "3dfb67c39958461da6071e4c19c3fa41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e6efd99f7d346e485b002fb0fa85cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a8e24025594e5e9ff3b8581c344691": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f00135fe1044051a50ee5e841cbb8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb904e05ece143c79ecc4f20de482f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a4ba348cb004f8ab7b2b1395539c81b",
              "IPY_MODEL_1ce393d9afcf427d9d352259c5d32678"
            ],
            "layout": "IPY_MODEL_56a8e24025594e5e9ff3b8581c344691"
          }
        },
        "d2ea5009dd16442cb5d8a0ac468e50a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
