{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll go over some of the key features of the OpenAI Agents SDK - as explored through a notebook-ified version of their [Research Bot](https://github.com/openai/openai-agents-python/tree/main/examples/research_bot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You don't need to run this cell if you're running this notebook locally. \n",
    "\n",
    "#!pip install -qU openai-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nest Async:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "As may be expected, the primary thing we'll do in the Agents SDK is construct Agents!\n",
    "\n",
    "Agents are constructed with a few basic properties:\n",
    "\n",
    "- A prompt, which OpenAI is using the language \"instruction\" for, that determines the behaviour or goal of the Agent\n",
    "- A model, the \"brain\" of the Agent\n",
    "\n",
    "They also typically include an additional property: \n",
    "\n",
    "- Tool(s) that equip the Agent with things it can use to get stuff done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Create Planner Agent\n",
    "\n",
    "Let's start by creating our \"Planner Agent\" - which will come up with the initial set of search terms that should answer a query provided by the user. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import Agent\n",
    "\n",
    "PLANNER_PROMPT = (\n",
    "    \"You are a helpful research assistant. Given a query, come up with a set of web searches to perform\" \n",
    "    \"to best answer the query. Output between 5 and 20 terms to query for.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the data models that our Planner Agent will use to structure its output. We'll create:\n",
    "\n",
    "1. `WebSearchItem` - A model for individual search items, containing the search query and reasoning\n",
    "2. `WebSearchPlan` - A container model that holds a list of search items\n",
    "\n",
    "These Pydantic models will help ensure our agent returns structured data that we can easily process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebSearchItem(BaseModel):\n",
    "    reason: str\n",
    "    \"Your reasoning for why this search is important to the query.\"\n",
    "\n",
    "    query: str\n",
    "    \"The search term to use for the web search.\"\n",
    "\n",
    "class WebSearchPlan(BaseModel):\n",
    "    searches: list[WebSearchItem]\n",
    "    \"\"\"A list of web searches to perform to best answer the query.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our Planner Agent using the Agent class from the OpenAI Agents SDK. This agent will use the instructions defined in `PLANNER_PROMPT` and will output structured data in the form of our WebSearchPlan model. We're using the GPT-4o model for this agent to ensure high-quality search term generation.\n",
    "\n",
    "> NOTE: When we provide an `output_type` - the model will return a [structured response](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_agent = Agent(\n",
    "    name=\"PlannerAgent\",\n",
    "    instructions=PLANNER_PROMPT,\n",
    "    model=\"gpt-4.1\",\n",
    "    output_type=WebSearchPlan,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“Question #1:\n",
    "\n",
    "Why is it important to provide a structured response template? (As in: Why are structured outputs helpful/preferred in Agentic workflows?)\n",
    "\n",
    "Answer #1:\n",
    "\n",
    "Structured response templates are important in agentic workflows because they ensure consistency, reliability, and machine-readability of the modelâ€™s outputs, which is critical when chaining multiple tools or agents together. Without a structured format, downstream components may misinterpret free-form text, leading to errors or unpredictable behavior. By enforcing a schema (e.g., JSON with predefined fields), developers can guarantee that outputs align with expected data types, reduce the need for brittle text parsing, and make the overall workflow more robust, interpretable, and easier to debug or extend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Create Search Agent\n",
    "\n",
    "Now we'll create our Search Agent, which will be responsible for executing web searches based on the terms generated by the Planner Agent. This agent will take each search query, perform a web search using the `WebSearchTool`, and then summarize the results in a concise format.\n",
    "\n",
    "> NOTE: We are using the `WebSearchTool`, a hosted tool that can be used as part of an `OpenAIResponsesModel` as outlined in the [documentation](https://openai.github.io/openai-agents-python/tools/). This is based on the tools available through OpenAI's new [Responses API](https://openai.com/index/new-tools-for-building-agents/).\n",
    "\n",
    "The `SEARCH_PROMPT` below instructs the agent to create brief, focused summaries of search results. These summaries are designed to be 2-3 paragraphs, under 300 words, and capture only the essential information without unnecessary details. The goal is to provide the Writer Agent with clear, distilled information that can be efficiently synthesized into the final report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_PROMPT = (\n",
    "    \"You are a research assistant. Given a search term, you search the web for that term and\"\n",
    "    \"produce a concise summary of the results. The summary must 2-3 paragraphs and less than 300\"\n",
    "    \"words. Capture the main points. Write succinctly, no need to have complete sentences or good\"\n",
    "    \"grammar. This will be consumed by someone synthesizing a report, so its vital you capture the\"\n",
    "    \"essence and ignore any fluff. Do not include any additional commentary other than the summary\"\n",
    "    \"itself.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our Search Agent using the Agent class from the OpenAI Agents SDK. This agent will use the instructions defined in `SEARCH_PROMPT` and will utilize the `WebSearchTool` to perform web searches. We're configuring it with `tool_choice=\"required\"` to ensure it always uses the search tool when processing requests.\n",
    "\n",
    "> NOTE: We can, as demonstrated, indicate how we want our model to use tools. You can read more about that at the bottom of the page [here](https://openai.github.io/openai-agents-python/agents/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import WebSearchTool\n",
    "from agents.model_settings import ModelSettings\n",
    "\n",
    "search_agent = Agent(\n",
    "    name=\"Search agent\",\n",
    "    instructions=SEARCH_PROMPT,\n",
    "    tools=[WebSearchTool()],\n",
    "    model_settings=ModelSettings(tool_choice=\"required\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ Question #2: \n",
    "\n",
    "What other tools are supported in OpenAI's Responses API?\n",
    "\n",
    "Answer #2: \n",
    "\n",
    "OpenAI's Responses API is the new, unified interface for building agentic workflows, and it includes several first-party, built-in tools developers can invoke directly:\n",
    "\n",
    "- Web Search: Provides real-time internet search with citations. It uses the same search model powering ChatGPT and delivers timely, accurate results.\n",
    "\n",
    "- File Search: Lets you search over your uploaded documents or vector stores to retrieve relevant passagesâ€”ideal for Retrieval-Augmented Generation scenarios.\n",
    "\n",
    "- Computer Use (Code Interpreter / Operator): Enables the model to perform actions on your computerâ€”like typing, clicking, or navigating a GUIâ€”by leveraging screenshot feedback. This can automate tasks that previously required manual interaction.\n",
    "\n",
    "- Image Generation: You can generate images from text prompts using built-in capabilitiesâ€”though note it may require organization-level permission.\n",
    "\n",
    "- Function Calling: Similar to Chat Completions API, you can define custom functions/tools and have the model call them. This broadens capabilities beyond built-in tools.\n",
    "\n",
    "- Model Context Protocol (MCP) Tooling: You can integrate with MCP servers for external systems (e.g., GitHub, Stripe, enterprise databases). MCP enables pre-registered tools to be used within agent workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Create Writer Agent\n",
    "\n",
    "Finally, we'll create our Writer Agent, which will synthesize all the research findings into a comprehensive report. This agent takes the original query and the research summaries from the Search Agent, then produces a structured report with follow-up questions.\n",
    "\n",
    "The Writer Agent will:\n",
    "1. Create an outline for the report structure\n",
    "2. Generate a detailed markdown report (5-10 pages)\n",
    "3. Provide follow-up questions for further research\n",
    "\n",
    "We'll define the prompt for this agent in the next cell. This prompt will instruct the Writer Agent on how to synthesize research findings into a comprehensive report with follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_PROMPT = (\n",
    "    \"You are a senior researcher tasked with writing a cohesive report for a research query. \"\n",
    "    \"You will be provided with the original query, and some initial research done by a research \"\n",
    "    \"assistant.\\n\"\n",
    "    \"You should first come up with an outline for the report that describes the structure and \"\n",
    "    \"flow of the report. Then, generate the report and return that as your final output.\\n\"\n",
    "    \"The final output should be in markdown format, and it should be lengthy and detailed. Aim \"\n",
    "    \"for 5-10 pages of content, at least 1000 words.\\n\"\n",
    "    \"For the follow-up questions, provide exactly 5 unique questions that would help extend \"\n",
    "    \"this research. Do not repeat questions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ—ï¸ Activity #1: \n",
    "\n",
    "This prompt is quite generic - modify this prompt to produce a report that is more personalized to either your personal preference, or more appropriate for a specific use case (eg. law domain research)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our Writer Agent using the Agent class from the OpenAI Agents SDK. This agent will synthesize all the research findings into a comprehensive report. We're configuring it with the `ReportData` output type to structure the response with a short summary, markdown report, and follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportData(BaseModel):\n",
    "    short_summary: str\n",
    "    \"\"\"A short 2-3 sentence summary of the findings.\"\"\"\n",
    "\n",
    "    markdown_report: str\n",
    "    \"\"\"The final report\"\"\"\n",
    "\n",
    "    follow_up_questions: list[str]\n",
    "    \"\"\"Suggested topics to research further\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define our Writer Agent using the Agent class from the OpenAI Agents SDK. This agent will take the original query and research summaries, then synthesize them into a comprehensive report with follow-up questions. We've defined a custom output type called `ReportData` that structures the response with a short summary, markdown report, and follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_agent = Agent(\n",
    "    name=\"WriterAgent\",\n",
    "    instructions=WRITER_PROMPT,\n",
    "    model=\"o3-mini\",\n",
    "    output_type=ReportData,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ Question #3: \n",
    "\n",
    "Why are we electing to use a reasoning model for writing our report?\n",
    "\n",
    "Asnwer #3:\n",
    "\n",
    "We are electing to use a reasoning model for writing our report because reasoning models are designed to handle tasks that require structured analysis, step-by-step logic, and synthesis of information across multiple sources. Unlike standard completion models that may optimize for fluency or brevity, reasoning models explicitly track intermediate steps, which reduces errors, ensures consistency, and makes the report more reliable. This is especially valuable when comparing retrievers, weighing trade-offs like cost, latency, and performance, or when we need to present structured, evidence-based insights. In short, reasoning models help us move beyond surface-level summaries and toward clear, well-justified conclusions that hold up under scrutiny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Create Utility Classes \n",
    "\n",
    "We'll define utility classes to help with displaying progress and managing the research workflow. The Printer class below will provide real-time updates on the research process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Printer class provides real-time progress updates during the research process. It uses Rich's Live display to show dynamic content with spinners for in-progress items and checkmarks for completed tasks. The class maintains a dictionary of items with their completion status and can selectively hide checkmarks for specific items. This creates a clean, interactive console experience that keeps the user informed about the current state of the research workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from rich.console import Console, Group\n",
    "from rich.live import Live\n",
    "from rich.spinner import Spinner\n",
    "\n",
    "class Printer:\n",
    "    def __init__(self, console: Console):\n",
    "        self.live = Live(console=console)\n",
    "        self.items: dict[str, tuple[str, bool]] = {}\n",
    "        self.hide_done_ids: set[str] = set()\n",
    "        self.live.start()\n",
    "\n",
    "    def end(self) -> None:\n",
    "        self.live.stop()\n",
    "\n",
    "    def hide_done_checkmark(self, item_id: str) -> None:\n",
    "        self.hide_done_ids.add(item_id)\n",
    "\n",
    "    def update_item(\n",
    "        self, item_id: str, content: str, is_done: bool = False, hide_checkmark: bool = False\n",
    "    ) -> None:\n",
    "        self.items[item_id] = (content, is_done)\n",
    "        if hide_checkmark:\n",
    "            self.hide_done_ids.add(item_id)\n",
    "        self.flush()\n",
    "\n",
    "    def mark_item_done(self, item_id: str) -> None:\n",
    "        self.items[item_id] = (self.items[item_id][0], True)\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self) -> None:\n",
    "        renderables: list[Any] = []\n",
    "        for item_id, (content, is_done) in self.items.items():\n",
    "            if is_done:\n",
    "                prefix = \"âœ… \" if item_id not in self.hide_done_ids else \"\"\n",
    "                renderables.append(prefix + content)\n",
    "            else:\n",
    "                renderables.append(Spinner(\"dots\", text=content))\n",
    "        self.live.update(Group(*renderables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a ResearchManager class that will orchestrate the research process. This class will:\n",
    "1. Plan searches based on the query\n",
    "2. Perform those searches to gather information\n",
    "3. Write a comprehensive report based on the gathered information\n",
    "4. Display progress using our Printer class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "from agents import Runner, custom_span, gen_trace_id, trace\n",
    "\n",
    "class ResearchManager:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        self.printer = Printer(self.console)\n",
    "\n",
    "    async def run(self, query: str) -> None:\n",
    "        trace_id = gen_trace_id()\n",
    "        with trace(\"Research trace\", trace_id=trace_id):\n",
    "            self.printer.update_item(\n",
    "                \"trace_id\",\n",
    "                f\"View trace: https://platform.openai.com/traces/trace?trace_id={trace_id}\",\n",
    "                is_done=True,\n",
    "                hide_checkmark=True,\n",
    "            )\n",
    "\n",
    "            self.printer.update_item(\n",
    "                \"starting\",\n",
    "                \"Starting research...\",\n",
    "                is_done=True,\n",
    "                hide_checkmark=True,\n",
    "            )\n",
    "            search_plan = await self._plan_searches(query)\n",
    "            search_results = await self._perform_searches(search_plan)\n",
    "            report = await self._write_report(query, search_results)\n",
    "\n",
    "            final_report = f\"Report summary\\n\\n{report.short_summary}\"\n",
    "            self.printer.update_item(\"final_report\", final_report, is_done=True)\n",
    "\n",
    "            self.printer.end()\n",
    "\n",
    "        print(\"\\n\\n=====REPORT=====\\n\\n\")\n",
    "        print(f\"Report: {report.markdown_report}\")\n",
    "        print(\"\\n\\n=====FOLLOW UP QUESTIONS=====\\n\\n\")\n",
    "        unique_questions = []\n",
    "        seen = set()\n",
    "        \n",
    "        for question in report.follow_up_questions:\n",
    "            if question not in seen:\n",
    "                unique_questions.append(question)\n",
    "                seen.add(question)\n",
    "        \n",
    "        for i, question in enumerate(unique_questions, 1):\n",
    "            print(f\"{i}. {question}\")\n",
    "\n",
    "    async def _plan_searches(self, query: str) -> WebSearchPlan:\n",
    "        self.printer.update_item(\"planning\", \"Planning searches...\")\n",
    "        result = await Runner.run(\n",
    "            planner_agent,\n",
    "            f\"Query: {query}\",\n",
    "        )\n",
    "        self.printer.update_item(\n",
    "            \"planning\",\n",
    "            f\"Will perform {len(result.final_output.searches)} searches\",\n",
    "            is_done=True,\n",
    "        )\n",
    "        return result.final_output_as(WebSearchPlan)\n",
    "\n",
    "    async def _perform_searches(self, search_plan: WebSearchPlan) -> list[str]:\n",
    "        with custom_span(\"Search the web\"):\n",
    "            self.printer.update_item(\"searching\", \"Searching...\")\n",
    "            num_completed = 0\n",
    "            max_concurrent = 5\n",
    "            results = []\n",
    "            \n",
    "            for i in range(0, len(search_plan.searches), max_concurrent):\n",
    "                batch = search_plan.searches[i:i+max_concurrent]\n",
    "                tasks = [asyncio.create_task(self._search(item)) for item in batch]\n",
    "                \n",
    "                for task in asyncio.as_completed(tasks):\n",
    "                    try:\n",
    "                        result = await task\n",
    "                        if result is not None:\n",
    "                            results.append(result)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Search error: {e}\")\n",
    "                        \n",
    "                    num_completed += 1\n",
    "                    self.printer.update_item(\n",
    "                        \"searching\", f\"Searching... {num_completed}/{len(search_plan.searches)} completed\"\n",
    "                    )\n",
    "            \n",
    "            self.printer.mark_item_done(\"searching\")\n",
    "            return results\n",
    "\n",
    "    async def _search(self, item: WebSearchItem) -> str | None:\n",
    "        input = f\"Search term: {item.query}\\nReason for searching: {item.reason}\"\n",
    "        try:\n",
    "            result = await Runner.run(\n",
    "                search_agent,\n",
    "                input,\n",
    "            )\n",
    "            return str(result.final_output)\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for '{item.query}': {e}\")\n",
    "            return None\n",
    "\n",
    "    async def _write_report(self, query: str, search_results: list[str]) -> ReportData:\n",
    "        self.printer.update_item(\"writing\", \"Thinking about report...\")\n",
    "        input = f\"Original query: {query}\\nSummarized search results: {search_results}\"\n",
    "        \n",
    "        result = Runner.run_streamed(\n",
    "            writer_agent,\n",
    "            input,\n",
    "        )\n",
    "        \n",
    "        update_messages = [\n",
    "            \"Thinking about report...\",\n",
    "            \"Planning report structure...\",\n",
    "            \"Writing outline...\",\n",
    "            \"Creating sections...\",\n",
    "            \"Cleaning up formatting...\",\n",
    "            \"Finalizing report...\",\n",
    "            \"Finishing report...\",\n",
    "        ]\n",
    "\n",
    "        last_update = time.time()\n",
    "        next_message = 0\n",
    "        \n",
    "        async for event in result.stream_events():\n",
    "            if time.time() - last_update > 5 and next_message < len(update_messages):\n",
    "                self.printer.update_item(\"writing\", update_messages[next_message])\n",
    "                next_message += 1\n",
    "                last_update = time.time()\n",
    "\n",
    "        self.printer.mark_item_done(\"writing\")\n",
    "        return result.final_output_as(ReportData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ—ï¸ Activity #2:\n",
    "\n",
    "Convert the above flow into a flowchart style image (software of your choosing, but if you're not sure which to use try [Excallidraw](https://excalidraw.com/)) that outlines how the different Agents interact with each other. \n",
    "\n",
    "> HINT: Cursor's AI (CMD+L or CTRL+L on Windows) would be a helpful way to get a basic diagram that you can add more detail to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Running Our Agent\n",
    "\n",
    "Now let's run our agent! The main function below will prompt the user for a research topic, then pass that query to our ResearchManager to handle the entire research process. The ResearchManager will: \n",
    "\n",
    "1. Break down the query into search items\n",
    "2. Search for information on each item\n",
    "3. Write a comprehensive report based on the search results\n",
    "\n",
    "Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main() -> None:\n",
    "    query = input(\"What would you like to research? \")\n",
    "    await ResearchManager().run(query)\n",
    "\n",
    "    query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e456a45ac9a4efdb59bc0e1f2693e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=====REPORT=====\n",
      "\n",
      "\n",
      "Report: # The State of Artificial Intelligence in 2025: Leaders, Benchmarks, and Future Directions\n",
      "\n",
      "## Introduction\n",
      "\n",
      "The rapid evolution of artificial intelligence (AI) over the past few years shows no sign of slowing down. With the advent of increasingly sophisticated models, the question of \"which AI is the best\" has taken center stage. However, determining the best model involves considering a myriad of factors such as performance benchmarks, application niches, market impact, and technological innovations. This report presents an extensive review of the current AI landscape as of 2025, examining the leading models, their performance on a variety of benchmarks, the key companies driving the industry, and the challenges and opportunities that lie ahead.\n",
      "\n",
      "## Overview of Leading AI Models\n",
      "\n",
      "In the evolving world of AI, several models have risen to prominence by demonstrating exceptional capabilities in specific application domains and across a range of benchmarks. Below are some of the leaders:\n",
      "\n",
      "- **OpenAI's GPT-5**: Launched in August 2025, GPT-5 is lauded for its superior mathematics, scientific, and coding capabilities. Its introduction of \"vibe coding\" for prompt-based software development has made it a favorite among developers and business users alike. Its strong performance on benchmarks combined with significant market adoption underscores its leadership position in natural language processing.\n",
      "\n",
      "- **Anthropic's Claude Series (including Claude 4 and Claude 4.1)**: Anthropic's models have been engineered to excel in reasoning-intensive tasks, including complex code generation, logical reasoning, and multi-modal processing. They have been recognized particularly for their performance in scientific and coding benchmarks, positioning them as strong competitors to models like GPT-5.\n",
      "\n",
      "- **Google's Gemini Series (Gemini 2.5 Pro)**: Gemini 2.5 Pro, released during Google I/O 2025, emphasizes enhanced reasoning, multimodal integration, and a massive context window that supports diverse applications. Its integration with enterprise tools like Google Workspace reinforces Googleâ€™s broad ecosystem reach.\n",
      "\n",
      "- **Meta's Llama 3.3 and xAI's Grok 4 Heavy**: Meta and xAI have also entered this competitive field with capable models. Llama 3.3 offers varied parameter sizes and has integrated well with Metaâ€™s social media platforms. Meanwhile, xAI's Grok 4 Heavy has demonstrated impressive reasoning skills and a less filtered approach, appealing to users who value transparency and open-source initiatives.\n",
      "\n",
      "- **Upstage's Solar Pro 2**: Although fewer parameters are used by this model, its performance in benchmark tests is exceptionally strong, highlighting that sheer scale is not always synonymous with superior output. This model has set new standards in many areas, cementing Upstage as a key player, particularly in multinational and high-stake environments.\n",
      "\n",
      "## Benchmarks and Evaluations\n",
      "\n",
      "Benchmarking tools have become indispensable for evaluating the performance of AI systems. Traditional tests are rapidly becoming obsolete as new standards are developed to measure capabilities like complex reasoning, real-world problem-solving, and multi-modal processing. Key developments include:\n",
      "\n",
      "- **Custom Benchmarks**: Tools such as MMLU, ARC, GSM8K, and newer domain-specific tests (e.g., GPQA, SWE-bench Verified, and the OlympicArena benchmark) focus on areas from basic reasoning to complex scientific and coding tasks. For example, GPT-5 leads in various coding benchmarks, while Claude models often excel in domains such as physics, chemistry, and biology.\n",
      "\n",
      "- **Multi-Dimensional Evaluations**: Modern benchmarks now assess not just correctness but also other parameters like latency, cost efficiency, and energy consumption. Initiatives such as the ML Research Benchmark (MLRB) and Eureka-Bench provide a more rounded view of performance and are critical for understanding both strengths and potential limitations.\n",
      "\n",
      "- **Continuous Evolution**: With AI models improving at a rapid pace, old benchmarks are quickly replaced or updated. Companies are now investing in internal benchmarks and collaborating with independent organizations to ensure unbiased evaluations. This trend illustrates the dynamic nature of AI evaluation methodologies and highlights the industry's dedication to maintaining rigorous testing environments.\n",
      "\n",
      "## Technological Innovations and Industry Impact\n",
      "\n",
      "Not only do the performance metrics of these AI models signify progress, but their impact on various industries also speaks volumes. The report highlights several technological and business outcomes influenced by these AI advancements:\n",
      "\n",
      "- **Coding and Development:** GPT-5â€™s introduction of advanced coding assistance and the concept of \"vibe coding\" showcase how AI is streamlining software development. Similar models by Anthropic and Google are also enabling faster, more accurate code generation, thus reducing development cycles and improving productivity.\n",
      "\n",
      "- **Scientific Research:** Models like Google DeepMindâ€™s initiatives (including AlphaFold for protein structure prediction) underpin revolutionary changes in scientific research. Enhanced predictive models are accelerating research in fields such as biology, chemistry, and even emerging quantum computing applications.\n",
      "\n",
      "- **Enterprise Adoption:** Corporations like Microsoft, Google, and NVIDIA continue to invest heavily in AI, integrating these models into cloud services, enterprise software, and hardware. Microsoftâ€™s Azure platform now leverages advanced AI services which power everything from natural language processing to complex data analysis. This has led to a marked increase in market share and industry influence.\n",
      "\n",
      "- **Consumer Applications:** With widespread applications ranging from personal assistants to creative platforms like AI-based image generation and content creation, AI is deeply embedded in daily life. Tools like ChatGPT continue to dominate user engagements despite fierce competition, while apps like DeepSeekâ€™s Janus Pro are setting new benchmarks for creative applications.\n",
      "\n",
      "- **Safety and Ethics:** As models become increasingly powerful, concerns about AI biases, data privacy, and ethical usage have grown. Regulatory frameworks such as the European Unionâ€™s AI Act are shaping how companies deploy AI. This, in turn, has led to investments in frameworks like the \"LLM Checker\" for compliance, ensuring that developers consider both performance and responsible AI practices.\n",
      "\n",
      "## The Role of Industry Leaders\n",
      "\n",
      "The AI industry is characterized by the dynamic interplay between established tech giants and nimble startups. Companies like OpenAI, Google DeepMind, and Anthropic not only drive technological innovation but also set strategic trends in research, funding, and market penetration:\n",
      "\n",
      "- **OpenAI**: By continuously pushing the boundaries of what large language models can achieve, OpenAI remains at the forefront. Their ability to scale models such as GPT-5 has far-reaching implications, both in terms of market adoption and technical breakthroughs. OpenAIâ€™s rapid growth, supported by significant investments, has catalyzed developments across multiple sectors.\n",
      "\n",
      "- **Google DeepMind**: With both its deep research roots and enterprise reach, Google DeepMind effectively bridges academia and industry. The focus on AGI (Artificial General Intelligence), as well as durable contributions to scientific research (e.g., AlphaFold), underscores Googleâ€™s holistic approach to AI innovation.\n",
      "\n",
      "- **Anthropic**: Emphasizing safety and ethical AI, Anthropic has distinguished itself by designing models that are both highly capable and mindful of deployment risks. Their initiatives targeting governmental applications highlight a strategic move to foster public trust and regulatory compliance.\n",
      "\n",
      "- **Hardware and Infrastructure Leaders**: Beyond software, companies like NVIDIA and Microsoft invest in the physical infrastructure required to support advanced AI. NVIDIA's dominance in GPU production is fundamental to the training and deployment of these large-scale models, while Microsoftâ€™s cloud investments ensure that AI services are scalable and reliable.\n",
      "\n",
      "## Challenges and Future Directions\n",
      "\n",
      "Despite these impressive advancements, the AI field faces several challenges that could shape future developments:\n",
      "\n",
      "- **Benchmark Limitations**: Although advanced benchmarks exist, no single test fully captures an AI model's capability. The difficulty in simulating all real-world applications means that models excelling on paper may underperform in practical scenarios. Continued innovation in benchmark design is critical for future assessments.\n",
      "\n",
      "- **Ethics and Bias**: With increasing reliance on AI systems, ensuring fairness, transparency, and accountability remains a pressing concern. The industry must address the lasting influence of biased training data and strive towards models that are both robust and equitable.\n",
      "\n",
      "- **Energy Consumption and Efficiency**: As models grow larger and more capable, the energy required for training and inference has become a sustainability issue. Innovations that minimize energy expenditures without compromising performance are essential for the industryâ€™s long-term viability.\n",
      "\n",
      "- **Regulatory and Compliance Pressures**: With regulatory frameworks, such as the EU AI Act, continuing to evolve, companies must navigate a complex landscape of compliance and legal accountability. This will likely drive more research into safe and transparent AI systems that mitigate risks while pushing technological boundaries.\n",
      "\n",
      "- **General Versus Specialized Intelligence**: While current models demonstrate remarkable proficiency in specialized tasks, the journey toward true Artificial General Intelligence (AGI) remains long and filled with challenges. Researchers are exploring new architectures and evaluation methodologies that might one day bridge this gap.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The landscape of artificial intelligence in 2025 is marked by exceptional diversity and rapid evolution. While OpenAI's GPT-5, Google's Gemini 2.5 Pro, and Anthropic's Claude models exemplify leading innovations, the quest to define the absolute \"best\" AI is more nuanced than merely comparing benchmark scores. Instead, each model brings unique strengths tailored to specific domainsâ€”from coding and mathematics to scientific research and enterprise applications. Moreover, the continuing evolution of benchmarks, ethical considerations, and hardware advancements highlights that the future of AI depends on a balanced approach between raw computational power and responsible development practices.\n",
      "\n",
      "This report underscores that the current state of AI is not about an ultimate victor, but rather a competitive ecosystem where specialization, scalability, and ethical deployment drive progress. The interplay between technical achievements and societal impact will ultimately dictate which models emerge as enduring leaders in this dynamic field.\n",
      "\n",
      "In summary, the AI domain is on a transformative journey. The decision about the leading AI often depends on the specific application or performance metric considered. As research pushes forward, the landscape will further diversify, inspiring both incremental innovation and groundbreaking achievements in the pursuit of more intelligent, capable, and ethical AI systems.\n",
      "\n",
      "## References and Further Reading\n",
      "\n",
      "While numerous sources have contributed to this report, the following areas are particularly recommended for further insight:\n",
      "\n",
      "- Performance benchmarks and evaluations (e.g., MMLU, GPQA, OlympicArena)\n",
      "- Recent technological innovations from OpenAI, Google DeepMind, and Anthropic\n",
      "- Regulatory frameworks and ethical considerations surrounding AI deployment\n",
      "- Advances in hardware and cloud infrastructure supporting large language models\n",
      "\n",
      "These topics underline the multifaceted nature of the AI industry and provide a foundation for ongoing research and discussion in the field.\n",
      "\n",
      "\n",
      "\n",
      "=====FOLLOW UP QUESTIONS=====\n",
      "\n",
      "\n",
      "1. How can emerging benchmarks further improve the assessment of AI systems across diverse applications?\n",
      "2. What strategies can be adopted to balance model performance with ethical considerations and bias mitigation?\n",
      "3. How will advancements in hardware and cloud infrastructure impact the future scalability and sustainability of AI models?\n",
      "4. What are the implications of specialized versus general AI in sectors like scientific research and coding?\n",
      "5. How can regulatory frameworks evolve to better accommodate rapid advances in AI technology while ensuring public trust?\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sample Report in Markdown \n",
    "\n",
    "=====REPORT=====\n",
    "\n",
    "\n",
    "Report: # The State of Artificial Intelligence in 2025: Leaders, Benchmarks, and Future Directions\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The rapid evolution of artificial intelligence (AI) over the past few years shows no sign of slowing down. With the advent of increasingly sophisticated models, the question of \"which AI is the best\" has taken center stage. However, determining the best model involves considering a myriad of factors such as performance benchmarks, application niches, market impact, and technological innovations. This report presents an extensive review of the current AI landscape as of 2025, examining the leading models, their performance on a variety of benchmarks, the key companies driving the industry, and the challenges and opportunities that lie ahead.\n",
    "\n",
    "## Overview of Leading AI Models\n",
    "\n",
    "In the evolving world of AI, several models have risen to prominence by demonstrating exceptional capabilities in specific application domains and across a range of benchmarks. Below are some of the leaders:\n",
    "\n",
    "- **OpenAI's GPT-5**: Launched in August 2025, GPT-5 is lauded for its superior mathematics, scientific, and coding capabilities. Its introduction of \"vibe coding\" for prompt-based software development has made it a favorite among developers and business users alike. Its strong performance on benchmarks combined with significant market adoption underscores its leadership position in natural language processing.\n",
    "\n",
    "- **Anthropic's Claude Series (including Claude 4 and Claude 4.1)**: Anthropic's models have been engineered to excel in reasoning-intensive tasks, including complex code generation, logical reasoning, and multi-modal processing. They have been recognized particularly for their performance in scientific and coding benchmarks, positioning them as strong competitors to models like GPT-5.\n",
    "\n",
    "- **Google's Gemini Series (Gemini 2.5 Pro)**: Gemini 2.5 Pro, released during Google I/O 2025, emphasizes enhanced reasoning, multimodal integration, and a massive context window that supports diverse applications. Its integration with enterprise tools like Google Workspace reinforces Googleâ€™s broad ecosystem reach.\n",
    "\n",
    "- **Meta's Llama 3.3 and xAI's Grok 4 Heavy**: Meta and xAI have also entered this competitive field with capable models. Llama 3.3 offers varied parameter sizes and has integrated well with Metaâ€™s social media platforms. Meanwhile, xAI's Grok 4 Heavy has demonstrated impressive reasoning skills and a less filtered approach, appealing to users who value transparency and open-source initiatives.\n",
    "\n",
    "- **Upstage's Solar Pro 2**: Although fewer parameters are used by this model, its performance in benchmark tests is exceptionally strong, highlighting that sheer scale is not always synonymous with superior output. This model has set new standards in many areas, cementing Upstage as a key player, particularly in multinational and high-stake environments.\n",
    "...\n",
    "2. What strategies can be adopted to balance model performance with ethical considerations and bias mitigation?\n",
    "3. How will advancements in hardware and cloud infrastructure impact the future scalability and sustainability of AI models?\n",
    "4. What are the implications of specialized versus general AI in sectors like scientific research and coding?\n",
    "5. How can regulatory frameworks evolve to better accommodate rapid advances in AI technology while ensuring public trust?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "=====REPORT=====\n",
    "\n",
    "\n",
    "Report: # Voice Conversation Technology in EdTech\n",
    "\n",
    "The rapid evolution of voice technology has paved the way for significant innovations in educational technology. From language learning to assistive solutions for students with disabilities, voice conversational systems have the potential to transform the educational landscape. This report provides a thorough analysis of the current state of voice-powered EdTech solutions, examines their benefits and challenges, and offers insights into future research directions and practical implementations.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The education sector has witnessed a paradigm shift as digital solutions increasingly leverage artificial intelligence (AI) and voice technology to deliver personalized, accessible, and interactive learning experiences. With the advent of voice recognition, text-to-speech (TTS), and conversational AI, educators and students alike are discovering new ways to engage with content. This report delves into the integration of voice conversation technology into EdTech, exploring its potential for enhancing learning outcomes and streamlining administrative processes.\n",
    "\n",
    "### Key Drivers:\n",
    "\n",
    "- **Personalization:** Voice assistants provide individualized feedback, adapting lessons to each student's learning style.\n",
    "- **Accessibility:** Tools like TTS and voice recognition make learning more inclusive for students with disabilities.\n",
    "- **Efficiency:** Automation of administrative tasks allows educators to focus more on teaching and student engagement.\n",
    "\n",
    "Voice technology opens avenues for innovative learning strategies, fostering engagement, retention, and improved performance.\n",
    "\n",
    "## 2. Voice Technology Applications in Education\n",
    "\n",
    "Voice conversation technologies have rapidly expanded in the education space, with a range of applications designed to meet diverse learning needs. Below are some key areas where these technologies are making an impact:\n",
    "\n",
    "### 2.1 Language Learning and Pronunciation\n",
    "\n",
    "Many voice-powered edtech platforms focus on enhancing language skills through immediate, real-time feedback. For instance, applications like **ELSA Speak** and **Duolingo** utilize speech recognition to help learners improve their pronunciation and fluency. These systems are capable of:\n",
    "\n",
    "- Detecting pronunciation errors\n",
    "- Offering corrective feedback\n",
    "- Simulating real-life conversational scenarios\n",
    "\n",
    "By analyzing speech patterns and offering tailored practice routines, these tools not only increase confidence but also accelerate language acquisition.\n",
    "\n",
    "### 2.2 Enhanced Accessibility\n",
    "\n",
    "Voice technology plays a crucial role in making education more accessible. Students with visual impairments or motor challenges benefit greatly from converting speech to text and vice versa. Tools such as **Proloquo2Go** help non-verbal students communicate, while other applications offer autonomous navigation and content accessibility for learners facing various disabilities. Furthermore, these systems are becoming an essential component of special education, providing:\n",
    "\n",
    "- Real-time assistance\n",
    "- Customizable interfaces\n",
    "- Adaptability to individual needs\n",
    "\n",
    "### 2.3 Personalized Learning Experiences\n",
    "\n",
    "Through careful analysis of speech inputs and user interactions, voice conversational systems can craft personalized learning journeys. They can track progress, identify weaknesses, and suggest content that aligns with a studentâ€™s skill level. This personalization extends beyond language learning to include various subjects, ensuring that each learner receives the support they need.\n",
    "\n",
    "### 2.4 Administrative Efficiency\n",
    "\n",
    "Beyond direct instruction, voice assistants are streamlining educational administration. Educators are leveraging voice commands to:\n",
    "\n",
    "- Create lesson plans\n",
    "- Grade assignments\n",
    "- Provide dynamic feedback\n",
    "\n",
    "By automating routine tasks, teachers can devote more energy to fostering engaging, student-centric classrooms.\n",
    "\n",
    "## 3. Implementation Challenges and Considerations\n",
    "\n",
    "While the adoption of voice technology in EdTech brings a plethora of benefits, several challenges need to be addressed for effective implementation.\n",
    "\n",
    "### 3.1 Technical Limitations and Integration\n",
    "\n",
    "Voice systems are heavily dependent on reliable internet connectivity and device performance. Inconsistent network quality and legacy hardware can lead to poor audio quality, misinterpretations, and interruptions. Furthermore, the integration of these solutions into existing digital infrastructures poses challenges such as compatibility issues and the need for specialized technical expertise.\n",
    "\n",
    "### 3.2 Accessibility and Inclusivity Concerns\n",
    "\n",
    "Although voice technology is designed to enhance accessibility, not all students have equal access to the required devices or a stable internet connection. Additionally, individuals with auditory processing challenges or speech difficulties might face hurdles, necessitating the development of adaptive and flexible solutions that cater to diverse populations.\n",
    "\n",
    "### 3.3 Privacy and Data Security\n",
    "\n",
    "The utilization of voice data in educational settings raises significant privacy concerns. Voice recordings can inadvertently capture sensitive personal information, thus demanding rigorous security measures. Educational institutions must comply with strict regulations such as FERPA, COPPA, and GDPR. Recommended practices include:\n",
    "\n",
    "- Implementing privacy-by-design principles\n",
    "- Employing robust encryption techniques\n",
    "- Conducting regular security audits\n",
    "\n",
    "### 3.4 Bias and Fairness in AI Algorithms\n",
    "\n",
    "Bias in voice recognition, particularly in systems that have not been trained on diverse datasets, can lead to the misinterpretation of accents and dialects. This bias not only affects the quality of feedback but can also have a broader impact on the inclusivity of educational delivery systems.\n",
    "\n",
    "### 3.5 User Experience and Engagement\n",
    "\n",
    "Striking the balance between automation and human interaction is essential. Over-reliance on voice technology may limit critical thinking and human creativity, suggesting the need for a blended approach. Continuous user feedback, combined with iterative improvements, is essential to maintain a productive equilibrium between technology and traditional pedagogical methods.\n",
    "\n",
    "## 4. Case Studies and Emerging Solutions\n",
    "\n",
    "By analyzing specific implementations, we gain valuable insights into the practical applications of voice technology in education.\n",
    "\n",
    "### 4.1 Case Studies in Language Acquisition & Pronunciation\n",
    "\n",
    "- **Amira Learning:** This platform listens to young learners read aloud, detects errors, and offers real-time feedback to improve reading fluency. The technology assists in early literacy screening and dyslexia detection, revolutionizing traditional methods.\n",
    "- **ELSA Speak:** Focused on accent reduction and pronunciation enhancement, this tool is particularly useful for non-native speakers. It provides users with detailed, adaptive practice sessions, making it an effective language training method.\n",
    "- **Duolingoâ€™s AI-powered sessions:** Duolingo integrates conversational AI to simulate natural dialogue, providing learners with real-time language practice in a low-pressure environment.\n",
    "\n",
    "### 4.2 Voice Assistants for Administrative Automation\n",
    "\n",
    "- **Chatbots in Learning Management Systems (LMS):** Voice-enabled chatbots serve as around-the-clock personal assistants. They support students by answering questions, scheduling sessions, and delivering immediate feedback while reducing administrative overhead for educators.\n",
    "- **AI Teaching Assistants:** Tools like those implemented at institutions such as Morehouse College offer two-way vocal interactions, enabling both content delivery and real-time student support, further facilitating personalized instruction.\n",
    "\n",
    "### 4.3 Assistive Technologies in Special Education\n",
    "\n",
    "- **Proloquo2Go:** This AAC application supports non-verbal students, offering a voice-enabled communication system that adapts to individual needs. It exemplifies how voice technology can break down barriers in learning for populations with special challenges.\n",
    "- **Echo-Teddy and PicTalky:** These emerging systems use conversational AI to help autistic students and language-impaired children develop social skills and improve language comprehension.\n",
    "\n",
    "### 4.4 Voice-Enabled e-Learning Platforms\n",
    "\n",
    "Platforms such as **Murf.ai**, **DubSmart**, and **Speechify** are transforming content creation and localization. They harness advanced TTS and voice cloning technologies to convert traditional static content into dynamic audio experiences, enriching the learning environment for a global audience.\n",
    "\n",
    "## 5. Future Directions and Considerations\n",
    "\n",
    "The integration of voice technology in EdTech is still evolving. Future efforts should focus on:\n",
    "\n",
    "### 5.1 Enhancing Accuracy and Inclusivity\n",
    "\n",
    "Advancements in machine learning and neural networks will further refine speech recognition capabilities. This will involve training algorithms on more diverse datasets to reduce biases and ensure that systems accurately understand various accents and dialects.\n",
    "\n",
    "### 5.2 Ethical and Regulatory Compliance\n",
    "\n",
    "As technologies become more sophisticated, ethical considerations around data privacy and user consent will become paramount. It is essential that developers and educational institutions adhere to robust guidelines and establish transparent policies.\n",
    "\n",
    "### 5.3 Expanding the Range of Applications\n",
    "\n",
    "Beyond language learning and accessibility, future research could explore applications in other curricular areas such as STEM, social sciences, and the arts. Integrating multi-modal inputs (voice, gesture, and visual cues) can pave the way for more immersive and interactive learning experiences.\n",
    "\n",
    "### 5.4 Blended Learning Approaches\n",
    "\n",
    "A promising direction is the incorporation of voice technology into blended learning models. By combining digital engagement with traditional classroom settings, educators can optimize the benefits of both modalities to create dynamic, interactive, and efficient learning environments.\n",
    "\n",
    "### 5.5 Continuous User-Centric Iteration\n",
    "\n",
    "Ongoing feedback from educators and students is critical to the success of voice-enabled systems. Pilot programs, iterative testing, and user studies will ensure that the technology is aligned with the real-world needs of all stakeholders, leading to continual improvements.\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "The integration of voice conversation technology in EdTech offers a transformative potential that spans language learning, administrative efficiency, and accessibility enhancements. Despite the challenges in technical integration, privacy, and ensuring unbiased systems, the benefits of personalized learning and increased engagement are undeniable.\n",
    "\n",
    "Educators, developers, and policymakers must work collaboratively to harness these innovations responsibly. By addressing technical hurdles and ethical considerations, the future of voice-enabled education can be shaped into a tool that offers equitable learning opportunities to all.\n",
    "\n",
    "Voice technology is no longer a futuristic conceptâ€”it is a present-day reality that is gradually redefining the boundaries of education. The evolution of this technology in the next decade holds the promise of more dynamic, inclusive, and engaging learning environments for all learners.\n",
    "\n",
    "---\n",
    "\n",
    "*This comprehensive analysis highlights current trends, challenges, and future opportunities in voice-based EdTech solutions, serving as a robust resource for stakeholders in the educational technology ecosystem.*\n",
    "\n",
    "\n",
    "=====FOLLOW UP QUESTIONS=====\n",
    "\n",
    "\n",
    "1. How can advanced machine learning techniques further reduce biases in voice recognition systems for non-standard accents?\n",
    "2. What are effective strategies to integrate voice interaction tools into traditional classroom settings to create blended learning environments?\n",
    "3. How can mobile and low-bandwidth solutions be developed to enhance the accessibility of voice-enabled education in under-resourced regions?\n",
    "4. What policy frameworks and ethical guidelines are essential to ensure data privacy in the deployment of voice conversation technologies in schools?\n",
    "5. How might future research explore multi-modal learning experiences that integrate voice, gesture, and visual cues to enhance student engagement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Agents SDK: A Comprehensive Report\n",
    "\n",
    "*Published: October 2023*\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Core Concepts and Key Features](#core-concepts-and-key-features)\n",
    "3. [Architecture and Developer Experience](#architecture-and-developer-experience)\n",
    "4. [Comparative Analysis with Alternative Frameworks](#comparative-analysis-with-alternative-frameworks)\n",
    "5. [Integrations and Real-World Applications](#integrations-and-real-world-applications)\n",
    "6. [Troubleshooting, Observability, and Debugging](#troubleshooting-observability-and-debugging)\n",
    "7. [Community Impact and Future Directions](#community-impact-and-future-directions)\n",
    "8. [Conclusion](#conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In March 2025, OpenAI released the Agents SDK, a groundbreaking, open-source framework aimed at simplifying the development of autonomous AI agents capable of performing intricate tasks with minimal human intervention. Designed with a Python-first approach, the SDK offers a minimal set of abstractions, yet provides all the necessary components to build, debug, and optimize multi-agent workflows. The release marked a significant milestone for developers who seek to integrate large language models (LLMs) with advanced task delegation mechanisms, enabling next-generation automation in various industries.\n",
    "\n",
    "The primary goal of the OpenAI Agents SDK is to streamline the creation of agentic applications by offering core primitives such as *agents*, *handoffs*, and *guardrails*. These primitives are essential for orchestrating autonomous AI systems that perform key functions such as web search, file operations, and even actions on a computer. This report delves into the SDK's features, its operational architecture, integration capabilities, and how it compares to other frameworks in the rapidly evolving landscape of AI development tools.\n",
    "\n",
    "## Core Concepts and Key Features\n",
    "\n",
    "### Agents\n",
    "\n",
    "At the heart of the SDK are **agents**â€”intelligent entities that encapsulate a specific set of instructions and tools. Each agent is built on top of a large language model and can be customized with its own personality, domain expertise, and operational directives. For example, a \"Math Tutor\" agent could be designed to solve math problems by explaining each step clearly.\n",
    "\n",
    "**Key elements of an agent include:**\n",
    "\n",
    "- **Instructions:** Specific guidelines that shape the agent's responses and behavior in the context of its designated role.\n",
    "- **Tools:** Predefined or dynamically integrated tools that the agent can leverage to access external resources (e.g., web search or file search functionalities).\n",
    "\n",
    "### Handoffs\n",
    "\n",
    "A unique feature introduced by the SDK is the concept of **handoffs**. Handoffs allow agents to delegate tasks to one another based on expertise and contextual needs. This orchestration paves the way for sophisticated workflows where multiple agents work in tandem, each contributing its specialized capabilities to complete a complex task.\n",
    "\n",
    "### Guardrails\n",
    "\n",
    "Safety and reliability remain a cornerstone in AI development, and the SDK introduces **guardrails** as a means of controlling input and output validation. Guardrails help ensure that agents operate within defined safety parameters, preventing unintended actions and mitigating risks associated with autonomous decision-making.\n",
    "\n",
    "### Built-in Debugging and Observability\n",
    "\n",
    "The development process is further enhanced by built-in **tracing and visualization tools**. These tools offer real-time insights into agent interactions, tool invocations, and decision-making pathways, thereby making debugging and optimization more accessible and systematic. The tracing functionality is a vital feature for developers looking to fine-tune agent performance in production environments.\n",
    "\n",
    "## Architecture and Developer Experience\n",
    "\n",
    "### Python-First Approach\n",
    "\n",
    "The SDK is inherently Python-based, making it highly accessible to the vast community of Python developers. By leveraging existing language features without introducing excessive abstractions, the SDK provides both simplicity and power. The installation is straightforward:\n",
    "\n",
    "```bash\n",
    "mkdir my_project\n",
    "cd my_project\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install openai-agents\n",
    "```\n",
    "\n",
    "Once installed, developers can create and configure agents with minimal boilerplate code. The emphasis on a minimal learning curve has been a significant point of praise among early adopters.\n",
    "\n",
    "### Developer Tools and Tutorials\n",
    "\n",
    "In addition to comprehensive official documentation available on OpenAIâ€™s GitHub pages, the community has contributed numerous tutorials and code examples. Video tutorials by experts such as Sam Witteveen and James Briggs provide hands-on demonstrations, ranging from simple agent creation to more sophisticated scenarios involving parallel execution and advanced tool integrations.\n",
    "\n",
    "### Use of Python's Ecosystem\n",
    "\n",
    "The integration with Pythonâ€™s ecosystem means that developers can immediately apply a range of established libraries and frameworks. For instance, utilizing Pydantic for input validation in guardrails or leveraging visualization libraries to display agent workflows are examples of how the SDK embraces the strengths of Python.\n",
    "\n",
    "## Comparative Analysis with Alternative Frameworks\n",
    "\n",
    "While the OpenAI Agents SDK has received acclaim for its simplicity and robust integration with OpenAIâ€™s ecosystem, other frameworks such as LangGraph, CrewAI, and AutoGen have emerged as viable alternatives. Hereâ€™s how they compare:\n",
    "\n",
    "- **LangGraph:** Known for its graph-based architecture, LangGraph is ideal for handling complex and cyclical workflows that require sophisticated state management. However, it comes with a steeper learning curve, making it less accessible for projects that require quick prototyping.\n",
    "\n",
    "- **CrewAI:** Emphasizing a role-based multi-agent system, CrewAI excels in scenarios where collaboration among agents is critical. Its design promotes clear segregation of duties among different agents, which can be beneficial in customer service or large-scale business automation.\n",
    "\n",
    "- **AutoGen:** This framework supports flexible conversation patterns and diverse agent interactions, particularly useful in applications where adaptive dialogue is essential. Nevertheless, AutoGen may introduce additional overhead when managing state and coordinating multiple agents.\n",
    "\n",
    "In contrast, the OpenAI Agents SDK strikes an effective balance by offering a lightweight yet powerful toolset geared towards production readiness. Its strengths lie in its minimal abstractions, ease of integration with various tools (like web search and file search), and built-in observability features that are crucial for debugging and tracing agent interactions.\n",
    "\n",
    "## Integrations and Real-World Applications\n",
    "\n",
    "### Diverse Integrations\n",
    "\n",
    "The real power of the OpenAI Agents SDK surfaces when it is integrated with other systems and platforms. Notable integrations include:\n",
    "\n",
    "- **Box Integration:** Enhancing enterprise content management, Box has adopted the SDK to enable secure AI-powered data processing. This integration allows agents to reliably access and interpret proprietary data.\n",
    "\n",
    "- **Coinbase AgentKit:** With financial capabilities in mind, Coinbase introduced AgentKit, leveraging the SDK to incorporate financial operations and risk analysis directly into AI agents.\n",
    "\n",
    "- **Milvus and Ollama:** These integrations allow the SDK to handle high-performance data queries and run agents on local infrastructure respectively, ensuring both speed and privacy.\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "The versatility of the SDK lends itself to a multitude of applications:\n",
    "\n",
    "- **Customer Support:** Automated agents can be built to handle customer inquiries, providing faster and more accurate responses while reducing workload on human agents.\n",
    "\n",
    "- **Content Generation:** In marketing and media, agents can generate high-quality articles, detailed reports, and even code reviews with built-in content guidelines.\n",
    "\n",
    "- **Financial Analysis:** Specialized agents capable of real-time data fetching and market analysis can generate actionable insights for investors and analysts.\n",
    "\n",
    "- **Health and Wellness:** Custom agents can handle tasks such as appointment scheduling, patient record management, and even provide personalized fitness and dietary recommendations.\n",
    "\n",
    "- **Educational Tools:** Intelligent tutoring agents can assist students by providing personalized learning experiences and instant feedback on assignments.\n",
    "\n",
    "These applications underscore the SDKâ€™s transformative potential across various industries, driving the trend towards increased automation and efficiency.\n",
    "\n",
    "## Troubleshooting, Observability, and Debugging\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "As with any cutting-edge technology, developers working with the OpenAI Agents SDK have encountered challenges:\n",
    "\n",
    "- **API Key Management:** Authentication errors due to missing or invalid API keys are common. The solution involves ensuring that the `OPENAI_API_KEY` environment variable is correctly set or programmatically configured using OpenAIâ€™s helper functions.\n",
    "\n",
    "- **Rate Limitations:** Rate limits, an intrinsic challenge with API-based services, require developers to monitor dashboard usage and implement retry strategies with exponential backoff.\n",
    "\n",
    "- **Response Delays:** Network latency and high server loads can result in unexpected delays. Developers are advised to check network settings, adhere to best practices in setting request timeouts, and monitor OpenAIâ€™s service status.\n",
    "\n",
    "### Built-In Tracing Capabilities\n",
    "\n",
    "The SDK provides robust tracing tools that log agent inputs, outputs, tool interactions, and error messages. This level of observability is crucial for debugging complex workflows and allows developers to visualize the agentâ€™s decision-making process in real time. By configuring a `TracingConfig` object, developers can capture detailed insights and identify performance bottlenecks.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Prompt Engineering:** Refine prompts to reduce ambiguity and minimize unexpected outputs.\n",
    "- **Layered Validation:** Use guardrails extensively to ensure inputs and outputs are verified at multiple layers.\n",
    "- **Modular Design:** Break complex tasks into smaller, more manageable components using handoffs to delegate tasks appropriately.\n",
    "\n",
    "## Community Impact and Future Directions\n",
    "\n",
    "### Developer and Enterprise Adoption\n",
    "\n",
    "The release of the OpenAI Agents SDK has been met with enthusiasm within the developer community. Its ease of use, combined with comprehensive documentation and community-driven resources (such as tutorials on Class Central and DataCamp), has accelerated its adoption across educational, enterprise, and research sectors.\n",
    "\n",
    "Several leading organizations, including Box and Coinbase, have integrated the SDK into their workflows, demonstrating its capability to drive real-world business solutions. The open-source nature of the SDK, licensed under the MIT License, further encourages widespread industrial collaboration and innovation.\n",
    "\n",
    "### Future Prospects\n",
    "\n",
    "Looking forward, OpenAI plans to extend the SDKâ€™s support beyond Python, potentially embracing other programming languages like JavaScript. Additionally, future updates are anticipated to expand tool integrations, further enhance safety mechanisms, and streamline the development of multi-agent ecosystems. Planned deprecations of older APIs, such as the Assistants API in favor of the more unified Responses API, underline the SDKâ€™s evolving roadmap aimed at future-proofing agentic applications.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The OpenAI Agents SDK represents a significant step forward in the field of AI development. Its lightweight, Python-first framework facilitates the creation of autonomous agents that can handle a wide array of tasksâ€”from simple inquiries to complex multi-agent systems. The SDKâ€™s robust integration capabilities, combined with its focus on safety and observability, make it an ideal choice for both developers and enterprises seeking to build reliable, scalable agentic applications.\n",
    "\n",
    "In summary, the SDK not only lowers the barrier to entry for developing sophisticated AI applications but also sets the stage for further innovations as the ecosystem evolves. It is poised to become a standard toolkit for the next generation of AI-driven technologies, empowering users across sectors to achieve greater efficiency and creativity in task automation.\n",
    "\n",
    "---\n",
    "\n",
    "*For further reading, developers are encouraged to visit the official OpenAI documentation, join the community forums, and explore real-world use cases to deepen their understanding of this transformative tool.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "05-our-first-agent-with-langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
