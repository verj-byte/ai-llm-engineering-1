"""
MCP (Model Context Protocol) Sampling Server Implementation

This module implements an MCP server that operates in sampling mode.
In sampling mode, the server sends prompts and configuration to the client,
but the client is responsible for executing the actual LLM operations.

This approach is useful for:
- Client-side LLM processing
- Custom LLM integration
- Offline processing capabilities
- Testing and development scenarios

Key differences from standard MCP:
- Server provides prompts and configuration
- Client executes LLM operations via sampling callback
- No direct LLM execution on server side

Author: AI Assistant
Date: 2024
"""

from mcp.server.fastmcp import Context, FastMCP

from pydantic_ai import Agent
from pydantic_ai.models.mcp_sampling import MCPSamplingModel

# Initialize the FastMCP server with sampling capabilities
server = FastMCP('Pydantic AI Server with sampling')

# Create an agent with a system prompt that will be sent to the client
# The client's sampling callback will use this prompt for LLM execution
server_agent = Agent(system_prompt='always reply in rhyme')


@server.tool()
async def poet(ctx: Context, theme: str) -> str:
    """
    Generate a poem about a specified theme using MCP sampling mode.
    
    This tool demonstrates MCP sampling by:
    1. Taking a theme parameter from the user
    2. Creating a prompt for the client to execute
    3. Using MCPSamplingModel to delegate LLM execution to the client
    4. Returning the client-generated response
    
    Args:
        ctx (Context): MCP context containing session information
        theme (str): The theme or topic for the poem
        
    Returns:
        str: A poem about the specified theme, generated by the client's LLM
        
    Example:
        >>> await poet(ctx, "socks")
        "Socks for a fox, warm and cozy..."
        
    Note:
        In sampling mode, the actual LLM execution happens on the client side
        via the sampling_callback function. The server only provides the prompt
        and configuration.
    """
    # Create a prompt for the client to execute
    prompt = f'write a poem about {theme}'
    
    # Use MCPSamplingModel to delegate execution to the client
    # The client's sampling_callback will handle the actual LLM processing
    r = await server_agent.run(
        prompt, 
        model=MCPSamplingModel(session=ctx.session)
    )
    
    return r.output


if __name__ == '__main__':
    """
    Entry point for the MCP sampling server.
    
    When run directly, this script:
    1. Starts the MCP server in stdio mode
    2. Enables sampling mode for client-side LLM execution
    3. Waits for client connections and tool calls
    
    Usage:
        python mcp_sampling_server.py
        
    Note:
        This server should be invoked by an MCP client that provides
        a sampling_callback for LLM execution. The server itself
        does not execute LLM operations directly.
    """
    server.run()  # Run the server over stdio for client communication